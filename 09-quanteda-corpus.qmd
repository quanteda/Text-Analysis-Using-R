# Creating and Managing Corpora {#quanteda-corpus}

## Objectives


In this section, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus. 


## Methods


All text analysis projects involve choices about the texts to be analyzed. Creating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. Afterwards, the texts identified for analysis need to be collected, gathered in a text corpus, and (usually) accompanied by attributes that distinguish texts. Examples include the newspaper, the date of publication, and the article's author [@benoit2020text]. A text corpus could be all articles published on immigration in Irish newspapers, with each article constituting one document in the text corpus. These so-called document-level variables contain additional information on each document and allow researchers to distinguish between texts in their analysis. 

In some text analysis applications, the sample could constitute the entirety of texts. Analyzing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information that are not transcribed or published cannot be included in the analysis. When analyzing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions [@herzog15cuts]. The positions of politicians not selected for speaking at a budget debate, cannot be considered in the textual analysis. PROVIDE ANOTHER EXAMPLE? Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.

The selection of texts will determine the scope of the analysis, generalizability, and inferences you can draw from your analysis. The principles of your research design should justify the inclusion or exclusion of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in  televised debates and campaign speeches, the corpus will include debate transcripts and speeches. Thus, the research question should drive document selection.

::: {.callout-note appearance="simple"}
Including and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches [@benoit19sophistication]. Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in @sec-exploring-freqs. @sec-ml-topicmodels shows how to identify variation in topic prevalence for two or more groups of documents.
:::

Besides selecting texts for analysis, researchers need to determine the unit of analysis of the text corpus. The unit of analysis should be driven by your research design. If a researcher is interested in textual features associated with likes and retweets on Twitter, the unit of analysis is an individual tweet. A project about the association between sentiment and real-time voter reactions may shift the unit of analysis could be a candidate's speech utterance [@boussalis21reactions]. 


## Examples

For demonstration, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.[^09-quanteda-corpus-1] Following the **quanteda** naming conventinos, the object name starts with `data_` (since it contains data), followed by `corpus_` (indicating that the object is a text corpus) and `debates`, describing the text corpus. 

[^09-quanteda-corpus-1]: The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0. 


First, we inspect the text corpus using the `summary()` and `ndoc()` functions.

```{r,}
#| echo: false
#| message: false
library("quanteda")
library("rvest")
library("stringr")
library("quanteda.tidy")

# assign URL of debates to an object called url_debates
url_debates <- "https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50"

source_page <- read_html(url_debates)

# get debate meta-data

nodes_pres <- ".views-field-title a"
text_pres <- ".views-field-field-docs-start-date-time-value.text-nowrap"

debates_meta <- data.frame(
    location = html_text(html_nodes(source_page, nodes_pres)),
    date =  html_text(html_nodes(source_page, text_pres)),
    stringsAsFactors = FALSE
)

# format the date
debates_meta$date <- as.Date(trimws(debates_meta$date), 
                             format = "%b %d, %Y")

# get debate URLs
debates_links <- source_page |> 
    html_nodes(".views-field-title a") |> 
    html_attr(name = "href") 

# add first part of URL to debate links
debates_links <- paste0("https://www.presidency.ucsb.edu", debates_links)

# scrape search results
debates_scraped <- lapply(debates_links, read_html)

# get character vector, one element per debate
debates_text <- sapply(debates_scraped, function(x) {
    html_nodes(x, "p") |> 
        html_text() |>
        paste(collapse = "\n\n")
})

debates_meta$location <- str_remove_all(debates_meta$location, 
                                        "Presidential Debate at ")

data_corpus_debates <- corpus(debates_text, 
                              docvars = debates_meta)

docnames(data_corpus_debates) <- paste0("Debate: ", data_corpus_debates$date)
```


```{r}
summary(data_corpus_debates)

ndoc(data_corpus_debates)
```

The corpus consists of `r ndoc(data_corpus_debates)` documents. When inspecting the output of `summary(data_corpus_debates)` reveals that each document currently is the full transcript (Biden, Trump, and moderator) of a debate. The first document, containing the transcripts of the debate in Cleveland contains 24,548 tokens, 2565 types (i.e., unique tokens) and 1928 sentences. The second debate in Nashville is slightly shorter (21,652 tokens and 2,413 types). 


### Changing the Unit of Analysis

When analyzing debates, researchers often move to the level of utterances. We can achieve this using `corpus_segment()`. In the transcript, an utterance starts with the speaker's name in ALL CAPS, followed by a colon. The regular expression `"\\s*[[:upper:]]+:\\s+"` identifies speaker names in ALL CAPS (`\\s*[[:upper:]]+`), followed by a colon `+:` and a white space `\\s+`. For a primer on regular expression see @sec-appendix-regex. 


```{r}
# segment text corpus to level of utterances
data_corpus_debatesseg <- corpus_segment(data_corpus_debates, 
                                         pattern =  "\\s*[[:upper:]]+:\\s+", 
                                         valuetype = "regex", 
                                         case_insensitive = FALSE)

# overview of text corpus; n = 4 prints only the first four documents
summary(data_corpus_debatesseg, n = 4)

ndoc(data_corpus_debatesseg)
```

### Creating New Document-Level Variables

The new corpus consists of `r ndoc(data_corpus_debatesseg)` utterances by the moderators and candidates. The document-level variable `pattern` assigned the speaker name to each document. We can create a new `speaker` document-level variable by combining functions from the **stringr** and **quanteda.tidy** packages: `mutate()` creates a new `speaker` variable, and the stringr functions remove empty whitespaces (`str_trim()`), the colon (`str_remove_all()`) and change the names from UPPER CASE to Title Case (`str_to_title()`).

```{r}
library("stringr")
library("quanteda.tidy")

data_corpus_debatesseg <- data_corpus_debatesseg |> 
    mutate(speaker = stringr::str_trim(pattern),
           speaker = stringr::str_remove_all(speaker, ":"),
           speaker = stringr::str_to_title(speaker)) 
```

Next, we can use simple base R functions to inspect the count of utterances by speaker and debate. 

```{r}
# cross-table of speaker statements by debate
table(data_corpus_debatesseg$location,
      data_corpus_debatesseg$speaker)
```

We could further reshape the corpus to the level of sentences with `corpus_reshape()` if we are interested, for instance, in sentence-level sentiment or issue salience. 


```{r}
data_corpus_debatessent <- corpus_reshape(data_corpus_debatesseg,
                                          to = "sentences")

ndoc(data_corpus_debatessent)
```
The new text corpus moved from `r ndoc(data_corpus_debatesseg)` utterances to `r ndoc(data_corpus_debatessent)` sentences. Using functions such as **quanteda.textstat**'s `textstat_summary()` we can retrieve summary statistics about each sentence.

```{r}
library("quanteda.textstats")
dat_summary_sents <- textstat_summary(data_corpus_debatessent)

# aggregated summary statistics
summary(dat_summary_sents)
```
### Subset a Text Corpus

The last example in this chapter covers subsetting a text corpus based on document-level variables. Researchers who want to compare Trump and Biden may not be interested in the moderator's statements. We can exclude the moderators with `corpus_subset()`. 

```{r}
data_corpus_debatescand <- corpus_subset(data_corpus_debatesseg,
                                          speaker %in% c("Trump", "Biden"))

# check that subsetting worked as expected
table(data_corpus_debatescand$speaker,
      data_corpus_debatescand$location)
```


## Issues

### Identifying Patterns for Corpus Segmentation

Segmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker's surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected. 


### Reshaping Corpora after Statistical Analyis of Texts

In many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, @castanhosilva22eu study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models. 

@mueller22temporal studied the temporal focus of parties' campaign communication. First, he reshaped party manifestos to the level of sentences before identifying the temporal focus of each sentence using supervised classification (@sec-ml-classifiers). The regression models use an aggregated dataset with only three observations per manifesto (sentiment in sections about the past, present, and future).


::: {.callout-tip appearance="simple"}
Applying the `group_by()` in combination with the `summarise()` functions of the **dplyr** packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis.
:::


## Further Reading

- Selecting document and considerations of "found data": @grimmer22textasdata [ch. 4]
- Adjusting strings: @wickham17r4ds [ch. 14]

## Exercises

In the exercises below, we use a corpus of speeches from the 2017 UN General Debates (`data_corpus_ungd2017`, included in the **quanteda.corpora** package).^[We will update the corpus before publishing the book and use the most recent UNGD speeches.]

1. Identify the number of documents in `data_corpus_ungd2017`. 
2. Select only speeches delivered by representatives of African and Asian countries.
2. Reshape this subsetted corpus to the level of sentences.
3. Explore `textstat_summary()` of the **quanteda.textstats** package. Apply the function to `data_corpus_ungd2017` and assign it to an object called `tstat_sum_ungd`. 
4. What are the average, median, minimum, and maximum document lengths?
5. Advanced: filter only speeches consisting of at least ?? tokens. 
6. Advanced: use `tstat_sum_ungd` and create a histogram of document length using the **ggplot2** package. 
7. Advanced: rerun the code for plotting the distribution, but add `facet_wrap()` and create small multiples for each continent.
