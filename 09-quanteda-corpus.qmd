# Creating and Managing Corpora {#sec-quanteda-corpus}

## Objectives

In this section, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus.

## Methods

Text analysis projects involve choices about the texts to be analyzed. Texts are collected in a *text corpus*. Text corpora usually consist of three elements: the texts, document-level variables, and metadata about the corpus overall.

Creating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. A text corpus could include all articles published on immigration in Irish newspapers, with each article recorded as one *document* in the text corpus. A corpus could also be the reviews about one specific hotel, or a random sample of reviews about hotels in Europe. Researchers need to consider and justify why certain texts are (not) included in a corpus, since the generalisability and validity of findings can depend on the documents selected for analysis.

The principles of your specific project or research design should guide your decisions to include or exclude of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in televised debates and campaign speeches, the corpus will contain debate transcripts and speeches. Thus, the research question should drive document selection.

In some text analysis applications, the sample could constitute the entirety of texts. Analysing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information not transcribed or published cannot be included in the analysis. When analysing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions [@herzog15cuts]. The positions of politicians not selected for speaking at a budget debate cannot be considered in the textual analysis. Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.

::: {.callout-note appearance="simple"}
Including and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches [@benoit19sophistication]. Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in @sec-exploring-freqs. @sec-ml-topicmodels shows how to identify variation in topic prevalence for two or more groups of documents.
:::

Besides the raw text, corpora (usually) include attributes that distinguish texts. We call these attributes *document-level variables* or, in the language **quanteda**, "docvars". Docvars contain additional information on each document and allow researchers to differentiate between texts in their analysis. Document-level variables could be the name of the author of a text, the newspaper in which an article was published, the hotel which was reviewed on TripAdvisor, or the date when the document was created.

A text corpus also typically contains *metadata* recording important information about the corpus as a whole. Metadata that is generally useful to record in a corpus include the source where the corpus was obtained, including possibly the URL; the author of the corpus (if from a single source); a title for the corpus; and possibly important keywords that might be used later in categorising the corpus. Corpus metadata can be quite general, including a codebook, instructions how to cite the corpus, or a license for the use of the corpus or copyright information.

::: {.callout-tip appearance="simple"}
While there no universal standard exists for which metadata to record, there are guidelines for corpus objects that are followed in the **quanteda** packages that provide example corpora, including those in the package that accompanies this book. These all record the following metadata fields: *title* and  *description*, providing a title and short text description of the corpus respectively; *source* and *url*, documenting in plain text and as a web address where the corpus was obtained; *author*, even when there was no single author of the documents; and *keywords*, a vector of keywords categorising the corpus. We recommend using this scheme for new corpus object that you create.
:::

Just in the decision to selecting which documents should be included in the corpus, the text analyst must also think carefully about what will define a document. Sometimes, this decision is natural, such recording individual product reviews, individual speeches (as in the Presidential inaugural speech corpus), or individual social media posts. Because the definition of a document can be fluid, however, not all such decisions are so clear cut. A user might wish to cut some longer documents into sub-sections (like chapters of a book, or paragraphs of a speech) that will form documents. Or, when facing lots of smaller texts such as posts on Twitter, a user might wish to aggregate these by user, or by day or week, to define new documents. As with the decision to select texts for a corpus, the decision as to precisely how documents should be defined will depend on a user's needs for a specific project. Sometimes, this will produce a need to redefine the document units in which texts were collected, into document units that resemble the units that the user will analyse for the purposes of a broader study. In their study about the association between emotions and real-time voter reactions, for instance, @boussalis21reactions redefine the speech unit of documents in which campaign speeches where found, into specific utterances that were later analysed as specific statements associated with variable levels of sentiment. In reviews of hotels, the unit could be an individual review of a hotel, the mention of the hotel and its immediate context, or all texts that review a hotel. The point is that the document units in which texts are collected may not be the same as the document units that a text researcher will need for the purposes of their analysis. In the section that follows, we will show to reshape, split, and combine document units from a corpus that allow a flexible redefinition of document-level units to meet specific analytic needs. We also cover how to access and modify all elements of the corpus mentioned in this section.

::: {.callout-important appearance="simple"}
Some tools for text analysis call for "cleaning" a corpus, sometimes by removing elements that are deemed to be unwanted, such as punctuation. We strongly discourage this, because such radical interventions lessen the generality of a corpus. We take the same approach to defining documents: We prefer that a corpus contain *natural* units of analysis, such a individual reviews, rather than *analytic* units of analysis such as combined or aggregated reviews. Chapters -@sec-quanteda-tokensadvanced and --@sec-quanteda-dfms show to combine documents to the _analytic_ unit of analysis using `tokens_group()` or `dfm_group()`. Cleaning should be limited to removing *textual cruft*, such as page numbers if a document was converted from pdf format, and these are not of interest.
:::


## Applications

::: {.callout-important appearance="simple"}
**Authors' note:** We should move the segmentation to the Advanced section, and use the Applications section for:

-   [x] Creating a corpus from different input sources, including some minimal cleanup
-   [x] Accessing and assigning docvars
-   [x] Accessing and assigning corpus metadata
-   [x] `corpus_subset()`, from below, using `data_corpus_inaugural`
-   [x] `corpus_sample()`, using `data_corpus_inaugural`
:::

In this section, we will demonstrate how to create a corpus from different input sources, how to access and assign docvars and corpus metadata, how to subset documents from a corpus based on document-level variables, and how to draw a random sample of documents from a text corpus. We will use the corpus of US inaugural speeches, which is included in the **quanteda** package as `data_corpus_inaugural`. 

### Creating a Corpus from Different Input Sources

We can create a *quanteda* corpus from different input sources, for example a data frame, a **tm** corpus object, or a keyword-in-context object (`kwic`). 


::: {.callout-note appearance="simple"}
In Chapter -@sec-acquire-files we show how users can import texts in various formats, e.g., PDF documents, Word documents, or spreadsheets with the **readtext** package. The output of the `readtext()` function is always a data frame, which can be easily  transformed to a corpus object using `corpus()`. 
:::


Creating a corpus from a data frame or tibble object is straightforward. We use `corpus()` and determine the `text_field` that contains the text. By default, all remaining variables of the data frame will be added as docvars.

```{r}
#| message: false
library("quanteda")

# create data frame for illustration purposes
dat <- data.frame(
    letter_factor = factor(rep(letters[1:3], each = 2)),
    some_ints = 1L:6L,
    some_text = paste0("This is text number ", 1:6, "."),
    stringsAsFactors = FALSE,
    row.names = paste0("fromDf_", 1:6))

head(dat)

# create corpus
corp_dataframe <- corpus(dat, text_field = "some_text",
                         meta = list(source = "From a data.frame called dat."))

summary(corp_dataframe)
```

We can also create a **quanteda** corpus from a **tm** corpus object. The following example uses the corpus `crude` corpus from the **tm** package which contains 20 exemplary news articles. 

```{r}
# load in a tm example VCorpus object
data(crude, package = "tm")

# create quanteda corpus
corp_newsarticles <- corpus(crude)
```


In addition, we can create a text corpus from a `kwic()` object. Keywords-in-context is covered extensively in Chapter @sec-exploring-kwic.

For example, we could extract all mentions of "terrible", "awful", and "disgusting", and the immediate context of 20 words from our corpus of hotel reviews, and convert the output to a new text corpus.

```{r}
library("TAUR")

kw_TAhotels <- data_corpus_TAhotels |> 
    tokens(remove_separators = FALSE) |> 
    kwic(pattern = c("terrible", "awful",
                     "disgusting"),
         window = 20,
         separator = " ")

# check number of matches
nrow(kw_TAhotels)

# check how often each pattern was matched
table(kw_TAhotels$pattern)

# convert kwic object to a new text corpus
corp_kwic <- corpus(kw_TAhotels, split_context = FALSE)

print(corp_kwic, max_ndoc = 5, max_nchar = 40)
```

### Inspecting Document-Level Variables and Metadata of a Corpus

Document-level variables are crucial for many text analysis projects. For instance, if we want to compare differences in issue emphasis in inaugural before and after World War II, we need a document-level variable specifying the year when a speech was delivered. We may also want to create a binary variable indicating whether a speech was delivered before or after World War II. Below, we provide examples on how to inspect document-level variables and how to create new docvars.

First, we load the packages and inspect the text corpus using `summary()`.

```{r}
# provide summary of corpus and print document-level variables for the first six documents
summary(data_corpus_inaugural, n = 6)
```

The corpus consists of `r ndoc(data_corpus_inaugural)` documents. Each document is one inaugural speech delivered between `r min(data_corpus_inaugural$Year)` and `r max(data_corpus_inaugural$Year)`. 

The `meta()` function returns a named list containing the corpus-level information stored in the corpus, reflecting the standard set of metadata fields that we have chosen to use for **quanteda** objects. The "keywords" element of this list is a character vector containing five keywords.

```{r}
meta(data_corpus_inaugural)
```

We can also access the names of our docvars.

```{r}
names(docvars(data_corpus_inaugural))
```

We see that the `Year` variable stores the year when each speech was delivered. We could create a binary variable `PrePostWW2` which distinguishes between speeches held before and after 1945. T

```{r}
#| message: false

# use $ operator
data_corpus_inaugural$PrePostWW2 <- ifelse(
    data_corpus_inaugural$Year > 1945, 
    "Post World War II", "Pre World War II")

# equivalent to
docvars(data_corpus_inaugural, "PrePostWW") <- ifelse(
    docvars(data_corpus_inaugural, "Year") > 1945, 
    "Post World War II", "Pre World War II")
```


::: {.callout-note appearance="simple"}
There are multiple ways to access docvars in a **quanteda** object---here a corpus, but also other objects derived from a corpus such as tokens object, or separate objects such as dictionaries. These can be `docvars(data_corpus_inaugural, "Party")` or, for individual docvars, using the `$` operator known from lists or data.frames, i.e. `data_corpus_inaugural$Party`.
:::


### Subsetting a Text Corpus

Applying `corpus_subset()` to a corpus allows us to subset a text corpus based on values of document-level variables. We can filter, for instance, only speeches delivered by Democratic Presidents, speeches delivered after World War II, or speeches from selected presidents. 

```{r}
# subset only Republican Presidents
corp_democrats <- corpus_subset(data_corpus_inaugural, Party == "Republican")

# subset speeches delivered after WW2
corp_postww2 <- corpus_subset(data_corpus_inaugural, PrePostWW2 == "Post World War II")

corp_postww2 <- corpus_subset(data_corpus_inaugural, Year > 1945)

# subset speeches delivered by Clinton, Obama, and Biden
corp_cob <- corpus_subset(data_corpus_inaugural, President %in% c("Clinton", "Obama", "Biden"))

# remove Biden's 2021 speech
corp_nobiden <- corpus_subset(data_corpus_inaugural, President != "Biden")
```


::: {.callout-tip  appearance="simple"}
Usually, we use logical operators for subsetting a text corpus or creating a new document-level variable based on certain conditions. The most relevant logical operators are: 

- `<`: less than
- `<=`: less than or equal to
- `>`: greater than
- `>=`: greater than or equal to
- `==`: equal
- `!=`: not equal
- `!x`: not x (negation)
- `x | y`: x OR y
- `x & y`: x AND y
:::

### Randomly Sample Documents From a Corpus

Often, users may be interested in taking a random sample of documentsf from the text corpus. For example, a user may implement and test the workflow for a small, random subset of the documents, and only later run the analysis on the full text corpus. 

The `corpus_sample()` function allows for sampling documents from a specified size (`size`) with or without replacement (`replace`), optionally stratified by grouping variables (`by`) or with probability weights (`prob`).

```{r}
# sample 30 inaugural speeches without replacement
corp_sample30 <- corpus_sample(data_corpus_inaugural, size = 30, replace = FALSE)

# check that the corpus consists of 30 documents
ndoc(corp_sample30)

# sample 15 speeches from before and 15 from after World War I (30 documents in total)
corp_sample_prepost <- corpus_sample(data_corpus_inaugural, size = 15, 
                                     replace = FALSE,
                                     by = PrePostWW2)

# check that corpus contains 15 pre- and 15 post-WW II documents
table(corp_sample_prepost$PrePostWW2)
```



## Advanced


### Corpus "Cleaning"

Why we favour a "conservative" approach, including storing documents in their natural format rather than an analytical format that can be created from the natural units, and especially not intervening to change documents in a corpus (such as removing punctuation).

We can discuss what constitutes recommended cleaning, and provide an example, perhaps from the debate transcript example.

### The Structure of a **quanteda** Corpus Object

To be added...

-   the structure of a corpus object, which is basically a character vector with special attributes.
-   Why to preserve these we need \[\] even when it's mycorpus\[seq_len(mycorpus)\] \<- some_character_transformation

### Conversion from and to Other Formats

Below, we provide examples of converting a **qunateda** corpus to other objects. Converting a `corpus` object to a data frame could be useful, for instance, if you want to reshape a corpus to sentences and hand-code a sample of these sentences for validation purposes or a training set for supervised classifiers (see @sec-ml-classifiers). The example below shows how to sample 1,000 hotel reviews, and converting this corpus to a data frame. 

```{r}
# set seed for reproducibility
set.seed(35)

corp_TAhotels_sample <- corpus_sample(data_corpus_TAhotels, 
                                      size = 1000, 
                                      replace = FALSE)

# convert corpus to data frame
dat_TAhotels_sample <- convert(corp_TAhotels_sample, to = "data.frame")
```

 which will consist of the columns `doc_id`, containing the document name; `text`, containing the actual text of the document; and any docvars that were in the corpus, which in this example consist of `location` and `date`.
 
The resulting data frame consists of `r nrow(dat_TAhotels_sample)` randomly sampled reviews and `r  ncol(dat_TAhotels_sample)` columns: `doc_id`, containing the document name; `text`, containing the actual text of the document; and any docvars that were in the corpus (in our case `Rating`).

We can easily store this data frame as a spreadsheet, which can be useful when hand-code a selection of reviews (for instance, based on their sentiment).

```{r}
#| eval: false

# store data frame as csv file with UTF-8 encoding and remove row names
write.csv(dat_TAhotels_sample_base_r, 
          file = "dat_TAhotels_sample.csv",
          fileEncoding = "UTF-8",
          row.names = FALSE)

# the rio package allows us to store 
# the data frame in many different file formats
library("rio")

export(dat_TAhotels_sample_base_r,
       file = "dat_TAhotels_sample.xlsx")
```

### Converting or Coercing to Other Object Types

We can coerce an object to a text corpus (`as.corpus()`) or extract the texts from a corpus (`as.character()`). `as.corpus()` can only be applied to a **quanteda** corpus object and upgrades it to the newest format. This function can be handy for researchers who work with older **quanteda** objects. For transforming data frames or **tm** corpus objects into a **quanteda** corpus, you should use the `corpus()` function. The function `as.character()` returns the corpus text as a plain character vector.

```{r}
chars_TAhotels <- as.character(data_corpus_TAhotels)

# inspect character object
str(chars_TAhotels)
```


### Changing the Unit of Analysis

In this section we will demonstrate the construction of a corpus that requires some reshaping. For this purpose, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.[^09-quanteda-corpus-1] The corpus **TAUR** package contains this text corpus as `data_corpus_debates`.

[^09-quanteda-corpus-1]: The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0.


```{r}
# inspect text corpus
summary(data_corpus_debates)
```


The output of `summary(data_corpus_debates)` reveals that the corpus contains only two documents, i.e. the full debate transcript. The first document contains the transcript of the debate in Cleveland, consisting of `r ntoken(corpus_subset(data_corpus_debates, date == "2020-09-29")) |> format(big.mark = ",")` tokens, `r  ntype(corpus_subset(data_corpus_debates, date == "2020-09-29")) |> format(big.mark = ",")` types (i.e., unique tokens), and `r nsentence(corpus_subset(data_corpus_debates, date == "2020-09-29")) |> format(big.mark = ",")` sentences. The second debate in Nashville is slightly shorter (`r  ntoken(corpus_subset(data_corpus_debates, date == "2020-10-22")) |> format(big.mark = ",")` tokens and `r  ntype(corpus_subset(data_corpus_debates, date == "2020-10-22")) |> format(big.mark = ",")` types).

::: {.callout-note appearance="simple"}
"Types" and "tokens" are specific linguistic terms that refer to the quantity and quality of the linguistic units found in a text. A *type* is a unique lexical unit, and a *token* is any lexical unit. Lexical units are most often words, but may also include punctuation characters, numerals, emoji, or even spaces. We cover this in greater detail in [Chapter -@sec-quanteda-tokens].
:::


The debate corpus contains only two documents, each a transcript of a length debate that included many different statements by the presidential candidates and the moderator. The corpus recorded these "natural" speech units as a document, but in the case of transcripts, that is often not the "analytical" document unit that will meet the analyst's needs.

To make the document unit align with our analytical purposes, we will need to reshape the corpus by segmenting it into individual statements, recording the speaker and the sequence in which the statement occurred, as new document-level variables.

The tool for this segmentation in **quanteda** is `corpus_segment()`. As discussed in [Chapter -@sec-quanteda-overview], the name of this function reflects the object that it takes as an input and produces as an output (a corpus) and the verb element describes the operation it will perform (segmentation). To perform this segmentation, we will rely on the presence in the transcript of regular patterns marking the introduction of a new statement. **quanteda** can take multiple forms of patterns, including a fixed match, a "glob" match, and a full regular expression match. Because we need to match a very specific pattern with more elements than a simple glob pattern can handle, we will need to use the regular expression "valuetype". (For a brief primer on pattern matching and regular expressions see [Appendix -@sec-appendix-regex].)

In the transcript, a statement starts with the speaker's name in ALL CAPS, followed by a colon. To match this marker of a new statement, we will use the regular expression `"\\s*[[:upper:]]+:\\s+"`. This identifies speaker names in ALL CAPS (`\\s*[[:upper:]]+`), indicating no or more spaces followed by one or more upper case words, followed by a colon (the literal`:`), followed by one or more white spaces `\\s+`. The `case_insensitive = FALSE` tells the pattern matcher to pay attention to case (how a word's letters are capitalised or not).

By default, `corpus_segment()` will split the original documents into smaller documents corresponding to the segments preceded by the pattern, and extract the pattern found to a new variable for the split document (a docvar named `pattern`). It is also smart enough to record the original, longer document from which each split document originally came.

```{r}
# segment text corpus to level of utterances
data_corpus_debatesseg <- corpus_segment(data_corpus_debates, 
                                         pattern =  "\\s*[[:upper:]]+:\\s+", 
                                         valuetype = "regex", 
                                         case_insensitive = FALSE)

# overview of text corpus; n = 4 prints only the first four documents
summary(data_corpus_debatesseg, n = 4)
```

The new, split corpus, has turned the 2 original documents into `r  ndoc(data_corpus_debatesseg) |> format(big.mark = ",")` new docments. Note also that we have kept the **quanteda** naming conventions in assigning the new object a name that identifies it as data, as a corpus, and describes what it is. Because any function in **quanteda** that begins with `corpus_` will always take in and output a corpus, we know that this object will be a corpus.

In the split document, the new docvar `pattern` contains the pattern that we matched. Because this also included additional elements such as the spaces and the colon, however, it's not as clean as we would prefer. Let's turn this into a new `speaker` document-level variable by cleaning it up and renaming it. To make this easy, we will use the **stringr** for string manipulation, and the **quanteda.tidy** package for applying some **dplyr** functions from the "tidyverse" to manipulate the variables. Specifically, we will use `mutate()` to create a new `speaker` variable, and the **stringr** functions to remove empty whitespace (`str_trim()`) and the colon (`str_remove_all()`), and to change the names from UPPER CASE to Title Case (`str_to_title()`).

```{r}
#| message: false
library("stringr")
library("quanteda.tidy")

data_corpus_debatesseg <- data_corpus_debatesseg |> 
    mutate(speaker = stringr::str_trim(pattern),
           speaker = stringr::str_remove_all(speaker, ":"),
           speaker = stringr::str_to_title(speaker)) 
```

Next, we can use simple base R functions to inspect the count of utterances by speaker and debate.

```{r}
# cross-table of speaker statements by debate
table(data_corpus_debatesseg$location,
      data_corpus_debatesseg$speaker)
```

We could further reshape the corpus to the level of sentences with `corpus_reshape()` if we are interested, for instance, in sentence-level sentiment or issue salience.

```{r}
data_corpus_debatessent <- corpus_reshape(data_corpus_debatesseg,
                                          to = "sentences")

ndoc(data_corpus_debatessent)
```

The new text corpus moved from `r ndoc(data_corpus_debatesseg)` utterances to `r ndoc(data_corpus_debatessent)` sentences. Using functions such as **quanteda.textstat**'s `textstat_summary()` we can retrieve summary statistics of the sentence-level corpus.

```{r}
library("quanteda.textstats")
dat_summary_sents <- textstat_summary(data_corpus_debatessent)

# aggregated summary statistics
summary(dat_summary_sents)
```


::: {.callout-tip appearance="simple"}
Segmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker's surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected.
:::


### Reshaping Corpora after Statistical Analyis of Texts

In many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, @castanhosilva22eu study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models.

::: {.callout-tip appearance="simple"}
Applying the `group_by()` in combination with the `summarise()` functions of the **dplyr** packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis.
:::

## Further Reading

-   Selecting document and considerations of "found data": @grimmer22textasdata [ch. 4]
-   Definitions of document units and the reasons for and implications of redefining these: @benoit2020text [pp480--481, "Defining Documents and Choosing the Unit of Analysis"]
-   Adjusting strings: @wickham17r4ds [ch. 14]

## Exercises

In the exercises below, we use a corpus of TripAdvisor hotel reviews, `data_corpus_TAhotels`, included in the **TAUR** package).

1.  Identify the number of documents in `data_corpus_TAhotels`.
2.  Subset the corpus by selecting only reviews with the maximum rating of 5.
3.  Reshape this subsetted corpus to the level of sentences.
4.  Explore `textstat_summary()` of the **quanteda.textstats** package. Apply the function to `data_corpus_TAhotels` and assign it to an object called `tstat_sum_TAhotels`.
5.  What are the average, median, minimum, and maximum document lengths?
6.  Advanced: use `data_corpus_TAhotels` filter only speeches consisting of at least 300 tokens by combining `ntoken()` and `corpus_subset()`.
7.  Create a new document-level variable `RankingRecoded` that which splits up the rankings into three categories: Negative (Ranking = 1 and 2), Neutral (Ranking = 3), and Positive (Ranking = 4 and 5). Which of the three categories is the most frequent one?
