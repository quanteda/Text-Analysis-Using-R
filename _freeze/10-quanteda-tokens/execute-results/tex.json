{
  "hash": "84598f7a34317798bbeb63bb09451271",
  "result": {
    "markdown": "# Creating and Managing Tokens {#sec-quanteda-tokens}\n\n\n## Objectives\n\nIn this chapter, we cover the basics of tokenisation and the **quanteda** tokens object.\nYou will learn what to pay attention to when tokenizing texts, and how to select, keep, and remove tokens. We also introduce the concept of \"stopwords\" and how to remove stopwords from a tokens object. We cover stemming and lemmatisation, methods for reducing words to their base or root form. Finally, we show how to manage metadata in a tokens object, which largely mirrors the way metadata is managed in a corpus object.\n\n## Methods\n\nAfter having collected the text for analysis and collecting these texts in a corpus (see @sec-quanteda-corpus), we usually tokenise our text. Tokenisation describes the process of breaking up text into individual units (tokens) that can be analysed. Usually, tokenisation works by identifying delimiters between words. In most languages, these delimiters are spaces, but they can also be newlines or tabs. Tokenisation is a critical step of every text analysis project. It is very important to pay very close attention to the tokenisation of your text.\n\nWe can tokenise our texts to different levels: words,sentences, or individual characters. Most of the time, we tokenise our documents to the level of words. Note that the level of tokenisation does not affect the unit of analysis in our text corpus: if the corpus is on the level of paragraphs, the tokenisation to words will preserve the unit of analysis, and documents will remain on the paragraph level.\n\nThroughout all tokenisation steps, we know the position of each tokens in the document, which we can use to identify and compound multiword-expressions or apply a dictionary with multiword expressions. These aspects will be covered in much more detail in @sec-quanteda-tokensadvanced. For now, it is important to keep in mind the main difference between tokens objects and a document-feature matrix: while we know the relative position of each feature in a tokens object, a document-feature matrix reports the counts of features (which can be words, punctuation characters, numbers, or multiword expressions) in each document, but does not allow us to identify _where_ a certain feature appeared in the document.\n\nThe next step after the tokenization of our documents  is often described as pre-processing, but we prefer \"processing.\" Processing does not precede the analysis, but is an integral part of the workflow and can influence subsequent results [@denny18preprocess].\nThe most common types is the lower-casing of text (e.g., \"Party\" will be change to \"party\"); the removal of punctuation characters and symbols; the removal of so-called stopwords which appear frequently throughout all documents but do not add specific meaning; stemming or lemmatisation; or compounding phrases/multiword expressions to a single token. All of these decisions can influence our results. In this chapter, we focus on lower-casing, the removal of punctuation characters, and stopwords. The subsequent chapter covers more advanced tokenisation approaches, including phrases, tokens replacement, and chunking.\n\n**Lower-casing** words is a standard procedure in many text analysis projects. The rationale behind this is that \"Income\" and \"income\" should be interpreted as the same textual feature due to their shared meaning. Furthermore, it's a common practice to **remove punctuation characters** like commas, colons, semi-colons, question marks, and exclamation marks. Though these characters appear prolifically across texts, they often don't significantly contribute to a quantitative text analysis. However, in certain contexts, punctuation can carry significant weight. For instance, the frequency of question marks can differentiate between positive and negative reviews of movies or hotels. They can also demarcate the rhetoric of opposition versus governing parties in parliamentary debates. Negative reviews might employ more question marks than positive ones, while opposition parties might employ rhetorical questions to criticise the ruling party. **Symbols** are another category often pruned during text processing.\n\nThe removal of **stopwords** prior to quantitative analysis is another frequent step. The rationale behind removing stopwords might be to shrink the vector space, condense the size of document-feature matrices, or prevent common words from inflating document similarities. It's pivotal to understand that there's no one-size-fits-all stopwords list. These lists are usually developed by researchers and tend to be domain-specific. Some words might be redundant for specific research topics but invaluable for others. For instance, feminine pronouns like \"she\" and \"her\" are integral when scrutinising partisan bias in abortion debates [@monroe08intro], even though they might appear in many stopwords lists. In another case, the word \"will\" plays a pivotal role in discerning the temporal direction of a sentence [@mueller22temporal]. Applying stopword lists without close inspection may lead to the removal of essential terms, undermining subsequent analysis. It is imperative that researchers critically evaluate which words to retain or exclude.\n\n::: {.callout-note appearance=\"simple\"}\nStopword lists often originate from two primary methodologies. The first method involves examining frequent words in text corpora and manually pinpointing non-essential features. The second method leverages automated techniques, like term-frequency-inverse-document-frequency (tf-idf), to detect stopwords [@sarica21stopwords; @wilbur92stopwords]. Refer to @sec-exploring-freqs for an in-depth exploration of strategies to discern both informative and non-informative features.\n:::\n\n**Stemming** and **lemmatisation** serve as strategies to consolidate features. Stemming truncates tokens to their stems. In contrast, lemmatisation transforms a word into its fundamental form. Most stemming methodologies use predefined lists of suffixes and associated rules governing suffix removal. Many languages have these lists readily available. An exemplary rule-based stemming algorithm is the Snowball stemmer, developed by Martin F. Porter [@porter01snowball]. Lemmatisation, being more nuanced than stemming, ensures that tokens align with their root form. For example, a stemmer might truncate \"easily\" to \"easili\" and leave \"easier\" untouched. In contrast, a lemmatiser would convert both \"easily\" and \"easier\" to their root form: \"easy\". While stemming in particular, and lemmatisation to a lower degree, are very popular processing step, reducing features to their base forms often does not change substantive results. @schofield16stem compare and apply various stemmers before running topic models (@sec-ml-topicmodels). Their careful validation reveals that \"stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability\" [@schofield16stem: 287].\n\n\n## Applications\n\nIn this section, we apply the processing steps described above. The examples in this chapter are limited to tokenizing short texts. In practice and in most other chapters, you will be working with much larger text data sets. We always recommend creating a corpus object first and then tokenizing the corpus, rather than moving directly from a character vector or data frame to a tokens object.\n\n### Tokenizing and Lowercasing Texts\n\nLet's start with exploring the `tokens()` function.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# texts for examples\ntxt <- c(\n    doc1 = \"A sentence, showing how tokens() works.\",\n    doc2 = \"@quantedainit and #textanalysis https://quanteda.org\")\n\n# tokenisation without any processing\ntokens(txt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n:::\n\n\n\nThe `tokens()` function includes several arguments for changing the tokenisation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise to sentences (rarely used)\ntokens(txt, what = \"sentence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"A sentence, showing how tokens() works.\"\n\ndoc2 :\n[1] \"@quantedainit and #textanalysis https://quanteda.org\"\n```\n:::\n\n```{.r .cell-code}\n# tokenise to character-level\ntokens(txt, what = \"character\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\" \"s\" \"e\" \"n\" \"t\" \"e\" \"n\" \"c\" \"e\" \",\" \"s\" \"h\"\n[ ... and 22 more ]\n\ndoc2 :\n [1] \"@\" \"q\" \"u\" \"a\" \"n\" \"t\" \"e\" \"d\" \"a\" \"i\" \"n\" \"i\"\n[ ... and 37 more ]\n```\n:::\n:::\n\n\n\nWe can lowercase our tokens object by applying the function `tokens_tolower()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens(txt) |>\n    tokens_tolower()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"a\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n:::\n\n\n\n### Removing Punctuation, Separators, Symbols\n\nWe can remove several tokens with inbuilt functions or adjust how hyphens are tokenised.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove punctuation\ntokens(txt, remove_punct = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"A\"        \"sentence\" \"showing\"  \"how\"      \"tokens\"   \"works\"   \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n\n```{.r .cell-code}\n# remove numbers, symbols, and separators\ntokens(txt,\n    remove_numbers = TRUE,\n    remove_separators = TRUE,\n    remove_symbols = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n\n```{.r .cell-code}\n# split tags and hyphens\ntokens(txt,\n    split_tags = TRUE,\n    split_hyphens = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@\"                    \"quantedainit\"         \"and\"                 \n[4] \"#\"                    \"textanalysis\"         \"https://quanteda.org\"\n```\n:::\n:::\n\n\n\nDetails on processing steps are provided in the documentation of the tokens function, which can be accessed by typing `?tokens()` into the terminal.\n\n::: {.callout-warning appearance=\"simple\"}\nWith large text corpora, it might be difficult to assess whether the tokenisation works as expected. We therefore encourage researchers to work with minimal working examples, e.g., one or two sentences that contain certain features you want to tokenise, remove, keep, or compound. You can run your code on this small example and test whether the tokenisation worked as expected before applying the code to the entire corpus.\n:::\n\n\n### Inspecting and Removing Stopwords\n\nThe **quanteda** package contains several functions that process tokens. You start with tokenizing your text corpus, possibly apply some of the processing options included in the `tokens()` function, and proceed by applying more advanced processing steps, which always start with `tokens_`.\n\nLet's start examining pre-existing stopword lists. We use **quanteda**'s default Showball stopword list.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of stopwords in the English Snowball stopword list\nlength(quanteda::stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 175\n```\n:::\n\n```{.r .cell-code}\n# first 5 stopwords of of English Snowball stopword list\nhead(quanteda::stopwords(\"en\"), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"i\"      \"me\"     \"my\"     \"myself\" \"we\"    \n```\n:::\n\n```{.r .cell-code}\n# default German Snowball stopword list\nlength(quanteda::stopwords(\"de\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 231\n```\n:::\n\n```{.r .cell-code}\n# first 5 stopwords of German Snowball stopword list\nhead(quanteda::stopwords(\"de\"), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"aber\"  \"alle\"  \"allem\" \"allen\" \"aller\"\n```\n:::\n:::\n\n\n\nWe can also use other lists included in the **stopwords** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the stopwords package\nlibrary(stopwords)\n\n# check the first five stopwords from English ISO code list\n# (note that list includes numbers)\nhead(stopwords::stopwords(\"en\", source = \"stopwords-iso\"), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"'ll\"   \"'tis\"  \"'twas\" \"'ve\"   \"10\"   \n```\n:::\n:::\n\n\n\nFinally, you can create your own list of stopwords adding stopwords in a character vector. The short `my_stopwords` list below is for illustration purposes only since many custom lists will be considerably longer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_stopwords <- c(\"a\", \"an\", \"the\")\n```\n:::\n\n\n\nIn the next step, we apply various stopword lists to our tokens object using `tokens_select(selection = \"remove\")` and the wrapper function `tokens_remove()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove English stopwords and inspect output\ntokens(txt) |>\n    tokens_select(\n        pattern = quanteda::stopwords(\"en\"),\n        selection = \"remove\"\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"tokens\"   \"(\"        \")\"        \"works\"   \n[8] \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"#textanalysis\"        \"https://quanteda.org\"\n```\n:::\n\n```{.r .cell-code}\n# the following code is equivalent\ntokens(txt) |>\n    tokens_remove(pattern = quanteda::stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"tokens\"   \"(\"        \")\"        \"works\"   \n[8] \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"#textanalysis\"        \"https://quanteda.org\"\n```\n:::\n\n```{.r .cell-code}\n# remove patterns that match the custom stopword list\ntokens(txt) |>\n    tokens_remove(pattern = my_stopwords)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"   \"(\"        \")\"       \n[8] \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n:::\n\n\n\n::: {.callout-note appearance=\"simple\"}\nBy default, `tokens_select()` and the wrappers `tokens_remove()` and `tokens_keep()` are case insensitive, meaning the tokens in lowercase and uppercase will be removed if it matches a pattern. If we set `case_insensitive` to `FALSE`, the selection is sensitive to the spelling in uppercase or lowercase.\n\nIn the examples above, the tokens `A` is removed when we set `case_insensitive = TRUE` but it remains in the tokens object when `case_insensitive = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# stopword removal is case-insensitive\ntokens(txt) |>\n    tokens_remove(pattern = my_stopwords, case_insensitive = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"   \"(\"        \")\"       \n[8] \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n\n```{.r .cell-code}\n# stopword removal is case-sensitive\ntokens(txt) |>\n    tokens_remove(pattern = my_stopwords, case_insensitive = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n```\n:::\n:::\n\n\n:::\n\n\n## Stemming\n\nThe **quanteda** packages includes the function `tokens_wordstem()`, a wrapper around `wordStem` from the SnowballC package. The function uses Martin Porter's [@porter01snowball] algorithm described above. The example below shows how `tokens_wordstem()` adjust various words.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# example applied to tokens\ntxt <- c(\n    one = \"eating eater eaters eats ate\",\n    two = \"taxing taxis taxes taxed my tax return\"\n)\n\n# create tokens object\ntokens(txt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\none :\n[1] \"eating\" \"eater\"  \"eaters\" \"eats\"   \"ate\"   \n\ntwo :\n[1] \"taxing\" \"taxis\"  \"taxes\"  \"taxed\"  \"my\"     \"tax\"    \"return\"\n```\n:::\n\n```{.r .cell-code}\n# create tokens object and stem tokens\ntxt |>\n    tokens() |>\n    tokens_wordstem()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\none :\n[1] \"eat\"   \"eater\" \"eater\" \"eat\"   \"ate\"  \n\ntwo :\n[1] \"tax\"    \"taxi\"   \"tax\"    \"tax\"    \"my\"     \"tax\"    \"return\"\n```\n:::\n:::\n\n\n\nLemmatisation is more complex than stemming since it does not rely on pre-defined rules. The **spacyr** package allows you to lemmatise a text corpus. We describe lemmatisation in the Advanced section below.\n\n## Advanced\n\n### Applying Different Tokenisers\n\n**quanteda** contains several tokenisers, which can be applied in `tokens()`. Moreover, you can apply tokenisers included in other packages.\n\nThe current default tokeniser is `word3` included in **quanteda** version 3 and above. For forward compatibility including use of a more advanced tokeniser that will be used in major version 4, there is also a `word4` tokeniser that is even smarter than the defaults. You can apply the different tokenisers by specifying the `word` argument in `tokens()`.\n\nThe **tokenizers** package includes additional tokenisers [@tokenizers]. These tokenisers can also be applied and transformed to a **quanteda** tokens object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the tokenizers package\nlibrary(tokenizers)\n\n# tokenisation without processing\ntokenizers::tokenize_words(txt) %>%\n    tokens()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\none :\n[1] \"eating\" \"eater\"  \"eaters\" \"eats\"   \"ate\"   \n\ntwo :\n[1] \"taxing\" \"taxis\"  \"taxes\"  \"taxed\"  \"my\"     \"tax\"    \"return\"\n```\n:::\n\n```{.r .cell-code}\n# tokenisation with processing in both functions\ntokenizers::tokenize_words(txt, lowercase = FALSE, strip_punct = FALSE) %>%\n    tokens(remove_symbols = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\none :\n[1] \"eating\" \"eater\"  \"eaters\" \"eats\"   \"ate\"   \n\ntwo :\n[1] \"taxing\" \"taxis\"  \"taxes\"  \"taxed\"  \"my\"     \"tax\"    \"return\"\n```\n:::\n:::\n\n\n\n### Lemmatisation\n\nWhile stemming works directly in **quanteda** using `tokens_wordstem()`, lemmatisation, i.e., changing tokens to its base form, requires different packages. You can use the **spacyr** package, a wrapper around the **spaCy** Python library, to lemmatise a **quanteda** tokens object. Note that you will need to install Python and a virtual environment to use the **spaCy** package.^[Detailed instructions are provided  at [http://spacyr.quanteda.io](http://spacyr.quanteda.io) and @sec-appendix-installing.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load spacyr package\nlibrary(spacyr)\n\n# use spacy_install() to install spacy in a new or existing\n# virtual environment. Check ?spacy_install() for details\n\n# initialise and use English language model\nspacy_initialize(model = \"en_core_web_sm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFound 'spacy_condaenv'. spacyr will use this environment\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nsuccessfully initialized (spaCy Version: 3.6.1, language model: en_core_web_sm)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n(python options: type = \"condaenv\", value = \"spacy_condaenv\")\n```\n:::\n\n```{.r .cell-code}\ntxt_compare <- c(\n    one = \"The cats are running quickly.\",\n    two = \"The geese were flying overhead.\"\n)\n\n# parse texts, return, part-of-speech and lemma\ntoks_spacy <- spacy_parse(txt_compare, pos = TRUE, lemma = TRUE)\n\n# show first 10 tokens, which are stored as a data frame\nhead(toks_spacy, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   doc_id sentence_id token_id   token   lemma   pos entity\n1     one           1        1     The     the   DET       \n2     one           1        2    cats     cat  NOUN       \n3     one           1        3     are      be   AUX       \n4     one           1        4 running     run  VERB       \n5     one           1        5 quickly quickly   ADV       \n6     one           1        6       .       . PUNCT       \n7     two           1        1     The     the   DET       \n8     two           1        2   geese   geese  NOUN NORP_B\n9     two           1        3    were      be   AUX       \n10    two           1        4  flying     fly  VERB       \n```\n:::\n\n```{.r .cell-code}\n# transform object to a quanteda tokens object and use lemma\nas.tokens(toks_spacy, use_lemma = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\none :\n[1] \"the\"     \"cat\"     \"be\"      \"run\"     \"quickly\" \".\"      \n\ntwo :\n[1] \"the\"      \"geese\"    \"be\"       \"fly\"      \"overhead\" \".\"       \n```\n:::\n\n```{.r .cell-code}\n# compare with Snowball stemmer\ntxt_compare |>\n    tokens() |>\n    tokens_wordstem()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents.\none :\n[1] \"The\"   \"cat\"   \"are\"   \"run\"   \"quick\" \".\"    \n\ntwo :\n[1] \"The\"      \"gees\"     \"were\"     \"fli\"      \"overhead\" \".\"       \n```\n:::\n:::\n\n\n\nThe code above highlights the differences between stemming and lemmatisation. Stemming can truncate words, resulting in non-real words. Lemmatisation reduces words to their canonical, valid form.\nThe word `flying` is stemmed to `fli`, while the lemmatiser changes the word to its base form, `fly`.\n\n### Modifying Stopword Lists\n\nIn many cases, you might want to use an existing stopword list but remove or add certain features.\nYou can use **quanteda**'s `char_remove()` and base R's `c()` function to remove or add features. The examples below show how to remove features from the default English stopword list.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check if \"will\" is included in default stopword list\n\"will\" %in% stopwords(\"en\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# remove \"will\" and store output as new stopword list\nstopw_reduced <- char_remove(stopwords(\"en\"), pattern = \"will\")\n\n# check whether \"will\" was removed\n\"will\" %in% stopw_reduced\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\n\nWe use `c()` from base R to add words to stopword lists. For example, the feature `further` is included in the default English stopword list, but `furthermore` and `therefore` are not included. Let's add both terms.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check if terms are included in stopword list\nc(\"furthermore\", \"therefore\") %in% stopwords(\"en\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE FALSE\n```\n:::\n\n```{.r .cell-code}\n# extend stopword list\nstop_extended <- c(stopwords(\"en\"), \"furthermore\", \"therefore\")\n\n# check the last parts of the character vector\ntail(stop_extended)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"than\"        \"too\"         \"very\"        \"will\"        \"furthermore\"\n[6] \"therefore\"  \n```\n:::\n:::\n\n\n\nAs discussed above, tokenisation and processing involves many steps, and we can combine these steps using the base R pipe (`|>`). The example below shows a typical workflow.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise data_corpus_inaugural,\n# remove punctuation and numbers,\n# remove stopwords,\n# stem the tokens,\n# and transform object to lowercase\n\ntoks_inaugural <- data_corpus_inaugural |>\n    tokens(remove_punct = TRUE, remove_numbers = TRUE) |>\n    tokens_remove(pattern = stopwords(\"en\")) |>\n    tokens_wordstem() |>\n    tokens_tolower()\n\n# inspect first tokens from the first two speeches\nhead(toks_inaugural, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2 documents and 4 docvars.\n1789-Washington :\n [1] \"fellow-citizen\" \"senat\"          \"hous\"           \"repres\"        \n [5] \"among\"          \"vicissitud\"     \"incid\"          \"life\"          \n [9] \"event\"          \"fill\"           \"greater\"        \"anxieti\"       \n[ ... and 640 more ]\n\n1793-Washington :\n [1] \"fellow\"   \"citizen\"  \"call\"     \"upon\"     \"voic\"     \"countri\" \n [7] \"execut\"   \"function\" \"chief\"    \"magistr\"  \"occas\"    \"proper\"  \n[ ... and 50 more ]\n```\n:::\n:::\n\n\n::: {.callout-warning appearance=\"simple\"}\nThe sequence of processing steps during the tokensation is important. For example, if we first stem our tokens and remove stopwords or specific patterns afterwards, we might not remove all desired features. Consider the following example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntxt <- \"During my stay in London I visited the museum\nand attended a very good concert.\"\n\n# remove stopwords before stemming tokens\ntokens(txt, remove_punct = TRUE) %>%\n    tokens_remove(stopwords(\"en\")) %>%\n    tokens_wordstem()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"stay\"    \"London\"  \"visit\"   \"museum\"  \"attend\"  \"good\"    \"concert\"\n```\n:::\n\n```{.r .cell-code}\n# stem tokens before removing stopwords\ntokens(txt, remove_punct = TRUE) %>%\n    tokens_wordstem() %>%\n    tokens_remove(stopwords(\"en\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"Dure\"    \"stay\"    \"London\"  \"visit\"   \"museum\"  \"attend\"  \"veri\"   \n[8] \"good\"    \"concert\"\n```\n:::\n:::\n\n\n\nThe first example produces what most users want: it removes all terms from our stopword list (`during`, `my`, `I`, `the`, `and`, `a`, `very`), while the second example first stems `During` to `dure` and `very` to `veri`, which changes the terms to tokens that are not included in  `stopwords(\"en\")` (and therefore remain in the tokens object).\n:::\n\n\n### Managing Document-Level Variables and Metadata\n\nBy default, tokens object contain the document-level variables and the metadata assigned to your corpus. You can access or modify these variables in the same way as we did in @sec-quanteda-corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise US inaugural speeches\ntoks_inaugural <- tokens(data_corpus_inaugural)\n\n# add document level variable\ntoks_inaugural$post_1990 <- ifelse(\n    toks_inaugural$Year > 1990, \"Post-1990\", \"Pre-1990\"\n)\n\n# inspect new document-level variable\ntable(toks_inaugural$post_1990)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPost-1990  Pre-1990 \n        8        51 \n```\n:::\n:::\n\n\n\n## Further Reading\n\n- The concept of tokenisation and how to build a custom tokeniser: @hvitfeldt21ml [ch. 2]\n- The intuition behind processing and tokenising texts: @grimmer22textasdata  [ch. 5.3]\n- Introduction to the **tokenizers** package: @tokenizers\n- How processing decisions can influence results: @denny18preprocess\n\n## Exercises\n\nAdd some here.\n",
    "supporting": [
      "10-quanteda-tokens_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}