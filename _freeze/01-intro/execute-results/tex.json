{
  "hash": "bb126e9bec7887f874a7c47e1dce525b",
  "result": {
    "markdown": "\\pagenumbering{arabic}\n\n# Introduction and Motivation {#sec-intro}\n\n## Objectives\n\nTo dive right in to text analysis using R, by means of an extended demonstration of how to acquire, process, quantify, and analyze a series of texts. The objective is to demonstrate the techniques covered in the book and the tools required to apply them.\n\nThis overview is intended to serve more as a demonstration of the kinds of natural language processing and text analysis that can be done powerfully and easily in R, rather than as primary instructional material. Starting in Chapter \\@ref(rbasics), you learn about the R fundamentals and work our way gradually up to basic and then more advanced text analysis. Until then, enjoy the demonstration, and don't be discouraged if it seems too advanced to follow at this stage. By the time you have finished this book, this sort of analysis will be very familiar.\n\n## Application: Analyzing Candidate Debates from the US Presidential Election Campaign of 2020\n\nThe methods demonstrated in this chapter include:\n\n-   acquiring texts directly from the world-wide web;\n-   creating a corpus from the texts, with associated document-level variables;\n-   segmenting the texts by sections found in each document;\n-   cleaning up the texts further;\n-   tokenizing the texts into words;\n-   summarizing the texts in terms of who spoke and how much;\n-   examining keywords-in-context from the texts, and identifying keywords using statistical association measures;\n-   transforming and selecting features using lower-casing, stemming, and removing punctuation and numbers;\n-   removing selected features in the form of \"stop words\";\n-   creating a document-feature matrix from the tokens;\n-   performing sentiment analysis on the texts;\n-   fitting topic models on the texts.\n\nFor demonstration, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.[^01-intro-1]\n\n[^01-intro-1]: The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0.\n\n### Acquiring text directly from the world wide web.\n\nFull transcripts of both televized debates are available from [The American Presidency Project](https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/debates). We start our demonstration by scraping the debates using the **rvest** package [@rvest]. You will learn more about scraping data from the internet in Chapter \\@ref(acquire-internet). We first need to install and load the packages required for this demonstration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install packages\ninstall.packages(\"quanteda\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\ninstall.packages(\"rvest\")\ninstall.packages(\"stringr\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"quanteda/quanteda.tidy\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(\"quanteda\")\nlibrary(\"rvest\")\nlibrary(\"stringr\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.tidy\")\n```\n:::\n\n\nNext, we identify the presidential debates for the 2020 period, assign the URL of the debates to an object, and load the source page. We retrieve metadata (the location and dates of the debates), store the information as a data frame, and get the URL debates. Finally, we scrape the search results and save the texts as a character vector, with one element per debate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search presidential debates for the 2020 period\n# https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50\n\n# assign URL of debates to an object called url_debates\nurl_debates <- \"https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50\"\n\nsource_page <- read_html(url_debates)\n\n# get debate meta-data\n\nnodes_pres <- \".views-field-title a\"\ntext_pres <- \".views-field-field-docs-start-date-time-value.text-nowrap\"\n\ndebates_meta <- data.frame(\n    location = html_text(html_nodes(source_page, nodes_pres)),\n    date =  html_text(html_nodes(source_page, text_pres)),\n    stringsAsFactors = FALSE\n)\n\n# format the date\ndebates_meta$date <- as.Date(trimws(debates_meta$date), \n                             format = \"%b %d, %Y\")\n\n# get debate URLs\ndebates_links <- source_page |> \n    html_nodes(\".views-field-title a\") |> \n    html_attr(name = \"href\") \n\n# add first part of URL to debate links\ndebates_links <- paste0(\"https://www.presidency.ucsb.edu\", debates_links)\n\n# scrape search results\ndebates_scraped <- lapply(debates_links, read_html)\n\n# get character vector, one element per debate\ndebates_text <- sapply(debates_scraped, function(x) {\n    html_nodes(x, \"p\") |> \n        html_text() |>\n        paste(collapse = \"\\n\\n\")\n})\n```\n:::\n\n\nHaving retrieved the text of the two debates, we clean up the character vector. More specifically, we clean up the details on the location using regular expressions and the **stringr** package (see Chapter \\@ref(appendix-regex)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndebates_meta$location <- str_replace(debates_meta$location, \"^.* in \", \"\")\n```\n:::\n\n\n### Creating a text corpus\n\nNow we have two objects. The character vector `debates_text` contains the text of both debates, and the data frame `debates_meta` stores the date and location of both debates. These objects allow us to create a **quanteda** corpus, with the metadata. We use the `corpus()` function, specify the location of the document-level variables, and assign the corpus to an object called `data_corpus_debates`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debates <- corpus(debates_text, \n                              docvars = debates_meta)\n\n# check the number of documents included in the text corpus\nndoc(data_corpus_debates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nThe object `data_corpus_debates` contains two documents, one for each debate. While this unit of analysis may be suitable for some analyses, we want to identify all utterances by the moderator and the two candidates. With the function `corpus_segment()`, we can segment the corpus into statements. The unit of analysis changes from a full debate to a statement during a debate. Inspecting the page for one of the debates[^01-intro-2] reveals that a new utterance starts with the speaker's name in ALL CAPS, followed by a colon. We use this consistent pattern for segmenting the corpus to the level of utterances. The regular expression `\"\\\\s*[[:upper:]]+:\\\\s+\"` identifies speaker names in ALL CAPS (`\\\\s*[[:upper:]]+`), followed by a colon `+:` and a white space `\\\\s+`.\n\n[^01-intro-2]: https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatesseg <- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\nsummary(data_corpus_debatesseg, n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1207 documents, showing 4 documents:\n\n    Text Types Tokens Sentences        location       date       pattern\n text1.1   139    251        13 Cleveland, Ohio 2020-09-29 \\n\\nWALLACE: \n text1.2     6      6         1 Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n text1.3     5      5         1 Cleveland, Ohio 2020-09-29   \\n\\nTRUMP: \n text1.4     3      3         1 Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n```\n:::\n:::\n\n\nThe segmentation results in a new corpus consisting of 1,207 utterances. The `summary()` function provides a useful overview of each utterance. The first text, for example, contains 251 tokens, 139 types (i.e., unique tokens) and 13 sentences.\n\nHaving segmented the corpus, we improve the document-level variables since such meta information on each document is crucial for subsequent analyses like subsetting the corpus or grouping the corpus to the level of speakers. We create a new document-level variable called \"speaker\" based on the pattern extracted above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# str_trim() removes empty whitespaces, \n# str_remove_all() removes the colons,\n# and str_to_title() changes speaker names \n# from UPPER CASE to Title Case\n\ndata_corpus_debatesseg <- data_corpus_debatesseg |> \n    rename(speaker = pattern) |> \n    mutate(speaker = str_trim(speaker),\n           speaker = str_remove_all(speaker, \":\"),\n           speaker = str_to_title(speaker)) \n\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$speaker,\n      data_corpus_debatesseg$location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         \n          Cleveland, Ohio Nashville, Tennessee\n  Biden               269                   84\n  Trump               340                  122\n  Wallace             246                    0\n  Welker                0                  146\n```\n:::\n:::\n\n\nThe cross-table reports the number of statement by each speaker in each debate. The first debate in Cleveland seems to be longer: the number of Trump's and Biden's statements during the Cleveland debates are three times higher than those in Tennessee. The transcript reports 246 utterances by Chris Wallace during the first and 146 by Kristen Welker during the second debate. We can inspect all cleaned document-level variables in this text corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatesseg |> \n    docvars() |> \n    glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,207\nColumns: 3\n$ location <chr> \"Cleveland, Ohio\", \"Cleveland, Ohio\", \"Cleveland, Ohio\", \"Cle~\n$ date     <date> 2020-09-29, 2020-09-29, 2020-09-29, 2020-09-29, 2020-09-29, ~\n$ speaker  <chr> \"Wallace\", \"Biden\", \"Trump\", \"Biden\", \"Wallace\", \"Trump\", \"Wa~\n```\n:::\n:::\n\n\n### Tokenizing a corpus\n\nNext, we tokenize our text corpus. Typically, tokenization involves separating texts by white spaces. We tokenize the text corpus without any pre-processing using `tokens()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_usdebates2020 <- tokens(data_corpus_debatesseg)\n\n# let's inspect the first six tokens of the first four documents\nprint(toks_usdebates2020, max_ndoc = 4, max_ntoken = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1,207 documents and 3 docvars.\ntext1.1 :\n[1] \"Good\"      \"evening\"   \"from\"      \"the\"       \"Health\"    \"Education\"\n[ ... and 245 more ]\n\ntext1.2 :\n[1] \"How\"   \"you\"   \"doing\" \",\"     \"man\"   \"?\"    \n\ntext1.3 :\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"?\"    \n\ntext1.4 :\n[1] \"I'm\"  \"well\" \".\"   \n\n[ reached max_ndoc ... 1,203 more documents ]\n```\n:::\n\n```{.r .cell-code}\ntokens(data_corpus_debatesseg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1,207 documents and 3 docvars.\ntext1.1 :\n [1] \"Good\"       \"evening\"    \"from\"       \"the\"        \"Health\"    \n [6] \"Education\"  \"Campus\"     \"of\"         \"Case\"       \"Western\"   \n[11] \"Reserve\"    \"University\"\n[ ... and 239 more ]\n\ntext1.2 :\n[1] \"How\"   \"you\"   \"doing\" \",\"     \"man\"   \"?\"    \n\ntext1.3 :\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"?\"    \n\ntext1.4 :\n[1] \"I'm\"  \"well\" \".\"   \n\ntext1.5 :\n [1] \"Gentlemen\" \",\"         \"a\"         \"lot\"       \"of\"        \"people\"   \n [7] \"been\"      \"waiting\"   \"for\"       \"this\"      \"night\"     \",\"        \n[ ... and 137 more ]\n\ntext1.6 :\n [1] \"Thank\" \"you\"   \"very\"  \"much\"  \",\"     \"Chris\" \".\"     \"I\"     \"will\" \n[10] \"tell\"  \"you\"   \"very\" \n[ ... and 282 more ]\n\n[ reached max_ndoc ... 1,201 more documents ]\n```\n:::\n\n```{.r .cell-code}\n# check number of tokens and types\ntoks_usdebates2020 |> \n    ntoken() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 43742\n```\n:::\n\n```{.r .cell-code}\ntoks_usdebates2020 |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28174\n```\n:::\n:::\n\n\nWithout any pre-processing, the corpus consists of 43,742 tokens and 28,174 types. We can easily check how these numbers change when transforming all tokens to lowercase and removing punctuation characters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_usdebates2020_reduced <- toks_usdebates2020 |> \n    tokens(remove_punct = TRUE) |> \n    tokens_tolower()\n\n# check number of tokens and types\ntoks_usdebates2020_reduced |> \n    ntoken() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 36740\n```\n:::\n\n```{.r .cell-code}\ntoks_usdebates2020_reduced |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24965\n```\n:::\n:::\n\n\nThe number of tokens and types decreases to 36,740 and 28,174, respectively, after removing punctuation and harmonizing all terms to lowercase.\n\n### Keywords-in-context\n\nIn contrast to a document-feature matrix (covered below), tokens objects still preserve the order of words. We can use tokens objects to identify the occurrence of keywords and their immediate context.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkw_america <- kwic(toks_usdebates2020, \n                   pattern = c(\"america\"),\n                   window = 2)\n\n# number of mentions\nnrow(kw_america)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18\n```\n:::\n\n```{.r .cell-code}\n# print first 6 mentions of America and the context of ±2 words\nhead(kw_america, n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyword-in-context with 6 matches.                                                     \n [text1.284, 66]        towns in | America | , how   \n [text1.290, 80]       states in | America | with a  \n [text1.290, 96]       States of | America | , and   \n  [text1.335, 5] worst president | America | has ever\n [text1.489, 37]        whole of | America | . But   \n [text1.500, 53]      applied in | America | .\"      \n```\n:::\n:::\n\n\n### Text processing\n\nThe keywords-in-context analysis above reveals that all terms are still in upper case and that very frequent, uninformative words (so-called stopwords) and punctuation are still part of the text. In most applications, we remove very frequent features and transform all words to lowercase. The code below shows how to adjust the object accordingly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_usdebates2020_processed <- data_corpus_debatesseg |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\")) |> \n    tokens_tolower()\n```\n:::\n\n\nLet's inspect if the changes have been implemented as we expect by calling `kwic()` on the new tokens object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkw_america_processed <- kwic(toks_usdebates2020_processed, \n                             pattern = c(\"america\"),\n                             window = 2)\n\n# print first 6 mentions of America and the context of ±2 words\nhead(kw_america_processed, n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyword-in-context with 6 matches.                                                                 \n [text1.284, 32]     class towns | america | well guy            \n [text1.290, 29]     half states | america | significant increase\n [text1.290, 38]   united states | america | wants open          \n  [text1.335, 3] worst president | america | ever come           \n [text1.489, 15]  equality whole | america | never accomplished  \n [text1.500, 25] equally applied | america | believe separate    \n```\n:::\n\n```{.r .cell-code}\n# test: print as table+\nlibrary(kableExtra)\nkw_america_processed |> data.frame() |> \n  dplyr::select(Pre = pre, Keyword = keyword, Post = post, Pattern = pattern) |>  \n  kbl(booktabs = T) %>%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"), html_font = \"Source Sans Pro\", full_width = F)\n```\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}[t]{llll}\n\\toprule\nPre & Keyword & Post & Pattern\\\\\n\\midrule\n\\cellcolor{gray!6}{class towns} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{well guy} & \\cellcolor{gray!6}{america}\\\\\nhalf states & america & significant increase & america\\\\\n\\cellcolor{gray!6}{united states} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{wants open} & \\cellcolor{gray!6}{america}\\\\\nworst president & america & ever come & america\\\\\n\\cellcolor{gray!6}{equality whole} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{never accomplished} & \\cellcolor{gray!6}{america}\\\\\n\\addlinespace\nequally applied & america & believe separate & america\\\\\n\\cellcolor{gray!6}{defeat racism} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{} & \\cellcolor{gray!6}{america}\\\\\nincrease homicides & america & summer particularly & america\\\\\n\\cellcolor{gray!6}{less violence} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{today president} & \\cellcolor{gray!6}{america}\\\\\nfired plant & america & one's going & america\\\\\n\\addlinespace\n\\cellcolor{gray!6}{fire plant} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{going move} & \\cellcolor{gray!6}{america}\\\\\nunited states & america & situation thousands & america\\\\\n\\cellcolor{gray!6}{every company} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{blow away} & \\cellcolor{gray!6}{america}\\\\\nunited states & america &  & america\\\\\n\\cellcolor{gray!6}{united states} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{anybody seeking} & \\cellcolor{gray!6}{america}\\\\\n\\addlinespace\nsection race & america & want talk & america\\\\\n\\cellcolor{gray!6}{institutional racism} & \\cellcolor{gray!6}{america} & \\cellcolor{gray!6}{always said} & \\cellcolor{gray!6}{america}\\\\\ngrowing industry & america & electric excuse & america\\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table}\n:::\n:::\n\n\nThe processing of the tokens object worked as expected. Let's imagine we want to group the documents by debate and speaker, resulting in two documents for Trump, two for Biden, and one for each moderator. `tokens_group()` allows us to change the unit of analysis. After aggregating the documents, we can use the function `textplot_xray()` to observe the occurrences of specific keywords during the debates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# new document-level variable with date and speaker\ntoks_usdebates2020$speaker_date <- paste(\n    toks_usdebates2020$speaker,\n    toks_usdebates2020$date,\n    sep = \", \")\n\n# reshape the tokens object to speaker-date level \n# and keep only Trump and Biden\ntoks_usdebates2020_grouped <- toks_usdebates2020 |> \n    tokens_subset(speaker %in% c(\"Trump\", \"Biden\")) |> \n    tokens_group(groups = speaker_date)\n\n# check number of documents\nndoc(toks_usdebates2020_grouped)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\n# use absolute position of the token in the document\ntextplot_xray(\n    kwic(toks_usdebates2020_grouped, pattern = \"america\"), \n    kwic(toks_usdebates2020_grouped, pattern = \"tax*\")\n)\n```\n\n::: {.cell-output-display}\n![](01-intro_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe grouped document also allows us to check how often each candidate spoke during the debate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nntoken(toks_usdebates2020_grouped)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBiden, 2020-09-29 Biden, 2020-10-22 Trump, 2020-09-29 Trump, 2020-10-22 \n             7996              8093              8973              9016 \n```\n:::\n:::\n\n\nThe number of tokens between Trump and Biden do not differ substantively in both debates. Trump's share of speech is only slightly higher than Biden's (7996 v 8093 tokens; 8973 v 9016 tokens).\n\n### Identifying multiword expressions\n\nMany languages build on multiword expressions. For instance, \"income\" and \"tax\" as separate unigrams have a different meaning than the bigram \"income tax\". The package **quanteda.textstats** includes the function `textstat_collocation()` that automatically retrieves common multiword expressions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntstat_coll <- data_corpus_debatesseg |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\"), padding = TRUE) |> \n    textstat_collocations(size = 2:3, min_count = 5)\n\n# for illustration purposes select the first 20 collocations\nhead(tstat_coll, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       collocation count count_nested length   lambda        z\n1  president trump    71           44      2 6.928550 22.01175\n2        make sure    30            4      2 7.439572 21.53052\n3  president biden    52           52      2 6.450825 20.63073\n4     mr president    34           21      2 5.389748 19.78036\n5      health care    20           15      2 7.580147 19.32484\n6       number one    18           16      2 5.595991 17.94433\n7        right now    18           12      2 4.801444 16.82701\n8       number two    15           11      2 5.472508 16.69323\n9     half million    15           13      2 6.852610 16.55670\n10         mr vice    15           15      2 5.323919 16.52522\n11      four years    19           15      2 7.433288 16.49838\n12 american people    22            4      2 4.969408 16.44720\n13     two minutes    28           19      2 8.504176 16.09104\n14       come back    12            7      2 5.537489 15.46566\n15     three years    12            5      2 5.464293 15.20941\n16      one number    12           12      2 5.122270 14.65987\n17  climate change    11            8      2 8.963835 14.65614\n18  million people    18           18      2 4.073645 14.42229\n19  final question    14            7      2 7.008316 14.40121\n20  vice president    99           81      2 9.070192 14.05817\n```\n:::\n:::\n\n\nWe can use `tokens_compound()` to compound certain multiword expressions before creating a document-feature matrix which does not consider word order. For illustration purposes, we compound `climate change` ,`social securit*`, and `health insurance*`. By default, compounded tokens are concatenated by `_`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_usdebates2020_comp <- toks_usdebates2020 |> \n    tokens(remove_punct = TRUE) |> \n    tokens_compound(pattern = phrase(c(\"climate change\",\n                                       \"social securit*\",\n                                       \"health insuranc*\"))) |> \n    tokens_remove(pattern = stopwords(\"en\"))\n```\n:::\n\n\n### Document-feature matrix\n\nWe have come a long way already. We downloaded debate transcripts, segmented the texts to utterances, added document-level variables, tokenized the corpus, inspected keywords, and compounded multiword expressions. Next, we transform our tokens object into a document-feature matrix (dfm). A dfm counts the occurrences of tokens in each document. We can create a document feature matrix, print the structure, and get the most frequent words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfmat_presdebates20 <- dfm(toks_usdebates2020_comp)\n\n# most frequent features\ntopfeatures(dfmat_presdebates20, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npresident     going    people      said      know      want       get       say \n      288       278       259       164       139       129       119       114 \n     look      vice \n      105       101 \n```\n:::\n\n```{.r .cell-code}\n# most frequent features by speaker\n\ntopfeatures(dfmat_presdebates20, groups = speaker, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$Biden\n    going    people      fact      said       get      make      know president \n      126       119        72        62        54        46        46        45 \n     sure       can \n       44        43 \n\n$Trump\npeople  going   know   said   look   want    joe   done    say  think \n   111     92     85     77     65     62     55     55     54     52 \n\n$Wallace\npresident       sir     going  question        mr      vice     trump        go \n      110        56        44        39        39        36        33        30 \n      two     right \n       28        25 \n\n$Welker\npresident      vice     trump     biden  question     right       let      move \n      100        52        39        35        31        28        28        24 \n     talk      want \n       21        18 \n```\n:::\n:::\n\n\nMany methods build on document-feature matrices and the \"bag-of-words\" approach. In this section, we introduce `textstat_keyness()`, which identifies features that occur differentially across different categories -- in our case, Trump's and Biden's utterances. The function `textplot_keyness()` provides a straightforward way of visualize the results of the keyness analysis (Figure \\ref@(fig:keynesspres).)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntstat_key <- dfmat_presdebates20 |>\n    dfm_subset(speaker %in% c(\"Trump\", \"Biden\")) |> \n    dfm_group(groups = speaker) |> \n    textstat_keyness(target = \"Trump\")\n\ntextplot_keyness(tstat_key)\n```\n\n::: {.cell-output-display}\n![](01-intro_files/figure-pdf/keynesspres-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Bringing it all together: Sentiment analysis\n\nBefore introducing the R Basics in the next chapter, we show how to conduct a dictionary-based sentiment analysis using the Lexicoder Sentiment Dictionary [@young2012]. The dictionary, included in **quanteda** as `data_dictionary_LSD2015` contains 1709 positive and 2858 negative terms (as well as their negations). We discuss advanced sentiment analyses with measures of uncertainty in Chapter ??.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_sent <- toks_usdebates2020 |> \n    tokens_group(groups = speaker) |> \n    tokens_lookup(dictionary = data_dictionary_LSD2015,\n                  nested_scope = \"dictionary\")\n\n# create a dfm with the count of matches,\n# transform object into a data frame,\n# and add document-level variables\ndat_sent <- toks_sent |> \n    dfm() |> \n    convert(to = \"data.frame\") |> \n    cbind(docvars(toks_sent))\n\n# select Trump and Biden and aggregate sentiment\ndat_sent$sentiment = with(\n    dat_sent, \n    log((positive + neg_negative + 0.5) /  \n            (negative + neg_positive + 0.5)))\n\ndat_sent\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   doc_id negative positive neg_positive neg_negative speaker   sentiment\n1   Biden      357      465           33            8   Biden  0.19272394\n2   Trump      430      509            7            2   Trump  0.15627088\n3 Wallace      136      185            0            4 Wallace  0.32806441\n4  Welker      106      102            0            0  Welker -0.03828219\n```\n:::\n:::\n\n\n## Issues\n\nSome issues raised from the example, where we might have done things differently or added additional analysis.\n\n## Further Reading\n\nSome studies of debate transcripts? Or issues involved in analyzing interactive or \"dialogical\" documents.\n\n## Exercises\n\nAdd some here.\n",
    "supporting": [
      "01-intro_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}