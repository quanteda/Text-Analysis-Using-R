{
  "hash": "c4b349c329c4a8c06e8662bc0748e799",
  "result": {
    "markdown": "# Advanced Token Manipulation {#sec-quanteda-tokensadvanced}\n\n## Objectives\n\nThe previous chapter introduced you to the basics of tokenisation and processing of tokens objects. Now we move to advanced token manipulations. We explain why we often want to compound multi-word expressions into a single token. Afterwards, we show how to replace tokens and introduce n-grams and skip-grams. We also explain why you might want to keep specific tokens and a context window around these tokens, and close the chapter with a brief introduction to lookup functions, which we cover much more extensively in @sec-exploring-dictionaries.\n\n## Methods\n\nThe previous section treats all tokens as so-called unigrams. We separated tokens by spaces and did not combine two or more tokens that might form a multi-word expression. In languages with compound words, we do not need to pay much attention to **multi-word expresions**. For example, the German term \"Mehrwertsteuer\" is a single word in German, but its English equivalent consists of three words: \"value added tax\". Suppose we are interested in companies' or politicians' focus on different forms of taxation. In that case, we want to treat `value added tax` as a multi-word expression rather than three separate tokens `value`, `added`, `tax`. Identifying multi-word expressions is especially important for document-feature matrices (dfm) (@sec-quanteda-dfms), which contain the counts of features in each document. If we do not explicitly compound `value added tax`, the words will be included as three separate tokens in our dfm. Compounding the expression during the tokenisation process, will ensure that the dfm contains the compound noun `value_added_tax`.\n\nIn many cases, we know multi-word expressions through our domain knowledge. For example, in reviews about hotels in New York, we might want to compound `New_York`, `Madison_Square_Garden`, and `Wall_Street`. In parliamentary speeches, we want to compound party names: instead of treating the combination `green party` as separate tokens, we might prefer the multi-word expression `green party` before proceeding with our statistical analysis.\n\nIn many cases, researchers need to discover relevant multi-word expressions. We can use approaches such as **keywords-in-context** (@sec-exploring-kwic) to explore the context of specific words or conduct a **collocation analysis** to identify terms that tend to co-occur together automatically. We introduce these methods in _add reference to new chapter_. Having identified multi-word expressions, you can compound these collocations before continuing your textual analysis.\n\nOccasionally, you might want to **replace** certain tokens. For example, in political texts, `Labour` spelled usually implies a mention to the named-entity Labour Party, while `labour` refers to the noun. Country names is another example: if we want to understand mentions of the United States of America in UN General Debates, we could replace `US`, `USA`, and `United States` with `united_states_of_america`. Dictionaries, discussed in @sec-quanteda-dictionaries, are an effective tool for replacing tokens with an overarching \"key\", such as `united_states`.\n\nIn some applications, tokens sequences might reveal more information than individual tokens. Before transforming a tokens object to a dfm, many existing studies create n-grams or skipgrams. **N-grams** are sequences of \"n\" items. **Skip-grams** are variations of n-grams which pairs non-consecutive tokens. N-grams capture local word patterns, whereas skip-grams capture broader contexts within texts. The sentence `I loved my stay in New York.` would result in the following bi-grams (sequence of two tokens): `\"I_loved\"  \"loved_my\" \"my_stay\"  \"stay_in\"  \"in_New\" \"New_York\" \"York_.\"`. Skip-grams of size 2 with a distance of 0 and 1 would change the object to:  `\"I_loved\" \"I_my\" \"loved_my\" \"loved_time\" \"my_time\" \"my_in\"  \"time_in\" \"time_New\" \"in_New\" \"in_York\" \"New_York\" \"New_.\" \"York_.\"`. \n\nThese examples highlight advantages and shortcomings of n-grams and skip-grams. On the one hand, both approaches provide information about the context of each token. On the other hand, n-grams and skip-grams increase the number of types (i.e., unique tokens) in our corpus. For example, the number of types in the corpus of US inaugural speeches more than doubles when creating bi-grams rather than uni-grams and triples when creating tri-grams instead of uni-grams. Instead of creating bi-grams or tri-grams, manual or automated identification of meaningful multi-word expressions is often sufficient or even preferred over n-grams.\n\n\n\n\n\n**Keeping tokens and their context windows** is another effective---and sometimes underused---tokenisation operation. We can keep specific tokens and the words around these patterns to refine our research question and focus on specific aspects of our text corpus. Let's imagine the following example: we are working with all speeches delivered in parliament over a period of three decades and want to understand how parties' focus and positions about climate change have evolved. Most speeches in our corpus will focus on different policies or contain procedural language, but we could create a list of words and phrases relating to the environment, and keep these terms with a context of several words. This approach would allow for a \"targeted\" analysis. Instead of analysing the full text corpus, we narrowed down our documents to the parts relevant to our research question. For example, @lupia20nsf limit U.S Congressional speeches to sentences mentioning the National Science Foundation (NSF). Afterwards, the authors identify which of these context words distinguish Democrats from Republicans, and how the topics (@sec-ml-topicmodels) mentioned in these sentences are moderated by the party of a speaker. @rauh20eu extract mentions of European institutions and a context window of three sentences from speeches delivered by European prime ministers. In the next step, the authors measure speech complexity and sentiment in these statements on European institutions. Their results reveals that prime minister tend to speak more favourably about the European Union when they face a strong Eurosceptic challenger party.\n\nFinally, we briefly introduce the concept of **looking up** tokens. We match tokens against a predefined list. This approach requires researchers to develop \"dictionaries\", consisting of one or more \"keys\" or categories. Each of these keys, in turn, contains various patterns, usually words or multi-word expressions. A sentiment analysis, covered in @sec-exploring-dictionaries, often relies on lists of terms and phrases scored as \"positive\" and \"negative\" and involves looking up these tokens.\n\nClassifying topics, policy areas, or concepts can also be conducted with a \"lookup approach.\" For example, @gessler22immig create a dictionary of keywords and phrases related to immigration. Afterwards, the authors apply this dictionary to party press releases. The authors keep all documents containing keywords from their immigration dictionary. Their rule-based approach is more computationally efficient than supervised classification and produced valid results. Subsequent analyses apply scaling methods to this subset of immigration-related press releases to understand how the 2015 \"refugee crisis\" in Europe changed party positions on migration policy. @sec-exploring-dictionaries provides more details on creating and applying dictionaries.\n\n## Examples\n\nIn this section, we rely on short sentences and text corpora of political speeches and hotel reviews to explain how to compound tokens, how to replace them, how to create n-grams and skip-grams, and how to select tokens and their context. \n\n### Compounding Tokens\n\nWe start with compounding tokens and introducing `phrase()`. The `phrase()` function declares that an expression consists of multiple patterns and makes explicit that the elements should be used for matching multi-word sequences rather than individual matches to single words. It is vital to use `phrase()` in all functions involving multi-word expressions, including `tokens_compound()`.^[`tokens_lookup()`, which handles phrases internally, is an exception to the rule.] Let's explore how to compound the multi-word expressions \"social welfare\" and \"social security\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create tokens object for examples\ntoks_social <- tokens(\"We need to increase social welfare payments \n                      and improve social security.\")\n\n# compound the pattern \"social welfare\"\ntoks_social |> \n    tokens_compound(pattern = phrase(\"social welfare\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"             \"need\"           \"to\"             \"increase\"      \n [5] \"social_welfare\" \"payments\"       \"and\"            \"improve\"       \n [9] \"social\"         \"security\"       \".\"             \n```\n:::\n\n```{.r .cell-code}\n# note: compounding does not work without phrase()\ntoks_social |> \n    tokens_compound(pattern = \"social welfare\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"       \"need\"     \"to\"       \"increase\" \"social\"   \"welfare\" \n [7] \"payments\" \"and\"      \"improve\"  \"social\"   \"security\" \".\"       \n```\n:::\n:::\n\n\nWe can include several patterns in our character vector containing multi-word expressions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compound two patterns\ntokens_compound(toks_social,\n                pattern = phrase(c(\"social welfare\",\n                                   \"social security\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"              \"need\"            \"to\"              \"increase\"       \n [5] \"social_welfare\"  \"payments\"        \"and\"             \"improve\"        \n [9] \"social_security\" \".\"              \n```\n:::\n:::\n\n\n::: {.callout-tip appearance=\"simple\"}\nBy default, compounded tokens are concatenated using an underscore (`_`). The default is recommended since underscores will not be removed during normal cleaning and tokenisation. Using an underscore as a separator also allows you to check whether compounding worked as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check whether compounding worked as expected \n# by extracting patterns containing underscores\ntoks_social |> \n    tokens_compound(pattern = phrase(c(\"social welfare\", \n                                       \"social security\"))) |> \n    tokens_keep(pattern = \"*_*\") # keep patterns with underscores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"social_welfare\"  \"social_security\"\n```\n:::\n:::\n\n:::\n\nYou can also compound terms based on regular expressions (@sec-rstrings and @sec-appendix-regex) or \"wild card\" pattern matches. Below, we use the glob-style wildcard expression `*` to compound all multi-word expressions starting with \"social\" in US State of the Union speeches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise SOTU speeches, remove punctuation and numbers\n# before removing stopwords\ntoks_sotu <- TAUR::data_corpus_sotu |> \n    tokens(remove_punct = TRUE,\n           remove_numbers = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE)\n\n# compound all phrases starting with \"social\"\ntoks_sotu_comp <- tokens_compound(toks_sotu, pattern = phrase(\"social *\"))\n\n# spot-check results by keeping all tokens starting \n# with social using \"glob\"-style wildcard pattern match\n# and create dfm to check compounded terms\ntokens_keep(toks_sotu_comp, pattern = \"social_*\") |> \n    dfm() |> \n    topfeatures(n = 15) # get 15 most frequent compounded tokens\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    social_security     social_problems     social_services        social_needs \n                229                  16                  15                  13 \n        social_life        social_order     social_progress       social_system \n                 10                   9                   9                   7 \n     social_welfare     social_programs  social_intercourse    social_condition \n                  7                   7                   6                   6 \n      social_change    social_insurance social_institutions \n                  6                   6                   5 \n```\n:::\n:::\n\n\n## Replacing and Looking Up Tokens\n\nIn some cases, researchers may want to substitute tokens. Reasons to replace tokens include standardising terms, accounting for synonyms, acronyms, or fixing typographical errors. For example, it may be reasonable to harmonise \"EU\" and \"European Union\" in political texts. The function  `tokens_replace()` allows us to conduct one-to-one matching and replace EU with European Union.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_eu_uk <- tokens(\"The European Union negotiated with the UK.\")\n\n# important: use phrase if you want to detect a multi-word expression\ntokens_replace(toks_eu_uk, \n               pattern = phrase(\"European Union\"),\n               replacement = \"EU\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"        \"EU\"         \"negotiated\" \"with\"       \"the\"       \n[6] \"UK\"         \".\"         \n```\n:::\n\n```{.r .cell-code}\n# we can also replace \"UK\" with a multi-word expression \"United Kingdom\"\ntokens_replace(toks_eu_uk, \n               pattern = \"UK\", \n               replacement = phrase(\"United Kingdom\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"        \"European\"   \"Union\"      \"negotiated\" \"with\"      \n[6] \"the\"        \"United\"     \"Kingdom\"    \".\"         \n```\n:::\n\n```{.r .cell-code}\n# if we want to treat United Kingdom and European Union as multi-word expressions across all texts,\n# we can compound it after the replacement\ntoks_eu_uk |> \n    tokens_replace(pattern = \"UK\", replacement = phrase(\"United Kingdom\")) |> \n    tokens_compound(pattern = phrase(c(\"United Kingdom\",\n                                       \"European Union\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"            \"European_Union\" \"negotiated\"     \"with\"          \n[5] \"the\"            \"United_Kingdom\" \".\"             \n```\n:::\n:::\n\n\nMore common than one-to-one replacements is the conversation of tokens into equivalence classes defined by values of a dictionary object. Dictionaries, covered in much greater detail in chapters @sec-quanteda-dictionaries and @sec-exploring-dictionaries, allow us to look up uni-grams or multi-word expressions and replace these terms with the dictionary \"key\". We introduce the intuition behind `tokens_lookup()` with a simple example. The example below replaces selected European institutions with its dictionary key `eu_institution`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a dictionary (covered more extensively in the next chapter)\ndict_euinst <- dictionary(\n    list(eu_institution = c(\"european commission\", \"ecb\")))\n\n# tokenise a sentence \ntoks_eu <- tokens(\"The European Commission is based in Brussels \n                  and the ECB in Frankfurt.\")\n\n# look up institutions (default behaviour)\ntokens_lookup(toks_eu, dictionary = dict_euinst)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"eu_institution\" \"eu_institution\"\n```\n:::\n\n```{.r .cell-code}\n# show unmatched tokens\ntokens_lookup(toks_eu, dictionary = dict_euinst,\n              nomatch = \"_UNMATCHED\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"_UNMATCHED\"     \"eu_institution\" \"_UNMATCHED\"     \"_UNMATCHED\"    \n [5] \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"    \n [9] \"eu_institution\" \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"    \n```\n:::\n:::\n\n\nBy default, unmatched tokens are omitted, but we can assign a custom term to unmatched tokens. What is more, we can use `tokens_lookup()` as a more sophisticated form of `tokens_replace()`: setting `exclusive = FALSE` in `tokens_lookup()` replaces dictionary matches but leaves the other features unaffected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# replace dictionary matches and keep other features\ntokens_lookup(toks_eu, \n              dictionary = dict_euinst,\n              exclusive = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"The\"            \"EU_INSTITUTION\" \"is\"             \"based\"         \n [5] \"in\"             \"Brussels\"       \"and\"            \"the\"           \n [9] \"EU_INSTITUTION\" \"in\"             \"Frankfurt\"      \".\"             \n```\n:::\n:::\n\n\n### N-Grams and Skip-Grams\n\nYou can create n-grams and skip-grams in various lengths using `tokens_ngrams()` and `tokens_skipgrams()`. While using these functions is fairly straightforward, users need to make decisions about removing patterns before concatenating tokens and need to determine the size of n-grams and/or skips. We describe these options below. First, we create n-grams and skip-grams of various sizes. Then, we combine skip-grams and n-grams in the same function, and finally show how the output changes if we process a tokens object before constructing n-grams. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize a sentence\ntoks_social <- tokens(\"We should consider increasing social welfare payments.\")\n\n# form n-grams of size 2\ntokens_ngrams(toks_social, n = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_should\"           \"should_consider\"     \"consider_increasing\"\n[4] \"increasing_social\"   \"social_welfare\"      \"welfare_payments\"   \n[7] \"payments_.\"         \n```\n:::\n\n```{.r .cell-code}\n# form n-grams of size 3\ntokens_ngrams(toks_social, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_should_consider\"         \"should_consider_increasing\"\n[3] \"consider_increasing_social\" \"increasing_social_welfare\" \n[5] \"social_welfare_payments\"    \"welfare_payments_.\"        \n```\n:::\n\n```{.r .cell-code}\n# form n-grams of size 2 and 3\ntokens_ngrams(toks_social, n = 2:3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We_should\"                  \"should_consider\"           \n [3] \"consider_increasing\"        \"increasing_social\"         \n [5] \"social_welfare\"             \"welfare_payments\"          \n [7] \"payments_.\"                 \"We_should_consider\"        \n [9] \"should_consider_increasing\" \"consider_increasing_social\"\n[11] \"increasing_social_welfare\"  \"social_welfare_payments\"   \n[ ... and 1 more ]\n```\n:::\n\n```{.r .cell-code}\n# form skip-grams of size 2 and skip 1 token\ntokens_skipgrams(toks_social, n = 2, skip = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_consider\"        \"should_increasing\"  \"consider_social\"   \n[4] \"increasing_welfare\" \"social_payments\"    \"welfare_.\"         \n```\n:::\n\n```{.r .cell-code}\n# form skip-grams of size 2 and skip 1 and 2 tokens\ntokens_skipgrams(toks_social, n = 2, skip = 1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We_consider\"         \"We_increasing\"       \"should_increasing\"  \n [4] \"should_social\"       \"consider_social\"     \"consider_welfare\"   \n [7] \"increasing_welfare\"  \"increasing_payments\" \"social_payments\"    \n[10] \"social_.\"            \"welfare_.\"          \n```\n:::\n\n```{.r .cell-code}\n# form n-grams of size 1 and skip-grams with a skip of 1 token\ntokens_ngrams(toks_social, n = 2, skip = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_consider\"        \"should_increasing\"  \"consider_social\"   \n[4] \"increasing_welfare\" \"social_payments\"    \"welfare_.\"         \n```\n:::\n\n```{.r .cell-code}\n# remove stopwords and punctuation before creating n-grams\ntoks_social |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\")) |> \n    tokens_ngrams(n = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"consider_increasing\" \"increasing_social\"   \"social_welfare\"     \n[4] \"welfare_payments\"   \n```\n:::\n:::\n\n\nThe example above underscore that several combinations do not add much value to the context in which words appear. Many types are simply combinations of tokens and stopwords. From our experience, creating skip-grams or n-grams for all documents without any processing decisions in advance does not improve our analysis or results. \n\nIt is worth keeping in mind that n-grams applied to larger corpora inflate the number of types. We showcase the increase in tokens based on our corpus of 59 US inaugural speeches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of types with uni-grams and no processing\ndata_corpus_inaugural |> \n    tokens() |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 47496\n```\n:::\n\n```{.r .cell-code}\n# number of types with n-grams of size 2 \n# after removing stopwords and punctuation characters\ndata_corpus_inaugural |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\")) |> \n    tokens_ngrams(n = 2) |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 63971\n```\n:::\n\n```{.r .cell-code}\n# number of types with n-grams of size 2 and no processing\ndata_corpus_inaugural |> \n    tokens() |> \n    tokens_ngrams(n = 2) |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 118934\n```\n:::\n\n```{.r .cell-code}\n# number of types with n-grams of size 3 and no processing\ndata_corpus_inaugural |> \n    tokens() |> \n    tokens_ngrams(n = 3) |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 144820\n```\n:::\n:::\n\n\n\n::: {.callout-important appearance=\"simple\"}\n## When to pay attention to very sparse objects\n\n\n\n\nAn increase in types through n-grams increases the sparsity of a document-feature matrix, i.e., the proportion of cells that have zero counts.  (@sec-quanteda-dfms). The sparsity of US inaugural debates (`data_corpus_inaugural`) increases from 92% to 96.9% when using bi-grams instead of uni-grams. While **quanteda** handles sparse document-feature matrices very efficiently, a very high sparsity might result in convergence issues for unsupervised scaling models (@sec-ml-unsupervised-scaling) or topic models (@sec-ml-topicmodels). Therefore, n-grams or skip-grams may be counterproductive for some research questions.\n:::\n\n### Selecting Tokens within Windows\n\nIsolating specific tokens within a defined range of words can refine many research questions. For example, we could keep the term `room`  and the context of ±4 tokens in the corpus of hotel reviews. This approach might provide a first descriptive insights into aspects the customers really (dis-)liked about their hotel room.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize and process the corpus of hotel reviews\ntoks_hotels <- tokens(TAUR::data_corpus_TAhotels,\n                      remove_punct = TRUE,\n                      remove_numbers = TRUE,\n                      padding = TRUE)\n\n# keep \"room*\" and its context of ±3 tokens\ntoks_room <- toks_hotels |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE) |> \n    tokens_keep(pattern = \"room*\", \n                window = 4, padding = TRUE)\n\n# inspect the first three hotel reviews\nprint(toks_room, max_ndoc = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 20,491 documents and 1 docvar.\ntext1 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 86 more ]\n\ntext2 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 260 more ]\n\ntext3 :\n [1] \"nice\"       \"rooms\"      \"\"           \"\"           \"\"          \n [6] \"experience\" \"\"           \"\"           \"\"           \"\"          \n[11] \"\"           \"\"          \n[ ... and 226 more ]\n\ntext4 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 92 more ]\n\n[ reached max_ndoc ... 20,487 more documents ]\n```\n:::\n\n```{.r .cell-code}\n# transform tokens object into a document-feature matrix (dfm) and \n# get 30 most frequent words surrounding \"room*\" using topfeatures()\ntoks_room |> \n    dfm() |>\n    dfm_remove(pattern = \"\") |> # remove padding placeholder\n    topfeatures(n = 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       room       rooms       hotel       clean        nice       small \n      34357       12043        5523        4761        2908        2690 \n      great       staff         n't     service       floor        view \n       2630        2475        2414        2342        2181        2148 \n       good comfortable         bed    bathroom       large   breakfast \n       2106        1957        1677        1596        1594        1591 \n      night      stayed    spacious        stay    location         got \n       1425        1415        1332        1313        1274        1267 \n        day        just        size        beds      booked    friendly \n       1216        1181        1161        1050        1002         992 \n```\n:::\n:::\n\n\n::: {.callout-tip appearance=\"simple\"}\n## To pad or not to pad?\nPadding implies leaving an empty string where removed tokens previously existed. Padding can be useful when we want to remove certain patterns, but (1) still know the position of tokens that remain in the corpus or (2) if we select tokens and their context window. The examples below highlight the differences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks <- tokens(\"We're having a great time at the\n               pool and lovely food in the restaurant.\")\n\n# keep great, lovely and a context window of ±1 tokens\n# without padding\ntokens_keep(toks, pattern = c(\"great\", \"lovely\"),\n            window = 1,\n            padding = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"a\"      \"great\"  \"time\"   \"and\"    \"lovely\" \"food\"  \n```\n:::\n\n```{.r .cell-code}\n# keep great, lovely and a context window of ±1 tokens\n# with padding\ntokens_keep(toks, pattern = c(\"great\", \"lovely\"),\n            window = 1,\n            padding = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"\"       \"\"       \"a\"      \"great\"  \"time\"   \"\"       \"\"       \"\"      \n [9] \"and\"    \"lovely\" \"food\"   \"\"      \n[ ... and 3 more ]\n```\n:::\n:::\n\n:::\n\n\n## Advanced\n\nNext, we provide an overview of advanced tokens operations: splitting and chunking tokens. Both can be useful in some contexts, but tend to be used less frequently than the operations discussed so far. \n\n\n### Splitting\n\nSplitting tokens implies that we split one token into multiple replacements. The function `tokens_split()` splits a tokens by a separator pattern, effectively reversing the operation of `tokens_compound()`. The example below shows how to undo a compounding operation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks <- tokens(\"Value added tax is a multi-word expression.\")\n\n# compound value added tax\ntoks_vat <- tokens_compound(toks, \n                            pattern = phrase(\"value added tax*\"), \n                            concatenator = \"_\")\ntoks_vat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"Value_added_tax\" \"is\"              \"a\"               \"multi-word\"     \n[5] \"expression\"      \".\"              \n```\n:::\n\n```{.r .cell-code}\n# reverse compounding using \"_\" as the separator \n# for splitting tokens\ntokens_split(toks_vat, separator = \"_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"Value\"      \"added\"      \"tax\"        \"is\"         \"a\"         \n[6] \"multi-word\" \"expression\" \".\"         \n```\n:::\n:::\n\n\n\n### Chunking\n\nIn some applications, we may be interested by dividing our texts into equally-sized segments or chunks. You might be working with a set of very long documents, which cannot be segmented into smaller units such as paragraphs or sentences due to missing delimiters (see @sec-quanteda-corpus and using `corpus_reshape()` or `corpus_segment()`). Some methods, such as topic models (@sec-ml-topicmodels) work better when the documents have similar lengths. The function `tokens_chunk()` can be used to segment a tokens object by chunks of a given size. The `overlap` argument allows you to specify whether to take over tokens from the preceding chunk. We use the first hotel review of `data_corpus_TAhotels` and divide the reviews up into smaller chunks with and without overlaps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize the first hotel review\ntoks_r1 <- tokens(TAUR::data_corpus_TAhotels[1])\n\n# print first 15 tokens\nprint(toks_r1, max_ntoken = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document and 1 docvar.\ntext1 :\n [1] \"nice\"        \"hotel\"       \"expensive\"   \"parking\"     \"got\"        \n [6] \"good\"        \"deal\"        \"stay\"        \"hotel\"       \"anniversary\"\n[11] \",\"           \"arrived\"     \"late\"        \"evening\"     \"took\"       \n[ ... and 83 more ]\n```\n:::\n\n```{.r .cell-code}\n# chunk into chunks of 5 tokens without overlap\ntoks_r1_chunk <- tokens_chunk(toks_r1, size = 5)\n\n# inspect chunked tokens object\nprint(toks_r1_chunk, max_ndoc = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 20 documents and 1 docvar.\ntext1.1 :\n[1] \"nice\"      \"hotel\"     \"expensive\" \"parking\"   \"got\"      \n\ntext1.2 :\n[1] \"good\"        \"deal\"        \"stay\"        \"hotel\"       \"anniversary\"\n\ntext1.3 :\n[1] \",\"       \"arrived\" \"late\"    \"evening\" \"took\"   \n\n[ reached max_ndoc ... 17 more documents ]\n```\n:::\n\n```{.r .cell-code}\n# chunk into chunks of 5 tokens with overlap and inspect result\ntokens_chunk(toks_r1, size = 5, overlap = 2) |> \n    print(max_ndoc = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 33 documents and 1 docvar.\ntext1.1 :\n[1] \"nice\"      \"hotel\"     \"expensive\" \"parking\"   \"got\"      \n\ntext1.2 :\n[1] \"parking\" \"got\"     \"good\"    \"deal\"    \"stay\"   \n\ntext1.3 :\n[1] \"deal\"        \"stay\"        \"hotel\"       \"anniversary\" \",\"          \n\n[ reached max_ndoc ... 30 more documents ]\n```\n:::\n:::\n\n\nAs always, this example serves only for illustration purposes. Usually, the selected chunks would be larger than five documents to mirror the length of \"typical\" documents, such as sentences or paragraphs.\n\n\n\n## Further Reading\n\n- Examples of targeted analyses\n- Part-of-speech tagging\n- spacy\n\n\n## Exercises\n\nAdd some here.\n",
    "supporting": [
      "11-quanteda-tokensadvanced_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}