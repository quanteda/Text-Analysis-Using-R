{
  "hash": "e53ad9359b67d1cddcdca6a5fecc45fb",
  "result": {
    "markdown": "# Advanced Token Manipulation {#sec-quanteda-tokensadvanced}\n\n## Objectives\n\n\nThe previous chapter introduced you to the basics of tokenisation and processing of tokens objects. Now we move to advanced token manipulation. We explain why we might want to compound certain multiword expressions into a single token. We also introduce the `phrase()` function, which is required for handling multiword expressions. Afterwards, we show how to replace tokens throughout a corpus, and explain n-grams and skip-grams. We also explain why you might want to keep specific tokens and a context window around these tokens, and close the chapter with a brief introduction to lookup functions, which are explained in more detail in @sec-exploring-dictionaries.\n\n## Methods\n\nThe previous section treats all tokens as so-called unigrams. We separated tokens by spaces and did not combine two or more tokens that might form a multiword expression. In languages with compound words, we do not need to pay attention to **multi-word expresions**. For example, the term \"Mehrwertsteuer\" is a single word in German, but its English equivalent consists of three words: \"value added tax\". If we are interested in companies' or politicians' focus on different forms of taxation, we want to treat `value added tax` as a multiword expression rather than three separate tokens `value`, `added`, `tax`. This is especially important for bag-of-words analyses. A document-feature matrix (dfm) (@sec-quanteda-dfms) counts the frequencies of features in each document. If we do not explicitly compound `value added tax`, the words will be included as three separate tokens in our dfm. However, compounding the expression during the tokenisation process, will ensure that the dfm contains the compound noun `value_added_tax`.\n\nIn many cases, we know multiword expressions through our domain knowledge. For example, in reviews about hotels in New York, we might want to compound `New_York`, `Madison_Square_Garden`, and `Wall_Street`. In parliamentary speeches, we want to compound party names: instead of treating `green party` as two tokens, we might prefer the multiword expression `green party` before proceeding with our statistical analysis.\n\nIn other cases, researchers do not know relevant multiword expressions in advance. We can use approaches such as **keywords-in-context** (@sec-exploring-kwic) to explore the context of specific words, or conduct a **collocation analysis** to automatically identify terms that tend to co-occur together. We introduce these methods in **ADD REFERENCE TO CHAPTER**. Having identified multiword expressions, you can compound these collocations before continuing your textual analysis.\n\nSometimes, researchers might want to **replace** certain tokens. For example, in political texts, `Labour` spelled usually implies a mention to the named-entity Labour Party, while `labour` refers to the noun. Another example are country names: if we want to understand mentions of the United States of America in UN General Debates, we could replace `US`, `USA`, and `United States` with `united_states_of_america`. Dictionaries, discussed in @sec-quanteda-dictionaries, are another effective tool for replacing tokens with an overarching \"key\", such as `united_states`.\n\nIn some applications, tokens sequences might reveal more information than individual tokens. Before transforming a tokens object to a dfm, many existing studies create so-called n-grams or skipgrams. **N-grams** are sequences of \"n\" items. **Skip-grams** are variations of n-grams which pairs non-consecutive tokens. N-grams capture local word patterns, whereas skip-grams capture broader contexts within texts. The sentence `I loved my time in New York.` would result in the following bi-grams (sequence of two tokens): `\"I_loved\"  \"loved_my\" \"my_time\"  \"time_in\"  \"in_New\" \"New_York\" \"York_.\"`. Creating tri-grams changes the tokens object to `\"I_loved_my\" \"loved_my_time\" \"my_time_in\" \"time_in_New\" \"in_New_York\" \"New_York_.\"`. Skip-grams of size 2 with a distance of 0 and 1 would change the object to:  `\"I_loved\" \"I_my\" \"loved_my\" \"loved_time\" \"my_time\" \"my_in\"  \"time_in\" \"time_New\" \"in_New\" \"in_York\" \"New_York\" \"New_.\" \"York_.\"`. \n\nThe example above highlights advantages and shortcomings of n-grams and skip-grams. On the one hand, both approaches provide information about the context of each token. On the other hand, n-grams and skip-grams drastically increase the number of types (i.e., unique tokens). For example, the number of types in the corpus of US inaugural speeches more than doubles when creating bi-grams rather than uni-grams and triples when creating tri-grams instead of uni-grams. Instead of creating bi-grams or tri-grams, for many application the manual or automated identification of meaningful multi-word expressions is sufficient or even preferred over n-grams.\n\n\n\n\n\n\n\n**Keeping tokens and their context windows** is another advanced, and often very useful, tokenisation operation. We can keep  specific tokens and the words around these patterns to refine our research question and focus on specific aspects in our text corpus. Let's imagine the following example: we have a large set of parliamentary speeches and want to understand how parties talk about the environment and climate change. Most utterances will focus on different policies or contain procedural language. We could create a list of word and phrases relating to the environment, and keep these terms with a context of several words. This would allow for a \"targeted\" analysis. Instead of analysing the full text corpus, we have narrowed down our documents to the parts relevant to our research question. For example, @lupia20nsf limit U.S Congressional speeches to sentences mentioning the National Science Foundation (NSF). Afterwards, the authors identify which of these context words distinguish Democrats from Republicans, and how the topics (@sec-ml-topicmodels) mentioned in these sentences are moderated by the party of a speaker. Similarly, @muellercovid assess how receiving support (e.g., face masks) from China during the Covid-19 pandemic related to the way China was portrayed in newspapers by countries that received support. The authors limit the news articles to statements that mention China and the pandemic within a context of ±30 words. The results show that newspapers reported more favourably about China after having received support, relative to countries that have not (yet) received help from China. These findings show that China's \"mask diplomacy\" may have changed the global discourse. So-called \"targeted\" analyses can be very useful for understanding sentiment or opinions towards certain entities or persons. Limiting a large corpus to a defined set of words and their immediate contexts can be used to study a variety of research questions.\n\n\nFinally, we briefly introduce the concept of **looking up** tokens. We match tokens against a predefined list. This approach requires researchers to develop \"dictionaries\", consisting of one or more \"keys\" or categories. Each of these keys in turn contains various patterns, usually words or multi-word expressions. A sentiment analysis that relies list of terms scored as \"positive\" and \"negative\" is a classic use case of looking up tokens. \n\nCategorisation of topics, policy areas, or concepts can also be conducted with a \"lookup approach.\" For example, @gessler22immig create a dictionary of keywords and phrases related to immigration. Afterwards, the authors apply this dictionary to party press releases. The authors keep all documents containing keywords from their immigration dictionary. Their rule-based approach is much more computationally efficient than supervised classification and produced valid results. Subsequent analyses apply scaling methods to this subset of immigration-related press releases to understand how the 2015 \"refugee crisis\" in Europe changed party positions on migration policy. @sec-exploring-dictionaries provides more details on creating and applying dictionaries.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\n\nTAUR::data_corpus_TAhotels |> \n    tokens(remove_numbers = TRUE,\n           remove_punct = TRUE, padding = TRUE) |> \n    tokens_compound(pattern = phrase(\"not *\")) |> \n    tokens_compound(pattern = phrase(\"no *\")) |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE) |> \n    tokens_keep(pattern = c(\"terrible\", \"bad\", \"poor\"),\n                window = 5) |> \n    dfm() |> \n    topfeatures(25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  bad       poor      hotel   terrible        n't    service \n      5050       2261        904        697        614        587        506 \n      food       room       good    reviews       just     really      staff \n       417        387        360        309        241        225        199 \n     great     people     resort      rooms       stay experience     stayed \n       199        194        178        172        166        158        148 \n     night      place        say   location \n       147        146        146        140 \n```\n:::\n:::\n\n\n\n\n\n\n::: {.callout-tip appearance=\"simple\"}\n## Using a random subset of documents to detect collocations\nRunning `textstat_collocations()` on very large text corpora will take a lot of time. If you mainly use the function to explore multiword expressions, it is often sufficient to draw a random sample of documents and apply the function to this smaller sample. It speeds up the computation and---usually---identifies the same set of relevant multiword expressions. You can extract a random selection of documents by using `corpus_sample()` before running the collocation analysis. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\n\n# set seed for reproducibility\nset.seed(235)\n\n# sample 2000 hotel reviews from the set of 20,491 reviews\ncorp_sampled <- corpus_sample(TAUR::data_corpus_TAhotels, size = 2000)\n\n# get collocations in entire corpus\ntstat_col_hotels <- corp_sampled |> \n    tokens() |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE) |> \n    textstat_collocations(size = 2:3) # bigrams and trigrams\n```\n:::\n\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textstats)\n\n# get collocations ending with \"tax*\"\ntstat_col_tax <- data_corpus_irishbudget2010 |> \n    tokens() |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE) |> \n    tokens_keep(pattern = \"tax*\", window = c(2, 0)) |> \n    textstat_collocations(size = 2:3) # bigrams and trigrams\n\n# print first 10 rows\nhead(tstat_col_tax, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         collocation count count_nested length   lambda        z\n1         income tax    18            5      2 3.733104 4.369055\n2          low taxes     3            1      2 3.961232 3.977588\n3         carbon tax     9            2      2 3.003667 3.399154\n4      fair taxation     2            2      2 5.045078 3.221254\n5   additional taxes     2            1      2 4.688006 3.002396\n6  consumption taxes     2            1      2 4.688006 3.002396\n7     increase taxes     2            0      2 4.688006 3.002396\n8   increasing taxes     2            2      2 4.688006 3.002396\n9           tax rate     4            0      2 2.424701 2.557869\n10   corporation tax     4            1      2 3.323856 2.225367\n```\n:::\n\n```{.r .cell-code}\n# get collocations in entire corpus\ntstat_col_all <- data_corpus_irishbudget2010 |> \n    tokens() |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE) |> \n    textstat_collocations(size = 2:3) # bigrams and trigrams\n\n# print first ten rows of data frame\nhead(tstat_col_all, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      collocation count count_nested length   lambda        z\n1  social welfare    70           47      2 8.081165 28.82294\n2   child benefit    45           16      2 8.320662 24.96719\n3       next year    37           26      2 6.711878 24.00558\n4  public service    60           36      2 7.527788 23.23240\n5        per week    25           25      2 7.111602 21.99020\n6   public sector    30           16      2 5.143804 21.37849\n7    labour party    21           11      2 6.992273 19.92967\n8     green party    20            8      2 6.925414 19.58859\n9  private sector    19            7      2 6.842021 18.58472\n10     irish bank    14           14      2 6.618153 17.32452\n```\n:::\n:::\n\n\n\n\n## Examples\n\nIn this section, we rely on short sentences and the corpus of hotel reviews to explain how to compound tokens, how to replace them, how to create n-grams and skip-grams, and how to select tokens and their context. \n\n\n### Compounding Tokens\n\nWe start with compounding tokens and introducing the `phrase()` function. The `phrase()` function declares that an expression consists of multiple patterns and to make explicit that the elements should be used for matching multi-word sequences rather than individual matches to single words. It is vital to use `phrase()` in all functions involving multiword expressions, including `tokens_compound()`.^[`tokens_lookup()` is an exception to the rule]. Let's explore how to compound the multiword expressions \"social welfare\" and \"social security\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create tokens object for examples\ntoks_social <- tokens(\"We need to increase social welfare and improve social security.\")\n\n# compound the patterns \"social welfare\"\ntoks_social |> \n    tokens_compound(pattern = phrase(\"social welfare\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"             \"need\"           \"to\"             \"increase\"      \n [5] \"social_welfare\" \"and\"            \"improve\"        \"social\"        \n [9] \"security\"       \".\"             \n```\n:::\n\n```{.r .cell-code}\n# note: compounding does not work without phrase()!\ntoks_social |> \n    tokens_compound(pattern = \"social welfare\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"       \"need\"     \"to\"       \"increase\" \"social\"   \"welfare\" \n [7] \"and\"      \"improve\"  \"social\"   \"security\" \".\"       \n```\n:::\n:::\n\n\n\nWe can include several patterns in our character vector containing multi-word expressions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compound two patterns\ntokens_compound(toks_social,\n                pattern = phrase(c(\"social welfare\",\n                                   \"social security\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We\"              \"need\"            \"to\"              \"increase\"       \n[5] \"social_welfare\"  \"and\"             \"improve\"         \"social_security\"\n[9] \".\"              \n```\n:::\n:::\n\n\n\n::: {.callout-tip appearance=\"simple\"}\n## Choose the concatenator carefully\nBy default, compounded tokens are concatenated using an underscore (`_`). The default is recommended since underscores will not be removed during normal cleaning and tokenisation. Using an underscore as a separator also allows you to check whether compounding worked as expected.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check whether compounding worked as expected \n# by extracting patterns containing underscores\ntoks_social |> \n    tokens_compound(pattern = phrase(c(\"social welfare\", \"social security\"))) |> \n    tokens_keep(pattern = \"*_*\") # keep patterns with underscores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"social_welfare\"  \"social_security\"\n```\n:::\n:::\n\n\n:::\n\n\nNote that we can also compound terms based on regular expressions (@sec-rstrings and @sec-appendix-regex) or \"wild card\" pattern matches. Below, we use the glob-style wildcard expression `*` to compound all multiword expressions starting with \"social\" in US State of the Union speeches.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenise SOTU speeches, remove punctuation and numbers\n# before removing stopwords\ntoks_sotu <- TAUR::data_corpus_sotu |> \n    tokens(remove_punct = TRUE,\n           remove_numbers = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE)\n\n# compound all phrases starting with \"social\"\ntoks_sotu_comp <- tokens_compound(toks_sotu, pattern = phrase(\"social *\"))\n\n# spot-check results by keeping all tokens starting \n# with social using \"glob\"-style wildcard pattern match\n# and create dfm to check compounded terms\ntokens_keep(toks_sotu_comp, pattern = \"social_*\") |> \n    dfm() |> \n    topfeatures(n = 15) # get 15 most frequent compounded tokens\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    social_security     social_problems     social_services        social_needs \n                229                  16                  15                  13 \n        social_life        social_order     social_progress       social_system \n                 10                   9                   9                   7 \n     social_welfare     social_programs  social_intercourse    social_condition \n                  7                   7                   6                   6 \n      social_change    social_insurance social_institutions \n                  6                   6                   5 \n```\n:::\n:::\n\n\n\n## Replacing and Looking Up Tokens\n\nIn some cases, researchers may want to substitute tokens. Reasons to replace tokens include standardising terms, accounting for synonyms, or fixing typographical errors. One example are acronyms. For example, it may be reasonable to harmonise \"EU\" and \"European Union\" in political texts. The function  `tokens_replace()` allows us to conduct one-to-one matching and replace EU with European Union.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks_eu_uk <- tokens(\"The European Union negotiates with the UK.\")\n\n# important: use phrase if you want to detect a multiword expression\ntokens_replace(toks_eu_uk, \n               pattern = phrase(\"European Union\"),\n               replacement = \"EU\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"        \"EU\"         \"negotiates\" \"with\"       \"the\"       \n[6] \"UK\"         \".\"         \n```\n:::\n\n```{.r .cell-code}\n# we can also replace \"UK\" with a multi-word expression \"United Kingdom\"\ntokens_replace(toks_eu_uk, \n               pattern = \"UK\", \n               replacement = phrase(\"United Kingdom\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"        \"European\"   \"Union\"      \"negotiates\" \"with\"      \n[6] \"the\"        \"United\"     \"Kingdom\"    \".\"         \n```\n:::\n\n```{.r .cell-code}\n# if we want to treat United Kingdom and European Union as a multiword expressions across all texts,\n# we can compound it after the replacement\ntokens_replace(toks_eu_uk, pattern = \"UK\", replacement = phrase(\"United Kingdom\")) |> \n    tokens_compound(pattern = phrase(c(\"United Kingdom\",\n                                       \"European Union\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"            \"European_Union\" \"negotiates\"     \"with\"          \n[5] \"the\"            \"United_Kingdom\" \".\"             \n```\n:::\n:::\n\n\nMore common than one-to-one replacements is the conversation of tokens into equivalence classes defined by values of a dictionary object. Dictionaries, covered in much greater detail in chapters @sec-quanteda-dictionaries and @sec-exploring-dictionaries, allow us to look up uni-grams or multi-word expressions and replacing these terms with the dictionary \"key\". We introduce the intuition behind `tokens_lookup()` with a simple example. The example below replaces selected European institutions with its dictionary key `eu_institution`. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a dictionary\ndict_euinst <- dictionary(\n    list(eu_institution = c(\"european parliament\",\n                            \"ecb\",\n                            \"european commission\")))\n\n# tokenize a sentence \ntoks_eu <- tokens(\"The European Commission is based in Brussels and the ECB in Frankfurt.\")\n\n# look up institutions (default behaviour)\ntokens_lookup(toks_eu, dictionary = dict_euinst)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"eu_institution\" \"eu_institution\"\n```\n:::\n\n```{.r .cell-code}\n# show unmatched tokens\ntokens_lookup(toks_eu, dictionary = dict_euinst,\n              nomatch = \"_UNMATCHED\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"_UNMATCHED\"     \"eu_institution\" \"_UNMATCHED\"     \"_UNMATCHED\"    \n [5] \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"    \n [9] \"eu_institution\" \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"    \n```\n:::\n:::\n\n\nBy default, unmatched tokens are omitted, but we can assign a custom term to unmatched tokens. What is more, we can use `tokens_lookup()` as a more sophisticated form of `tokens_replace()`: setting `exclusive = FALSE` in `tokens_lookup()` replaces dictionary matches but leaves the other features unaffected.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# replace dictionary matches and keep other features\ntokens_lookup(toks_eu, \n              dictionary = dict_euinst,\n              exclusive = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"The\"            \"EU_INSTITUTION\" \"is\"             \"based\"         \n [5] \"in\"             \"Brussels\"       \"and\"            \"the\"           \n [9] \"EU_INSTITUTION\" \"in\"             \"Frankfurt\"      \".\"             \n```\n:::\n:::\n\n\n\n### N-Grams and Skip-Grams\n\nYou can create n-grams and skip-grams in various lengths using `tokens_ngrams()` and `tokens_skipgrams()`. While using these functions is fairly straightforward, users need to make decisions about removing certain patterns before concatenating tokens and need to decide about the size and skips. We describe these options below. First, we create n-grams and skip-grams of various sizes. Then, we combine skip-grams and n-grams in the same function, and finally show how the output changes if we process a tokens object before constructing n-grams. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize a sentence\ntoks_social <- tokens(\"We should consider increasing social welfare payments.\")\n\n# form n-grams of size 2\ntokens_ngrams(toks_social, n = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_should\"           \"should_consider\"     \"consider_increasing\"\n[4] \"increasing_social\"   \"social_welfare\"      \"welfare_payments\"   \n[7] \"payments_.\"         \n```\n:::\n\n```{.r .cell-code}\n# form n-grams of size 3\ntokens_ngrams(toks_social, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_should_consider\"         \"should_consider_increasing\"\n[3] \"consider_increasing_social\" \"increasing_social_welfare\" \n[5] \"social_welfare_payments\"    \"welfare_payments_.\"        \n```\n:::\n\n```{.r .cell-code}\n# form n-grams of size 2 and 3\ntokens_ngrams(toks_social, n = 2:3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We_should\"                  \"should_consider\"           \n [3] \"consider_increasing\"        \"increasing_social\"         \n [5] \"social_welfare\"             \"welfare_payments\"          \n [7] \"payments_.\"                 \"We_should_consider\"        \n [9] \"should_consider_increasing\" \"consider_increasing_social\"\n[11] \"increasing_social_welfare\"  \"social_welfare_payments\"   \n[ ... and 1 more ]\n```\n:::\n\n```{.r .cell-code}\n# form skip-grams of size 2 and skip 1 token\ntokens_skipgrams(toks_social, n = 2, skip = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_consider\"        \"should_increasing\"  \"consider_social\"   \n[4] \"increasing_welfare\" \"social_payments\"    \"welfare_.\"         \n```\n:::\n\n```{.r .cell-code}\n# form skip-grams of size 2 and skip 1 and 2 tokens\ntokens_skipgrams(toks_social, n = 2, skip = 1:2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"We_consider\"         \"We_increasing\"       \"should_increasing\"  \n [4] \"should_social\"       \"consider_social\"     \"consider_welfare\"   \n [7] \"increasing_welfare\"  \"increasing_payments\" \"social_payments\"    \n[10] \"social_.\"            \"welfare_.\"          \n```\n:::\n\n```{.r .cell-code}\n# form n-grams of size 1 and skip-grams with a skip of 1 token\ntokens_ngrams(toks_social, n = 2, skip = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_consider\"        \"should_increasing\"  \"consider_social\"   \n[4] \"increasing_welfare\" \"social_payments\"    \"welfare_.\"         \n```\n:::\n\n```{.r .cell-code}\n# remove stopwords and punctuation before creating n-grams\n\ntoks_social |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\")) |> \n    tokens_ngrams(n = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"consider_increasing\" \"increasing_social\"   \"social_welfare\"     \n[4] \"welfare_payments\"   \n```\n:::\n:::\n\n\n\nThe example above underscore that many combinations do not add much value to the context in which words appear. Many types are simply combinations of tokens and stopwords. From our experience, creating skip-grams or n-grams for all documents without any processing decisions in advance does not improve our analysis or results. \n\n::: {.callout-warning appearance=\"simple\"}\n## How n-grams and skipgrams and affect types and sparsity\n\nThe example below shows how creating n-grams on a larger corpora inflates the number of types. We use US inaugural speeches as an example.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of types with uni-grams and no processing\ndata_corpus_inaugural |> \n    tokens() |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 47496\n```\n:::\n\n```{.r .cell-code}\n# number of types with n-grams of size 2 \n# after removing stopwords and punctuation characters\ndata_corpus_inaugural |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\")) |> \n    tokens_ngrams(n = 2) |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 63971\n```\n:::\n\n```{.r .cell-code}\n# number of types with n-grams of size 2 and no processing\ndata_corpus_inaugural |> \n    tokens() |> \n    tokens_ngrams(n = 2) |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 118934\n```\n:::\n\n```{.r .cell-code}\n# number of types with n-grams of size 3 and no processing\ndata_corpus_inaugural |> \n    tokens() |> \n    tokens_ngrams(n = 3) |> \n    ntype() |> \n    sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 144820\n```\n:::\n:::\n\n\nThe increasing in types has direct consequences on subsequent analyses. Many combinations appear very infrequently, which increases the sparsity, i.e., the proportion of roportion of cells that have zero counts, in a document-feature matrix (@sec-quanteda-dfms).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sparsity using unigrams\ndata_corpus_inaugural |> \n    tokens() |> \n    dfm() |> \n    sparsity()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9183978\n```\n:::\n\n```{.r .cell-code}\n# sparsity using bigrams\ndata_corpus_inaugural |> \n    tokens() |> \n    tokens_ngrams(n = 2) |> \n    dfm() |> \n    sparsity()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9691176\n```\n:::\n:::\n\n\n\nThe sparsity increases from 91.8% to 96.9% when using bi-grams instead of uni-grams. While **quanteda** handles sparse document-feature matrices very efficiently, a very high sparsity might result in convergence issues for unsupervised scaling models (@sec-ml-unsupervised-scaling) or topic models (@sec-ml-topicmodels).\n:::\n\n### Selecting Tokens within Windows\n\nIsolating specific tokens within a defined range of words can be beneficial for many research questions. For example, we could keep a selection of very negative and very positive adjectives and the context of ±3 tokens in the corpus of hotel reviews. This approach might provide descriptive insights into aspects customers really (dis-)liked.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tokenize and process the corpus of hotel reviews\ntoks_hotels <- tokens(TAUR::data_corpus_TAhotels)\n\n# keep three positive terms and their context of ±3 tokens\ntoks_hotels_pos <- toks_hotels |> \n    tokens_keep(pattern = c(\"great\", \"fantastic\", \"nice\"), window = 3, padding = TRUE)\n\n# let's inspect the first three hotel reviews\nprint(toks_hotels_pos, max_ndoc = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 20,491 documents and 1 docvar.\ntext1 :\n [1] \"nice\"      \"hotel\"     \"expensive\" \"parking\"   \"\"          \"\"         \n [7] \"\"          \"\"          \"\"          \"\"          \"\"          \"\"         \n[ ... and 86 more ]\n\ntext2 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 260 more ]\n\ntext3 :\n [1] \"nice\"  \"rooms\" \"not\"   \"4\"     \"\"      \"\"      \"\"      \"\"      \"\"     \n[10] \"\"      \"\"      \"\"     \n[ ... and 226 more ]\n\n[ reached max_ndoc ... 20,488 more documents ]\n```\n:::\n:::\n\n\n\n\n::: {.callout-tip appearance=\"simple\"}\n# The importance of padding\n\nPadding implies leaving an empty string where removed tokens previously existed. Padding can be useful when we want to remove certain patterns, but (1) still know the position of tokens that remain in the corpus or (2) if we select tokens and their context window. The examples below highlight the differences.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoks <- tokens(\"We're having a great time at the\n               pool and lovely food in the restaurant.\")\n\n# keep great, lovely and a context window of ±1 tokens\n# without padding\ntokens_keep(toks, pattern = c(\"great\", \"lovely\"),\n            window = 1,\n            padding = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n[1] \"a\"      \"great\"  \"time\"   \"and\"    \"lovely\" \"food\"  \n```\n:::\n\n```{.r .cell-code}\n# keep great, lovely and a context window of ±1 tokens\n# with padding\ntokens_keep(toks, pattern = c(\"great\", \"lovely\"),\n            window = 1,\n            padding = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 1 document.\ntext1 :\n [1] \"\"       \"\"       \"a\"      \"great\"  \"time\"   \"\"       \"\"       \"\"      \n [9] \"and\"    \"lovely\" \"food\"   \"\"      \n[ ... and 3 more ]\n```\n:::\n:::\n\n\n:::\n\n- Practical applications: Sentiment analysis, context extraction, etc.\n- Example: Extracting adjectives within a window of 3 words around a noun.\n\n### Accounting for Negations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spacyr)\nlibrary(TAUR)\nlibrary(quanteda.tidy)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda.tidy'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n```{.r .cell-code}\n# initialise spacy\nspacy_initialize()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFound 'spacy_condaenv'. spacyr will use this environment\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nsuccessfully initialized (spaCy Version: 3.6.1, language model: en_core_web_sm)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n(python options: type = \"condaenv\", value = \"spacy_condaenv\")\n```\n:::\n\n```{.r .cell-code}\n# sample 2000 hotel reviews\nset.seed(235)\ncorp_hotels <- TAUR::data_corpus_TAhotels |> \n    corpus_sample(size = 2000)\n\nndoc(corp_hotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2000\n```\n:::\n\n```{.r .cell-code}\n# parse tweets and add part-of-speech\ntoks_pos <- spacy_parse(corp_hotels) |> \n    as.tokens(include_pos = \"pos\")\n\n# show structure of tokens object (=token/POS-TAG)\nprint(toks_pos, max_ndoc = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 2,000 documents.\ntext19221 :\n [1] \"loved/VERB\"     \"hotel/NOUN\"     \"friendly/ADJ\"   \"staf/NOUN\"     \n [5] \"hotel/NOUN\"     \"price/NOUN\"     \"payed/VERB\"     \"verry/VERB\"    \n [9] \"good/ADJ\"       \"service/NOUN\"   \"staf/NOUN\"      \"reception/NOUN\"\n[ ... and 43 more ]\n\ntext7029 :\n [1] \"just/ADV\"      \"opinion/NOUN\"  \"loved/VERB\"    \",/PUNCT\"      \n [5] \"fiance/NOUN\"   \"just/ADV\"      \"returned/VERB\" \"4/NUM\"        \n [9] \"day/3/PROPN\"   \"night/NOUN\"    \"stay/VERB\"     \"memorial/ADJ\" \n[ ... and 409 more ]\n\n[ reached max_ndoc ... 1,998 more documents ]\n```\n:::\n\n```{.r .cell-code}\n# select tokens mentioning \"room*\" and context of ±6 words\ntoks_hotel <- tokens_keep(toks_pos, pattern = c(\"room*\"),  window = 6)\n\n# keep only adjectives \ntoks_room <- toks_hotel |> \n    tokens_keep(pattern = c(\"*ADJ\"))\n\n# get 30 most frequent adjectives\n# mentioned in the context of \"room*\"\ntoks_room |> \n    dfm() |> \n    topfeatures(n = 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      clean/adj        nice/adj       great/adj        good/adj       small/adj \n            468             376             364             300             286 \ncomfortable/adj       large/adj    spacious/adj    friendly/adj     helpful/adj \n            210             171             139             121             113 \n     little/adj       quiet/adj   beautiful/adj   excellent/adj      modern/adj \n            112              97              96              94              94 \n       free/adj      lovely/adj         big/adj        huge/adj       ready/adj \n             92              91              87              79              76 \n   standard/adj   available/adj   wonderful/adj         new/adj         old/adj \n             69              63              58              58              56 \n      right/adj      better/adj      double/adj        best/adj       close/adj \n             56              56              55              54              49 \n```\n:::\n:::\n\n\n\n\n### Looking Up Tokens\n\n\n## Advanced\n\n\n### Splitting\n- Definition: Dividing a token into multiple separate tokens.\n- Use-cases: Dividing concatenated words, splitting hashtags into words, etc.\n- Example: Splitting \"lovebirds\" into \"love\" and \"birds\".\n- Implementing token splitting in quanteda.\n\n### Chunking\n- Definition: Dividing texts into consistent segments or chunks.\n- Use-cases: Analyzing data by paragraphs, sentences, or other structural units.\n- Example: Chunking a chapter into paragraphs or sentences.\n- How to use quanteda for chunking texts.\n\n\n\n\n\n\n### Ngrams\n- Definition: Sequences of 'n' tokens considered as one entity.\n- Benefits: Captures local word patterns and can be insightful for phraseology.\n- Example: Bi-grams (2-word sequences) like \"United States\".\n- Generating ngrams using quanteda.\n\n### Skipgrams\n- Definition: Similar to ngrams, but allows tokens to be skipped.\n- Benefits: Can capture non-adjacent word relationships.\n- Example: 1-skip-2-grams could generate \"United of America\" from \"United States of America\".\n- How to create skipgrams in quanteda.\n\n\n## Objectives\n\nMaster advanced operations with tokens.\n\n\n## Methods\n\nApplicable methods for the objectives listed above.\n\n## Applications\n\nExamples of each of the above methods.\n\n## Issues\n\nAdditional issues.\n\n## Further Reading\n\n\n\n\n## Exercises\n\nAdd some here.\n",
    "supporting": [
      "11-quanteda-tokensadvanced_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}