{
  "hash": "6f2b898950c477c5735001df4adcf212",
  "result": {
    "markdown": "# Creating and Managing Corpora {#sec-quanteda-corpus}\n\n## Objectives\n\nIn this section, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus.\n\n## Methods\n\nText analysis projects involve choices about the texts to be analyzed. Texts are collected in a *text corpus*. Text corpora usually consist of three elements: the texts, document-level variables, and metadata about the corpus overall.\n\nCreating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. A text corpus could include all articles published on immigration in Irish newspapers, with each article recorded as one *document* in the text corpus. A corpus could also be the reviews about one specific hotel, or a random sample of reviews about hotels in Europe. Researchers need to consider and justify why certain texts are (not) included in a corpus, since the generalisability and validity of findings can depend on the documents selected for analysis.\n\nThe principles of your specific project or research design should guide your decisions to include or exclude of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in televised debates and campaign speeches, the corpus will contain debate transcripts and speeches. Thus, the research question should drive document selection.\n\nIn some text analysis applications, the sample could constitute the entirety of texts. Analysing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information not transcribed or published cannot be included in the analysis. When analysing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions [@herzog15cuts]. The positions of politicians not selected for speaking at a budget debate cannot be considered in the textual analysis. Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.\n\n::: {.callout-note appearance=\"simple\"}\nIncluding and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches [@benoit19sophistication]. Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in @sec-exploring-freqs. @sec-ml-topicmodels shows how to identify variation in topic prevalence for two or more groups of documents.\n:::\n\nBesides the raw text, corpora (usually) include attributes that distinguish texts. We call these attributes *document-level variables* or, in the language **quanteda**, \"docvars\". Docvars contain additional information on each document and allow researchers to differentiate between texts in their analysis. Document-level variables could be the name of the author of a text, the newspaper in which an article was published, the hotel which was reviewed on TripAdvisor, or the date when the document was created.\n\nA text corpus also typically contains *metadata* recording important information about the corpus as a whole. Metadata that is generally useful to record in a corpus include the source where the corpus was obtained, including possibly the URL; the author of the corpus (if from a single source); a title for the corpus; and possibly important keywords that might be used later in categorising the corpus. Corpus metadata can be quite general, including a codebook, instructions how to cite the corpus, or a license for the use of the corpus or copyright information.\n\n::: {.callout-tip appearance=\"simple\"}\nWhile there no universal standard exists for which metadata to record, there are guidelines for corpus objects that are followed in the **quanteda** packages that provide example corpora, including those in the package that accompanies this book. These all record the following metadata fields: *title* and *description*, providing a title and short text description of the corpus respectively; *source* and *url*, documenting in plain text and as a web address where the corpus was obtained; *author*, even when there was no single author of the documents; and *keywords*, a vector of keywords categorising the corpus. We recommend using this scheme for new corpus object that you create.\n:::\n\nJust in the decision to selecting which documents should be included in the corpus, the text analyst must also think carefully about what will define a document. Sometimes, this decision is natural, such recording individual product reviews, individual speeches (as in the Presidential inaugural speech corpus), or individual social media posts. Because the definition of a document can be fluid, however, not all such decisions are so clear cut. A user might wish to cut some longer documents into sub-sections (like chapters of a book, or paragraphs of a speech) that will form documents. Or, when facing lots of smaller texts such as posts on Twitter, a user might wish to aggregate these by user, or by day or week, to define new documents. As with the decision to select texts for a corpus, the decision as to precisely how documents should be defined will depend on a user's needs for a specific project. Sometimes, this will produce a need to redefine the document units in which texts were collected, into document units that resemble the units that the user will analyse for the purposes of a broader study. In their study about the association between emotions and real-time voter reactions, for instance, @boussalis21reactions redefine the speech unit of documents in which campaign speeches where found, into specific utterances that were later analysed as specific statements associated with variable levels of sentiment. In reviews of hotels, the unit could be an individual review of a hotel, the mention of the hotel and its immediate context, or all texts that review a hotel. The point is that the document units in which texts are collected may not be the same as the document units that a text researcher will need for the purposes of their analysis. In the section that follows, we will show to reshape, split, and combine document units from a corpus that allow a flexible redefinition of document-level units to meet specific analytic needs. We also cover how to access and modify all elements of the corpus mentioned in this section.\n\n::: {.callout-important appearance=\"simple\"}\nSome tools for text analysis call for \"cleaning\" a corpus, sometimes by removing elements that are deemed to be unwanted, such as punctuation. We strongly discourage this, because such radical interventions lessen the generality of a corpus. We take the same approach to defining documents: We prefer that a corpus contain *natural* units of analysis, such a individual reviews, rather than *analytic* units of analysis such as combined or aggregated reviews. @sec-quanteda-tokensadvanced and @sec-quanteda-dfms show to combine documents to the *analytic* unit of analysis using `tokens_group()` or `dfm_group()`. Cleaning should be limited to removing *textual cruft*, such as page numbers if a document was converted from pdf format, and these are not of interest.\n:::\n\n## Applications\n\n::: {.callout-important appearance=\"simple\"}\n**Authors' note:** We should move the segmentation to the Advanced section, and use the Applications section for:\n\n-   [x] Creating a corpus from different input sources, including some minimal cleanup\n-   [x] Accessing and assigning docvars\n-   [x] Accessing and assigning corpus metadata\n-   [x] `corpus_subset()`, from below, using `data_corpus_inaugural`\n-   [x] `corpus_sample()`, using `data_corpus_inaugural`\n:::\n\nIn this section, we will demonstrate how to create a corpus from different input sources, how to access and assign docvars and corpus metadata, how to subset documents from a corpus based on document-level variables, and how to draw a random sample of documents from a text corpus. We will use the corpus of US inaugural speeches, which is included in the **quanteda** package as `data_corpus_inaugural`.\n\n### Creating a Corpus from Different Input Sources\n\nWe can create a *quanteda* corpus from different input sources, for example a data frame, a **tm** corpus object, or a keyword-in-context object (`kwic`).\n\n::: {.callout-note appearance=\"simple\"}\nIn Chapter @sec-acquire-files we show how users can import texts in various formats, e.g., PDF documents, Word documents, or spreadsheets with the **readtext** package. The output of the `readtext()` function is always a data frame, which can be easily transformed to a corpus object using `corpus()`.\n:::\n\nCreating a corpus from a data frame or tibble object is straightforward. We use `corpus()` and determine the `text_field` that contains the text. By default, all remaining variables of the data frame will be added as docvars.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"quanteda\")\n\n# create data frame for illustration purposes\ndat <- data.frame(\n    letter_factor = factor(rep(letters[1:3], each = 2)),\n    some_ints = 1L:6L,\n    some_text = paste0(\"This is text number \", 1:6, \".\"),\n    stringsAsFactors = FALSE,\n    row.names = paste0(\"fromDf_\", 1:6))\n\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         letter_factor some_ints              some_text\nfromDf_1             a         1 This is text number 1.\nfromDf_2             a         2 This is text number 2.\nfromDf_3             b         3 This is text number 3.\nfromDf_4             b         4 This is text number 4.\nfromDf_5             c         5 This is text number 5.\nfromDf_6             c         6 This is text number 6.\n```\n:::\n\n```{.r .cell-code}\n# create corpus\ncorp_dataframe <- corpus(dat, text_field = \"some_text\",\n                         meta = list(source = \"From a data.frame called dat.\"))\n\nsummary(corp_dataframe)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 6 documents, showing 6 documents:\n\n     Text Types Tokens Sentences letter_factor some_ints\n fromDf_1     6      6         1             a         1\n fromDf_2     6      6         1             a         2\n fromDf_3     6      6         1             b         3\n fromDf_4     6      6         1             b         4\n fromDf_5     6      6         1             c         5\n fromDf_6     6      6         1             c         6\n```\n:::\n:::\n\n\nWe can also create a **quanteda** corpus from a **tm** corpus object. The following example uses the corpus `crude` corpus from the **tm** package which contains 20 exemplary news articles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load in a tm example VCorpus object\ndata(crude, package = \"tm\")\n\n# create quanteda corpus\ncorp_newsarticles <- corpus(crude)\n```\n:::\n\n\nIn addition, we can create a text corpus from a `kwic()` object. Keywords-in-context is covered extensively in Chapter @sec-exploring-kwic.\n\nFor example, we could extract all mentions of \"terrible\", \"awful\", and \"disgusting\", and the immediate context of 20 words from our corpus of hotel reviews, and convert the output to a new text corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"TAUR\")\n\nkw_TAhotels <- data_corpus_TAhotels |> \n    tokens(remove_separators = FALSE) |> \n    kwic(pattern = c(\"terrible\", \"awful\",\n                     \"disgusting\"),\n         window = 20,\n         separator = \" \")\n\n# check number of matches\nnrow(kw_TAhotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1181\n```\n:::\n\n```{.r .cell-code}\n# check how often each pattern was matched\ntable(kw_TAhotels$pattern)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  terrible      awful disgusting \n       642        387        152 \n```\n:::\n\n```{.r .cell-code}\n# convert kwic object to a new text corpus\ncorp_kwic <- corpus(kw_TAhotels, split_context = FALSE)\n\nprint(corp_kwic, max_ndoc = 5, max_nchar = 40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1,181 documents and 1 docvar.\ntext66.L5 :\n\"hated   inn  terrible,   room-service   ...\"\n\ntext98.L1 :\n\"terrible  hotel   approximately   2   we...\"\n\ntext111.L304 :\n\"  downtown   pike   place   waterfront ,...\"\n\ntext142.L1 :\n\"terrible  experience   awful   night   s...\"\n\ntext142.L5 :\n\"terrible   experience  awful  night   st...\"\n\n[ reached max_ndoc ... 1,176 more documents ]\n```\n:::\n:::\n\n\n### Inspecting Document-Level Variables and Metadata of a Corpus\n\nDocument-level variables are crucial for many text analysis projects. For instance, if we want to compare differences in issue emphasis in inaugural before and after World War II, we need a document-level variable specifying the year when a speech was delivered. We may also want to create a binary variable indicating whether a speech was delivered before or after World War II. Below, we provide examples on how to inspect document-level variables and how to create new docvars.\n\nFirst, we load the packages and inspect the text corpus using `summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# provide summary of corpus and print document-level variables for the first six documents\nsummary(data_corpus_inaugural, n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 59 documents, showing 6 documents:\n\n            Text Types Tokens Sentences Year  President FirstName\n 1789-Washington   625   1537        23 1789 Washington    George\n 1793-Washington    96    147         4 1793 Washington    George\n      1797-Adams   826   2577        37 1797      Adams      John\n  1801-Jefferson   717   1923        41 1801  Jefferson    Thomas\n  1805-Jefferson   804   2380        45 1805  Jefferson    Thomas\n    1809-Madison   535   1261        21 1809    Madison     James\n                 Party\n                  none\n                  none\n            Federalist\n Democratic-Republican\n Democratic-Republican\n Democratic-Republican\n```\n:::\n:::\n\n\nThe corpus consists of 59 documents. Each document is one inaugural speech delivered between 1789 and 2021.\n\nThe `meta()` function returns a named list containing the corpus-level information stored in the corpus, reflecting the standard set of metadata fields that we have chosen to use for **quanteda** objects. The \"keywords\" element of this list is a character vector containing five keywords.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta(data_corpus_inaugural)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$description\n[1] \"Transcripts of all inaugural addresses delivered by United States Presidents, from Washington 1789 onward.  Data compiled by Gerhard Peters.\"\n\n$source\n[1] \"Gerhard Peters and John T. Woolley. The American Presidency Project.\"\n\n$url\n[1] \"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses\"\n\n$author\n[1] \"(various US Presidents)\"\n\n$keywords\n[1] \"political\"     \"US politics\"   \"United States\" \"presidents\"   \n[5] \"presidency\"   \n\n$title\n[1] \"US presidential inaugural address speeches\"\n```\n:::\n:::\n\n\nWe can also access the names of our docvars.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(docvars(data_corpus_inaugural))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Year\"      \"President\" \"FirstName\" \"Party\"    \n```\n:::\n:::\n\n\nWe see that the `Year` variable stores the year when each speech was delivered. We could create a binary variable `PrePostWW2` which distinguishes between speeches held before and after 1945. T\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use $ operator\ndata_corpus_inaugural$PrePostWW2 <- ifelse(\n    data_corpus_inaugural$Year > 1945, \n    \"Post World War II\", \"Pre World War II\")\n\n# equivalent to\ndocvars(data_corpus_inaugural, \"PrePostWW\") <- ifelse(\n    docvars(data_corpus_inaugural, \"Year\") > 1945, \n    \"Post World War II\", \"Pre World War II\")\n```\n:::\n\n\n::: {.callout-note appearance=\"simple\"}\nThere are multiple ways to access docvars in a **quanteda** object---here a corpus, but also other objects derived from a corpus such as tokens object, or separate objects such as dictionaries. These can be `docvars(data_corpus_inaugural, \"Party\")` or, for individual docvars, using the `$` operator known from lists or data.frames, i.e. `data_corpus_inaugural$Party`.\n:::\n\n### Subsetting a Text Corpus\n\nApplying `corpus_subset()` to a corpus allows us to subset a text corpus based on values of document-level variables. We can filter, for instance, only speeches delivered by Democratic Presidents, speeches delivered after World War II, or speeches from selected presidents.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# subset only Republican Presidents\ncorp_democrats <- corpus_subset(data_corpus_inaugural, Party == \"Republican\")\n\n# subset speeches delivered after WW2\ncorp_postww2 <- corpus_subset(data_corpus_inaugural, PrePostWW2 == \"Post World War II\")\n\ncorp_postww2 <- corpus_subset(data_corpus_inaugural, Year > 1945)\n\n# subset speeches delivered by Clinton, Obama, and Biden\ncorp_cob <- corpus_subset(data_corpus_inaugural, President %in% c(\"Clinton\", \"Obama\", \"Biden\"))\n\n# remove Biden's 2021 speech\ncorp_nobiden <- corpus_subset(data_corpus_inaugural, President != \"Biden\")\n```\n:::\n\n\n::: {.callout-tip appearance=\"simple\"}\nUsually, we use logical operators for subsetting a text corpus or creating a new document-level variable based on certain conditions. The most relevant logical operators are:\n\n-   `<`: less than\n-   `<=`: less than or equal to\n-   `>`: greater than\n-   `>=`: greater than or equal to\n-   `==`: equal\n-   `!=`: not equal\n-   `!x`: not x (negation)\n-   `x | y`: x OR y\n-   `x & y`: x AND y\n:::\n\n### Randomly Sample Documents From a Corpus\n\nOften, users may be interested in taking a random sample of documentsf from the text corpus. For example, a user may implement and test the workflow for a small, random subset of the documents, and only later run the analysis on the full text corpus.\n\nThe `corpus_sample()` function allows for sampling documents from a specified size (`size`) with or without replacement (`replace`), optionally stratified by grouping variables (`by`) or with probability weights (`prob`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample 30 inaugural speeches without replacement\ncorp_sample30 <- corpus_sample(data_corpus_inaugural, size = 30, replace = FALSE)\n\n# check that the corpus consists of 30 documents\nndoc(corp_sample30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 30\n```\n:::\n\n```{.r .cell-code}\n# sample 15 speeches from before and 15 from after World War I (30 documents in total)\ncorp_sample_prepost <- corpus_sample(data_corpus_inaugural, size = 15, \n                                     replace = FALSE,\n                                     by = PrePostWW2)\n\n# check that corpus contains 15 pre- and 15 post-WW II documents\ntable(corp_sample_prepost$PrePostWW2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPost World War II  Pre World War II \n               15                15 \n```\n:::\n:::\n\n\n## Advanced\n\n### Corpus \"Cleaning\"\n\nWhy we favour a \"conservative\" approach, including storing documents in their natural format rather than an analytical format that can be created from the natural units, and especially not intervening to change documents in a corpus (such as removing punctuation).\n\nWe can discuss what constitutes recommended cleaning, and provide an example, perhaps from the debate transcript example.\n\n### The Structure of a **quanteda** Corpus Object\n\nTo be added...\n\n-   the structure of a corpus object, which is basically a character vector with special attributes.\n-   Why to preserve these we need \\[\\] even when it's mycorpus\\[seq_len(mycorpus)\\] \\<- some_character_transformation\n\n### Conversion from and to Other Formats\n\nBelow, we provide examples of converting a **qunateda** corpus to other objects. Converting a `corpus` object to a data frame could be useful, for instance, if you want to reshape a corpus to sentences and hand-code a sample of these sentences for validation purposes or a training set for supervised classifiers (see @sec-ml-classifiers). The example below shows how to sample 1,000 hotel reviews, and converting this corpus to a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducibility\nset.seed(35)\n\ncorp_TAhotels_sample <- corpus_sample(data_corpus_TAhotels, \n                                      size = 1000, \n                                      replace = FALSE)\n\n# convert corpus to data frame\ndat_TAhotels_sample <- convert(corp_TAhotels_sample, to = \"data.frame\")\n```\n:::\n\n\nwhich will consist of the columns `doc_id`, containing the document name; `text`, containing the actual text of the document; and any docvars that were in the corpus, which in this example consist of `location` and `date`.\n\nThe resulting data frame consists of 1000 randomly sampled reviews and 3 columns: `doc_id`, containing the document name; `text`, containing the actual text of the document; and any docvars that were in the corpus (in our case `Rating`).\n\nWe can easily store this data frame as a spreadsheet, which can be useful when hand-code a selection of reviews (for instance, based on their sentiment).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store data frame as csv file with UTF-8 encoding and remove row names\nwrite.csv(dat_TAhotels_sample_base_r, \n          file = \"dat_TAhotels_sample.csv\",\n          fileEncoding = \"UTF-8\",\n          row.names = FALSE)\n\n# the rio package allows us to store \n# the data frame in many different file formats\nlibrary(\"rio\")\n\nexport(dat_TAhotels_sample_base_r,\n       file = \"dat_TAhotels_sample.xlsx\")\n```\n:::\n\n\n### Converting or Coercing to Other Object Types\n\nWe can coerce an object to a text corpus (`as.corpus()`) or extract the texts from a corpus (`as.character()`). `as.corpus()` can only be applied to a **quanteda** corpus object and upgrades it to the newest format. This function can be handy for researchers who work with older **quanteda** objects. For transforming data frames or **tm** corpus objects into a **quanteda** corpus, you should use the `corpus()` function. The function `as.character()` returns the corpus text as a plain character vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchars_TAhotels <- as.character(data_corpus_TAhotels)\n\n# inspect character object\nstr(chars_TAhotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Named chr [1:20491] \"nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous re\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:20491] \"text1\" \"text2\" \"text3\" \"text4\" ...\n```\n:::\n:::\n\n\n### Changing the Unit of Analysis\n\nIn this section we will demonstrate the construction of a corpus that requires some reshaping. For this purpose, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.[^09-quanteda-corpus-1] The corpus **TAUR** package contains this text corpus as `data_corpus_debates`.\n\n[^09-quanteda-corpus-1]: The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# inspect text corpus\nsummary(data_corpus_debates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 2 documents, showing 2 documents:\n\n              Text Types Tokens Sentences             location       date\n Debate 2020-09-29  2450  24847      2039      Cleveland, Ohio 2020-09-29\n Debate 2020-10-22  2400  21727      1488 Nashville, Tennessee 2020-10-22\n```\n:::\n:::\n\n\nThe output of `summary(data_corpus_debates)` reveals that the corpus contains only two documents, i.e. the full debate transcript. The first document contains the transcript of the debate in Cleveland, consisting of 24,847 tokens, 2,450 types (i.e., unique tokens), and 2,039 sentences. The second debate in Nashville is slightly shorter (21,727 tokens and 2,400 types).\n\n::: {.callout-note appearance=\"simple\"}\n\"Types\" and \"tokens\" are specific linguistic terms that refer to the quantity and quality of the linguistic units found in a text. A *type* is a unique lexical unit, and a *token* is any lexical unit. Lexical units are most often words, but may also include punctuation characters, numerals, emoji, or even spaces. We cover this in greater detail in [Chapter @sec-quanteda-tokens].\n:::\n\nThe debate corpus contains only two documents, each a transcript of a length debate that included many different statements by the presidential candidates and the moderator. The corpus recorded these \"natural\" speech units as a document, but in the case of transcripts, that is often not the \"analytical\" document unit that will meet the analyst's needs.\n\nTo make the document unit align with our analytical purposes, we will need to reshape the corpus by segmenting it into individual statements, recording the speaker and the sequence in which the statement occurred, as new document-level variables.\n\nThe tool for this segmentation in **quanteda** is `corpus_segment()`. As discussed in [Chapter @sec-quanteda-overview], the name of this function reflects the object that it takes as an input and produces as an output (a corpus) and the verb element describes the operation it will perform (segmentation). To perform this segmentation, we will rely on the presence in the transcript of regular patterns marking the introduction of a new statement. **quanteda** can take multiple forms of patterns, including a fixed match, a \"glob\" match, and a full regular expression match. Because we need to match a very specific pattern with more elements than a simple glob pattern can handle, we will need to use the regular expression \"valuetype\". (For a brief primer on pattern matching and regular expressions see [Appendix -@sec-appendix-regex].)\n\nIn the transcript, a statement starts with the speaker's name in ALL CAPS, followed by a colon. To match this marker of a new statement, we will use the regular expression `\"\\\\s*[[:upper:]]+:\\\\s+\"`. This identifies speaker names in ALL CAPS (`\\\\s*[[:upper:]]+`), indicating no or more spaces followed by one or more upper case words, followed by a colon (the literal`:`), followed by one or more white spaces `\\\\s+`. The `case_insensitive = FALSE` tells the pattern matcher to pay attention to case (how a word's letters are capitalised or not).\n\nBy default, `corpus_segment()` will split the original documents into smaller documents corresponding to the segments preceded by the pattern, and extract the pattern found to a new variable for the split document (a docvar named `pattern`). It is also smart enough to record the original, longer document from which each split document originally came.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# segment text corpus to level of utterances\ndata_corpus_debatesseg <- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\n# overview of text corpus; n = 4 prints only the first four documents\nsummary(data_corpus_debatesseg, n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1208 documents, showing 4 documents:\n\n                Text Types Tokens Sentences        location       date\n Debate 2020-09-29.1   139    251        13 Cleveland, Ohio 2020-09-29\n Debate 2020-09-29.2     6      6         1 Cleveland, Ohio 2020-09-29\n Debate 2020-09-29.3     5      5         1 Cleveland, Ohio 2020-09-29\n Debate 2020-09-29.4     3      3         1 Cleveland, Ohio 2020-09-29\n     pattern\n   WALLACE: \n \\n\\nBIDEN: \n \\n\\nTRUMP: \n \\n\\nBIDEN: \n```\n:::\n:::\n\n\nThe new, split corpus, has turned the 2 original documents into 1,208 new docments. Note also that we have kept the **quanteda** naming conventions in assigning the new object a name that identifies it as data, as a corpus, and describes what it is. Because any function in **quanteda** that begins with `corpus_` will always take in and output a corpus, we know that this object will be a corpus.\n\nIn the split document, the new docvar `pattern` contains the pattern that we matched. Because this also included additional elements such as the spaces and the colon, however, it's not as clean as we would prefer. Let's turn this into a new `speaker` document-level variable by cleaning it up and renaming it. To make this easy, we will use the **stringr** for string manipulation, and the **quanteda.tidy** package for applying some **dplyr** functions from the \"tidyverse\" to manipulate the variables. Specifically, we will use `mutate()` to create a new `speaker` variable, and the **stringr** functions to remove empty whitespace (`str_trim()`) and the colon (`str_remove_all()`), and to change the names from UPPER CASE to Title Case (`str_to_title()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"stringr\")\nlibrary(\"quanteda.tidy\")\n\ndata_corpus_debatesseg <- data_corpus_debatesseg |> \n    mutate(speaker = stringr::str_trim(pattern),\n           speaker = stringr::str_remove_all(speaker, \":\"),\n           speaker = stringr::str_to_title(speaker)) \n```\n:::\n\n\nNext, we can use simple base R functions to inspect the count of utterances by speaker and debate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$location,\n      data_corpus_debatesseg$speaker)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                      \n                       Biden Trump Wallace Welker\n  Cleveland, Ohio        269   341     246      0\n  Nashville, Tennessee    84   122       0    146\n```\n:::\n:::\n\n\nWe could further reshape the corpus to the level of sentences with `corpus_reshape()` if we are interested, for instance, in sentence-level sentiment or issue salience.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatessent <- corpus_reshape(data_corpus_debatesseg,\n                                          to = \"sentences\")\n\nndoc(data_corpus_debatessent)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3560\n```\n:::\n:::\n\n\nThe new text corpus moved from 1208 utterances to 3560 sentences. Using functions such as **quanteda.textstat**'s `textstat_summary()` we can retrieve summary statistics of the sentence-level corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"quanteda.textstats\")\ndat_summary_sents <- textstat_summary(data_corpus_debatessent)\n\n# aggregated summary statistics\nsummary(dat_summary_sents)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   document             chars            sents       tokens         types      \n Length:3560        Min.   :  3.00   Min.   :1   Min.   : 1.0   Min.   : 1.00  \n Class :character   1st Qu.: 24.00   1st Qu.:1   1st Qu.: 6.0   1st Qu.: 6.00  \n Mode  :character   Median : 41.00   Median :1   Median :10.0   Median : 9.00  \n                    Mean   : 55.32   Mean   :1   Mean   :12.4   Mean   :10.85  \n                    3rd Qu.: 73.00   3rd Qu.:1   3rd Qu.:16.0   3rd Qu.:14.00  \n                    Max.   :414.00   Max.   :1   Max.   :79.0   Max.   :56.00  \n     puncts          numbers           symbols             urls        tags  \n Min.   : 0.000   Min.   :0.00000   Min.   :0.00000   Min.   :0   Min.   :0  \n 1st Qu.: 1.000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0   1st Qu.:0  \n Median : 2.000   Median :0.00000   Median :0.00000   Median :0   Median :0  \n Mean   : 2.111   Mean   :0.07893   Mean   :0.01376   Mean   :0   Mean   :0  \n 3rd Qu.: 3.000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0   3rd Qu.:0  \n Max.   :15.000   Max.   :4.00000   Max.   :2.00000   Max.   :0   Max.   :0  \n     emojis \n Min.   :0  \n 1st Qu.:0  \n Median :0  \n Mean   :0  \n 3rd Qu.:0  \n Max.   :0  \n```\n:::\n:::\n\n\n::: {.callout-tip appearance=\"simple\"}\nSegmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker's surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected.\n:::\n\n### Reshaping Corpora after Statistical Analyis of Texts\n\nIn many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, @castanhosilva22eu study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models.\n\n::: {.callout-tip appearance=\"simple\"}\nApplying the `group_by()` in combination with the `summarise()` functions of the **dplyr** packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis.\n:::\n\n## Further Reading\n\n-   Selecting document and considerations of \"found data\": @grimmer22textasdata [ch. 4]\n-   Definitions of document units and the reasons for and implications of redefining these: @benoit2020text [pp480--481, \"Defining Documents and Choosing the Unit of Analysis\"]\n-   Adjusting strings: @wickham17r4ds [ch. 14]\n\n## Exercises\n\nIn the exercises below, we use a corpus of TripAdvisor hotel reviews, `data_corpus_TAhotels`, included in the **TAUR** package).\n\n1.  Identify the number of documents in `data_corpus_TAhotels`.\n2.  Subset the corpus by selecting only reviews with the maximum rating of 5.\n3.  Reshape this subsetted corpus to the level of sentences.\n4.  Explore `textstat_summary()` of the **quanteda.textstats** package. Apply the function to `data_corpus_TAhotels` and assign it to an object called `tstat_sum_TAhotels`.\n5.  What are the average, median, minimum, and maximum document lengths?\n6.  Advanced: use `data_corpus_TAhotels` filter only speeches consisting of at least 300 tokens by combining `ntoken()` and `corpus_subset()`.\n7.  Create a new document-level variable `RankingRecoded` that which splits up the rankings into three categories: Negative (Ranking = 1 and 2), Neutral (Ranking = 3), and Positive (Ranking = 4 and 5). Which of the three categories is the most frequent one?\n",
    "supporting": [
      "09-quanteda-corpus_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}