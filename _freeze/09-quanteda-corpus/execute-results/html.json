{
  "hash": "20833d7be3da92da35e2e0a90c93d73e",
  "result": {
    "markdown": "# Creating and Managing Corpora {#sec-quanteda-corpus}\n\n## Objectives\n\nIn this section, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus.\n\n## Methods\n\nText analysis projects involve choices about the texts to be analyzed. Texts are collected in a *text corpus*. Text corpora usually consist of three elements: the texts, document-level variables, and metadata about the corpus overall.\n\nCreating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. A text corpus could include all articles published on immigration in Irish newspapers, with each article recorded as one *document* in the text corpus. A corpus could also be the reviews about one specific hotel, or a random sample of reviews about hotels in Europe. Researchers need to consider and justify why certain texts are (not) included in a corpus, since the generalisability and validity of findings can depend on the documents selected for analysis.\n\nThe principles of your specific project or research design should guide your decisions to include or exclude of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in televised debates and campaign speeches, the corpus will contain debate transcripts and speeches. Thus, the research question should drive document selection.\n\nIn some text analysis applications, the sample could constitute the entirety of texts. Analysing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information not transcribed or published cannot be included in the analysis. When analysing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions [@herzog15cuts]. The positions of politicians not selected for speaking at a budget debate cannot be considered in the textual analysis. Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.\n\n::: {.callout-note appearance=\"simple\"}\nIncluding and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches [@benoit19sophistication]. Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in @sec-exploring-freqs. @sec-ml-topicmodels shows how to identify variation in topic prevalence for two or more groups of documents.\n:::\n\nBesides the raw text, corpora (usually) include attributes that distinguish texts. We call these attributes *document-level variables* or, in the language **quanteda**, \"docvars\". Docvars contain additional information on each document and allow researchers to differentiate between texts in their analysis. Document-level variables could be the name of the author of a text, the newspaper in which an article was published, the hotel which was reviewed on TripAdvisor, or the date when the document was created.\n\nA text corpus also typically contains *metadata* recording important information about the corpus as a whole. Metadata that is generally useful to record in a corpus include the source where the corpus was obtained, including possibly the URL; the author of the corpus (if from a single source); a title for the corpus; and possibly important keywords that might be used later in categorising the corpus. Corpus metadata can be quite general, including a codeboo, instructions how to cite the corpus, or a license for the use of the corpus or copyright information.\n\n::: {.callout-tip appearance=\"simple\"}\nWhile there no universal standard exists for which metadata to record, there are guidelines for corpus objects that are followed in the **quanteda** packages that provide example corpora, including those in the package that accompanies this book. These all record the following metadata fields: *title and* description*, providing a title and short text description of the corpus respectively;* source\\* and *url*, documenting in plain text and as a web address where the corpus was obtained; *author*, even when there was no single author of the documents; and *keywords*, a vector of keywords categorising the corpus. We recommend using this scheme for new corpus object that you create.\n:::\n\nJust in the decision to selecting which documents should be included in the corpus, the text analyst must also think carefully about what will define a document. Sometimes, this decision is natural, such recording individual product reviews, individual speeches (as in the Presidential inaugural speech corpus), or individual social media posts. Because the definition of a document can be fluid, however, not all such decisions are so clear cut. A user might wish to cut some longer documents into sub-sections (like chapters of a book, or paragraphs of a speech) that will form documents. Or, when facing lots of smaller texts such as posts on Twitter, a user might wish to aggregate these by user, or by day or week, to define new documents. As with the decision to select texts for a corpus, the decision as to precisely how documents should be defined will depend on a user's needs for a specific project. Sometimes, this will produce a need to redefine the document units in which texts were collected, into document units that resemble the units that the user will analyse for the purposes of a broader study. In his study about the association between sentiment and real-time voter reactions, for instance, @boussalis21reactions redefines the speech unit of documents in which campaign speeches where found, into specific utterances that were later analysed as specific statements associated with variable levels of sentiment. In reviews of hotels, the unit could be an individual review of a hotel, the mention of the hotel and its immediate context, or all texts that review a hotel. The point is that the document units in which texts are collected may not be the same as the document units that a text researcher will need for the purposes of their analysis. In the section that follows, we will show to reshape, split, and combine document units from a corpus that allow a flexible redefinition of document-level units to meet specific analytic needs. We also cover how to access and modify all elements of the corpus mentioned in this section.\n\n::: {.callout-important appearance=\"simple\"}\nSome tools for text analysis call for \"cleaning\" a corpus, sometimes by removing elements that are deemed to be unwanted, such as punctuation. We strongly discourage this, because such radical interventions lessen the generality of a corpus. We take the same approach to defining documents: We prefer that a corpus contain *natural* units of analysis, such a individual reviews, rather than *analytic* units of analysis such as combined or aggregated reviews. Cleaning should be limited to removing *textual cruft*, such as page numbers if a document was converted from pdf format, and these are not of interest.\n:::\n\n## Applications\n\n::: {.callout-important appearance=\"simple\"}\n**Authors' note:** We should move the segmentation to the Advanced section, and use the Applications section for:\n\n-   [ ] Creating a corpus from different input sources, including some minimal cleanup\n-   [ ] Accessing and assigning docvars\n-   [ ] Accessing and assigning corpus metadata\n-   [ ] `corpus_subset()`, from below, using `data_corpus_inaugural`\n-   [ ] `corpus_sample()`, using `data_corpus_inaugural`\n:::\n\nIn this section we will demonstrate the construction of a corpus that requires some reshaping. For this purpose, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.[^09-quanteda-corpus-1] The corpus **TAUR** package contains this text corpus as `data_corpus_debates`. Following the **quanteda** naming conventions, the object name starts with `data_` (since it contains data), followed by `corpus_` (indicating that the object is a text corpus) and `debates`, describing the content of the text corpus.\n\n[^09-quanteda-corpus-1]: The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0.\n\nFirst, we load the packages and inspect the text corpus using `summary()`.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data_corpus_debates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 2 documents, showing 2 documents:\n\n               Text Types Tokens Sentences\n Debate: 2020-09-29  2537  24473      1926\n Debate: 2020-10-22  2378  21579      1415\n                                           location       date\n Case Western Reserve University in Cleveland, Ohio 2020-09-29\n         Belmont University in Nashville, Tennessee 2020-10-22\n```\n:::\n:::\n\n\nThe corpus consists of 2 documents. The output of `summary(data_corpus_debates)` also reveals that each document is the full debate transcript. The first document contains the transcript of the debate in Cleveland, consisting of 24,473 tokens, 2,537 types (i.e., unique tokens), and 1,926 sentences. The second debate in Nashville is slightly shorter (21,579 tokens and 2,378 types).\n\n::: {.callout-note appearance=\"simple\"}\n\"Types\" and \"tokens\" are specific linguistic terms that refer to the quantity and quality of the linguistic units found in a text. A *type* is a unique lexical unit, and a *token* is any lexical unit. Lexical units are most often words, but may also include punctuation characters, numerals, emoji, or even spaces. We cover this in greater detail in [Chapter -@sec-tokens].\n:::\n\nThe `meta()` function returns a named list containing the corpus-level information stored in the corpus, reflecting the standard set of metadata fields that we have chosen to use for **quanteda** objects. The \"keywords\" element of this list is a character vector containing seven keywords.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeta(data_corpus_debates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$description\n[1] \"2020 US Presidential Debates between Donald J. Trump pand Joe Biden\"\n\n$source\n[1] \"The Presidential Presidency Project\"\n\n$url\n[1] \"https://www.presidency.ucsb.edu\"\n\n$author\n[1] \"UC Santa Barbara\"\n\n$keywords\n[1] \"political\"         \"US politics\"       \"United States\"    \n[4] \"presidents\"        \"Trump\"             \"Biden\"            \n[7] \"televised debates\"\n\n$title\n[1] \"2020 US Presidential Debates\"\n```\n:::\n:::\n\n\n### Changing the Unit of Analysis\n\nThe debate corpus contains only two documents, each a transcript of a length debate that included many different statements by the presidential candidates and the moderator. The corpus recorded these \"natural\" speech units as a document, but in the case of transcripts, that is often not the \"analytical\" document unit that will meet the analyst's needs.\n\nTo make the document unit align with our analytical purposes, we will need to reshape the corpus by segmenting it into individual statements, recording the speaker and the sequence in which the statement occurred, as new document-level variables.\n\nThe tool for this segmentation in **quanteda** is `corpus_segment()`. As discussed in [Chapter -@sec-quanteda-overview], the name of this function reflects the object that it takes as an input and produces as an output (a corpus) and the verb element describes the operation it will perform (segmentation). To perform this segmentation, we will rely on the presence in the transcript of regular patterns marking the introduction of a new statement. **quanteda** can take multiple forms of patterns, including a fixed match, a \"glob\" match, and a full regular expression match. Because we need to match a very specific pattern with more elements than a simple glob pattern can handle, we will need to use the regular expression \"valuetype\". (For a brief primer on pattern matching and regular expressions see [Appendix -@sec-appendix-regex].)\n\nIn the transcript, a statement starts with the speaker's name in ALL CAPS, followed by a colon. To match this marker of a new statement, we will use the regular expression `\"\\\\s*[[:upper:]]+:\\\\s+\"`. This identifies speaker names in ALL CAPS (`\\\\s*[[:upper:]]+`), indicating no or more spaces followed by one or more upper case words, followed by a colon (the literal`:`), followed by one or more white spaces `\\\\s+`. The `case_insensitive = FALSE` tells the pattern matcher to pay attention to case (how a word's letters are capitalised or not).\n\nBy default, `corpus_segment()` will split the original documents into smaller documents corresponding to the segments preceded by the pattern, and extract the pattern found to a new variable for the split document (a docvar named `pattern`). It is also smart enough to record the original, longer document from which each split document originally came.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# segment text corpus to level of utterances\ndata_corpus_debatesseg <- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\n# overview of text corpus; n = 4 prints only the first four documents\nsummary(data_corpus_debatesseg, n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1207 documents, showing 4 documents:\n\n                 Text Types Tokens Sentences\n Debate: 2020-09-29.1   139    251        13\n Debate: 2020-09-29.2     6      6         1\n Debate: 2020-09-29.3     5      5         1\n Debate: 2020-09-29.4     3      3         1\n                                           location       date     pattern\n Case Western Reserve University in Cleveland, Ohio 2020-09-29   WALLACE: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29 \\n\\nBIDEN: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29 \\n\\nTRUMP: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29 \\n\\nBIDEN: \n```\n:::\n:::\n\n\nThe new, split corpus, has turned the 2 original documents into 1,207 new docments. Note also that we have kept the **quanteda** naming conventions in assigning the new object a name that identifies it as data, as a corpus, and describes what it is. Because any function in **quanteda** that begins with `corpus_` will always take in and output a corpus, we know that this object will be a corpus.\n\n### Creating New Document-Level Variables\n\nIn the split document, the new docvar `pattern` contains the pattern that we matched. Because this also included additional elements such as the spaces and the colon, however, it's not as clean as we would prefer. Let's turn this into a new `speaker` document-level variable by cleaning it up and renaming it. To make this easy, we will use the **stringr** for string manipulation, and the **quanteda.tidy** package for applying some **dplyr** functions from the \"tidyverse\" to manipulate the variables. Specifically, we will use `mutate()` to create a new `speaker` variable, and the **stringr** functions to remove empty whitespace (`str_trim()`) and the colon (`str_remove_all()`), and to change the names from UPPER CASE to Title Case (`str_to_title()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"stringr\")\nlibrary(\"quanteda.tidy\")\n\ndata_corpus_debatesseg <- data_corpus_debatesseg |> \n    mutate(speaker = stringr::str_trim(pattern),\n           speaker = stringr::str_remove_all(speaker, \":\"),\n           speaker = stringr::str_to_title(speaker)) \n```\n:::\n\n\nNext, we can use simple base R functions to inspect the count of utterances by speaker and debate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$location,\n      data_corpus_debatesseg$speaker)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                    \n                                                     Biden Trump Wallace Welker\n  Belmont University in Nashville, Tennessee            84   122       0    146\n  Case Western Reserve University in Cleveland, Ohio   269   340     246      0\n```\n:::\n:::\n\n\n::: {.callout-note appearance=\"simple\"}\nThere are multiple ways to access docvars in a **quanteda** object---here a corpus, but also other objects derived from a corpus such as tokens object, or separate objects such as dictionaries. These can be `docvars(data_corpus_debatesseg, \"speaker\")` or, for individual docvars, using the `$` operator known from lists or data.frames, i.e. `data_corpus_debatesseg$speaker`.\n:::\n\nWe could further reshape the corpus to the level of sentences with `corpus_reshape()` if we are interested, for instance, in sentence-level sentiment or issue salience.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatessent <- corpus_reshape(data_corpus_debatesseg,\n                                          to = \"sentences\")\n\nndoc(data_corpus_debatessent)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3560\n```\n:::\n:::\n\n\nThe new text corpus moved from 1207 utterances to 3560 sentences. Using functions such as **quanteda.textstat**'s `textstat_summary()` we can retrieve summary statistics of the sentence-level corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"quanteda.textstats\")\ndat_summary_sents <- textstat_summary(data_corpus_debatessent)\n\n# aggregated summary statistics\nsummary(dat_summary_sents)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   document             chars            sents       tokens     \n Length:3560        Min.   :  3.00   Min.   :1   Min.   : 1.00  \n Class :character   1st Qu.: 24.00   1st Qu.:1   1st Qu.: 6.00  \n Mode  :character   Median : 41.00   Median :1   Median : 9.00  \n                    Mean   : 55.31   Mean   :1   Mean   :12.26  \n                    3rd Qu.: 73.00   3rd Qu.:1   3rd Qu.:16.00  \n                    Max.   :414.00   Max.   :1   Max.   :79.00  \n     types           puncts          numbers           symbols       \n Min.   : 1.00   Min.   : 0.000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.: 6.00   1st Qu.: 1.000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median : 9.00   Median : 1.000   Median :0.00000   Median :0.00000  \n Mean   :10.82   Mean   : 1.964   Mean   :0.07949   Mean   :0.01376  \n 3rd Qu.:14.00   3rd Qu.: 3.000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :56.00   Max.   :15.000   Max.   :4.00000   Max.   :2.00000  \n      urls        tags       emojis \n Min.   :0   Min.   :0   Min.   :0  \n 1st Qu.:0   1st Qu.:0   1st Qu.:0  \n Median :0   Median :0   Median :0  \n Mean   :0   Mean   :0   Mean   :0  \n 3rd Qu.:0   3rd Qu.:0   3rd Qu.:0  \n Max.   :0   Max.   :0   Max.   :0  \n```\n:::\n:::\n\n\n### Subsetting a Text Corpus\n\nThe last example in this chapter covers subsetting a text corpus based on document-level variables. Researchers who want to compare Trump and Biden may not be interested in the moderator's statements. We can exclude the moderators with `corpus_subset()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatescand <- data_corpus_debatesseg |> \n    corpus_subset(speaker %in% c(\"Trump\", \"Biden\"))\n\n# check that subsetting worked as expected\ntable(data_corpus_debatescand$location,\n      data_corpus_debatescand$speaker)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                    \n                                                     Biden Trump\n  Belmont University in Nashville, Tennessee            84   122\n  Case Western Reserve University in Cleveland, Ohio   269   340\n```\n:::\n:::\n\n\n## Advanced\n\n::: {.callout-important appearance=\"simple\"}\n**Authors' note:** Segmentation and reshaping should be moved to this section.\n:::\n\n### Corpus \"cleaning\"\n\nWhy we favour a \"conservative\" approach, including storing documents in their natural format rather than an analytical format that can be created from the natural units, and especially not intervening to change documents in a corpus (such as removing punctuation).\n\nWe can discuss what constitutes recommended cleaning, and provide an example, perhaps from the debate transcript example.\n\n### The Structure of a **quanteda** Corpus Object\n\nTo be added...\n\n-   the structure of a corpus object, which is basically a character vector with special attributes.\n-   Why to preserve these we need \\[\\] even when it's mycorpus\\[seq_len(mycorpus)\\] \\<- some_character_transformation\n\n### Conversion from and to Other Formats\n\nWe recommend using a **quanteda** corpus object when starting a text analysis project. The \\*quanteda **package contains various functions for converting a corpus from and to other formats, like a data frame,** tm\\*\\* corpus, or keyword-in-context (`kwic`) object.\n\nBelow, we provide examples of the conversion. Converting a `corpus` object to a data frame could be useful, for instance, if you want to reshape a corpus to sentences and hand-code a sample of these sentences for validation purposes or a training set for supervised classifiers (see @sec-ml-classifiers). The example below shows how to convert the corpus of hotel reviews to a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create data frame containing texts\n# and document-level variables\n\ndat_TAhotels <- data.frame(\n    text = as.character(data_corpus_TAhotels),\n    docvars(data_corpus_TAhotels))\n\ndim(dat_TAhotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20491     2\n```\n:::\n:::\n\n\nThe resulting data frame consists of 20491 observations, which each observation being a hotel review, and 2 variables (`text` and `Rating`). We could also select a sample of observations and store them in a spreadsheet if we wanted to hand-code a selection of reviews (for instance, based on their sentiment).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducibility\nset.seed(125)\n\n# sample 1000 reviews\nn <- 1000\n\n# solution using base R\ndat_TAhotels_sample_base_r <- dat_TAhotels[sample(nrow(dat_TAhotels), n), ]\n\n# solution using sample_n() from the dplyr package\nlibrary(\"dplyr\")\n\nset.seed(125)\ndat_TAhotels_sample_dplyr <- sample_n(dat_TAhotels, size = n)\n\n# check whether both objects are identical\nidentical(dat_TAhotels_sample_base_r, \n          dat_TAhotels_sample_dplyr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# store data frame as csv file with UTF-8 encoding and remove row names\nwrite.csv(dat_TAhotels_sample_base_r, \n          file = \"dat_TAhotels_sample.csv\",\n          fileEncoding = \"UTF-8\",\n          row.names = FALSE)\n\n# the rio package allows us to store \n# the data frame in many different file formats\nlibrary(\"rio\")\n\nexport(dat_TAhotels_sample_base_r,\n       file = \"dat_TAhotels_sample.xlsx\")\n```\n:::\n\n\nWe can also create a `corpus` object from a data frame. The example below shows how to construct a text corpus from the data frame we created above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorp_sample <- corpus(dat_TAhotels_sample_base_r)\n\n# inspect structure of corpus and texts\nprint(corp_sample, max_ndoc = 5, max_nchar = 40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1,000 documents and 1 docvar.\ntext7658 :\n\"beautiful view bryant park hotel stayed ...\"\n\ntext19647 :\n\"lovely hotel enjoyed recent day trip bar...\"\n\ntext2169 :\n\"great place stay breakfast wonderful.loc...\"\n\ntext10083 :\n\"wonderful hotel definitely stay hotel lo...\"\n\ntext7508 :\n\"gave away pre-paid room 3 confirmation p...\"\n\n[ reached max_ndoc ... 995 more documents ]\n```\n:::\n:::\n\n\nIn addition, we can create a text corpus from a `kwic()` object. Keywords-in-context is covered extensively in Chapter @sec-exploring-kwic.\n\nFor example, we could extract all mentions of \"terrible\", \"awful\", and \"disgusting\", and the immediate context of 20 words from the corpus of hotel reviews, and convert the output to a new text corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkw_TAhotels <- data_corpus_TAhotels |> \n    tokens(remove_separators = FALSE) |> \n    kwic(pattern = c(\"terrible\", \"awful\",\n                     \"disgusting\"),\n         window = 20,\n         separator = \" \")\n\n# check number of matches\nnrow(kw_TAhotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1181\n```\n:::\n\n```{.r .cell-code}\n# check how often each pattern was matched\ntable(kw_TAhotels$pattern)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  terrible      awful disgusting \n       642        387        152 \n```\n:::\n\n```{.r .cell-code}\n# convert kwic object to a new text corpus\ncorp_kwic <- corpus(kw_TAhotels, split_context = FALSE)\n\nndoc(corp_kwic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1181\n```\n:::\n\n```{.r .cell-code}\nprint(corp_kwic, max_ndoc = 5, max_nchar = 40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1,181 documents and 1 docvar.\ntext66.L5 :\n\"hated   inn  terrible,   room-service   ...\"\n\ntext98.L1 :\n\"terrible  hotel   approximately   2   we...\"\n\ntext111.L304 :\n\"  downtown   pike   place   waterfront ,...\"\n\ntext142.L1 :\n\"terrible  experience   awful   night   s...\"\n\ntext142.L5 :\n\"terrible   experience  awful  night   st...\"\n\n[ reached max_ndoc ... 1,176 more documents ]\n```\n:::\n:::\n\n\n### Converting or coercing to other object types\n\nWe can also coerce an object to a text corpus (`as.corpus()`) or extract the texts from a corpus (`as.character()`). `as.corpus()` can only be applied to a **quanteda** corpus object and upgrades it to the newest format. This function can be handy for researchers who work with older **quanteda** objects. For transforming data frames or **tm** corpus objects into a **quanteda** corpus, you should use the `corpus()` function. The function `as.character()` returns the corpus text as a plain character vector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchars_TAhotels <- as.character(data_corpus_TAhotels)\n\n# inspect character object\nstr(chars_TAhotels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Named chr [1:20491] \"nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous re\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:20491] \"text1\" \"text2\" \"text3\" \"text4\" ...\n```\n:::\n:::\n\n\nWe can also convert a corpus to a `data.frame`, which will consist of the columns `doc_id`, containing the document name; `text`, containing the actual text of the document; and any docvars that were in the corpus, which in this example consist of `location` and `date`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconvert(data_corpus_debates, to = \"data.frame\") |>\n  pillar::glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 2\nColumns: 4\n$ doc_id   <chr> \"Debate: 2020-09-29\", \"Debate: 2020-10-22\"\n$ text     <chr> \"WALLACE: Good evening from the Health Education Campus of Ca…\n$ location <fct> \"Case Western Reserve University in Cleveland, Ohio\", \"Belmon…\n$ date     <date> 2020-09-29, 2020-10-22\n```\n:::\n:::\n\n\n### Identifying Patterns for Corpus Segmentation\n\nSegmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker's surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected.\n\n### Reshaping Corpora after Statistical Analyis of Texts\n\nIn many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, @castanhosilva22eu study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models.\n\n::: {.callout-tip appearance=\"simple\"}\nApplying the `group_by()` in combination with the `summarise()` functions of the **dplyr** packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis.\n:::\n\n## Further Reading\n\n-   Selecting document and considerations of \"found data\": @grimmer22textasdata [ch. 4]\n-   Definitions of document units and the reasons for and implications of redefining these: @benoit2020text [pp480--481, \"Defining Documents and Choosing the Unit of Analysis\"]\n-   Adjusting strings: @wickham17r4ds [ch. 14]\n\n## Exercises\n\nIn the exercises below, we use a corpus of TripAdvisor hotel reviews, `data_corpus_TAhotels`, included in the **TAUR** package).\n\n1.  Identify the number of documents in `data_corpus_TAhotels`.\n2.  Subset the corpus by selecting only reviews with the maximum rating of 5.\n3.  Reshape this subsetted corpus to the level of sentences.\n4.  Explore `textstat_summary()` of the **quanteda.textstats** package. Apply the function to `data_corpus_TAhotels` and assign it to an object called `tstat_sum_TAhotels`.\n5.  What are the average, median, minimum, and maximum document lengths?\n6.  Advanced: use `data_corpus_TAhotels` filter only speeches consisting of at least 300 tokens by combining `ntoken()` and `corpus_subset()`.\n7.  Create a new document-level variable `RankingRecoded` that which splits up the rankings into three categories: Negative (Ranking = 1 and 2), Neutral (Ranking = 3), and Positive (Ranking = 4 and 5). Which of the three categories is the most frequent one?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}