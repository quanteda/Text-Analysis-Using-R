{
  "hash": "c3c574816fcd911b15b1e4dbd19571f5",
  "result": {
    "markdown": "# Creating and Managing Corpora {#quanteda-corpus}\n\n## Objectives\n\n\nIn this section, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus. \n\n\n## Methods\n\n\nAll text analysis projects involve choices about the texts to be analyzed. Creating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. Afterwards, the texts identified for analysis need to be collected, gathered in a text corpus, and (usually) accompanied by attributes that distinguish texts. Examples include the newspaper, the date of publication, and the article's author [@benoit2020text]. A text corpus could be all articles published on immigration in Irish newspapers, with each article constituting one document in the text corpus. These so-called document-level variables contain additional information on each document and allow researchers to distinguish between texts in their analysis. \n\nIn some text analysis applications, the sample could constitute the entirety of texts. Analyzing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information that are not transcribed or published cannot be included in the analysis. When analyzing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions [@herzog15cuts]. The positions of politicians not selected for speaking at a budget debate, cannot be considered in the textual analysis. PROVIDE ANOTHER EXAMPLE? Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.\n\nThe selection of texts will determine the scope of the analysis, generalizability, and inferences you can draw from your analysis. The principles of your research design should justify the inclusion or exclusion of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in  televised debates and campaign speeches, the corpus will include debate transcripts and speeches. Thus, the research question should drive document selection.\n\n::: {.callout-note appearance=\"simple\"}\nIncluding and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches [@benoit19sophistication]. Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in @sec-exploring-freqs. @sec-ml-topicmodels shows how to identify variation in topic prevalence for two or more groups of documents.\n:::\n\nBesides selecting texts for analysis, researchers need to determine the unit of analysis of the text corpus. The unit of analysis should be driven by your research design. If a researcher is interested in textual features associated with likes and retweets on Twitter, the unit of analysis is an individual tweet. A project about the association between sentiment and real-time voter reactions may shift the unit of analysis could be a candidate's speech utterance [@boussalis21reactions]. \n\n\n## Examples\n\nFor demonstration, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.[^09-quanteda-corpus-1] Following the **quanteda** naming conventinos, the object name starts with `data_` (since it contains data), followed by `corpus_` (indicating that the object is a text corpus) and `debates`, describing the text corpus. \n\n[^09-quanteda-corpus-1]: The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0. \n\n\nFirst, we inspect the text corpus using the `summary()` and `ndoc()` functions.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data_corpus_debates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 2 documents, showing 2 documents:\n\n               Text Types Tokens Sentences\n Debate: 2020-09-29  2565  24548      1928\n Debate: 2020-10-22  2413  21652      1417\n                                           location       date\n Case Western Reserve University in Cleveland, Ohio 2020-09-29\n         Belmont University in Nashville, Tennessee 2020-10-22\n```\n:::\n\n```{.r .cell-code}\nndoc(data_corpus_debates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\n\nThe corpus consists of 2 documents. When inspecting the output of `summary(data_corpus_debates)` reveals that each document currently is the full transcript (Biden, Trump, and moderator) of a debate. The first document, containing the transcripts of the debate in Cleveland contains 24,548 tokens, 2565 types (i.e., unique tokens) and 1928 sentences. The second debate in Nashville is slightly shorter (21,652 tokens and 2,413 types). \n\n\n### Changing the Unit of Analysis\n\nWhen analyzing debates, researchers often move to the level of utterances. We can achieve this using `corpus_segment()`. In the transcript, an utterance starts with the speaker's name in ALL CAPS, followed by a colon. The regular expression `\"\\\\s*[[:upper:]]+:\\\\s+\"` identifies speaker names in ALL CAPS (`\\\\s*[[:upper:]]+`), followed by a colon `+:` and a white space `\\\\s+`. For a primer on regular expression see @sec-appendix-regex. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# segment text corpus to level of utterances\ndata_corpus_debatesseg <- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\n# overview of text corpus; n = 4 prints only the first four documents\nsummary(data_corpus_debatesseg, n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1207 documents, showing 4 documents:\n\n                 Text Types Tokens Sentences\n Debate: 2020-09-29.1   139    251        13\n Debate: 2020-09-29.2     6      6         1\n Debate: 2020-09-29.3     5      5         1\n Debate: 2020-09-29.4     3      3         1\n                                           location       date       pattern\n Case Western Reserve University in Cleveland, Ohio 2020-09-29 \\n\\nWALLACE: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29   \\n\\nTRUMP: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n```\n:::\n\n```{.r .cell-code}\nndoc(data_corpus_debatesseg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1207\n```\n:::\n:::\n\n\n\n### Creating New Document-Level Variables\n\nThe new corpus consists of 1207 utterances by the moderators and candidates. The document-level variable `pattern` assigned the speaker name to each document. We can create a new `speaker` document-level variable by combining functions from the **stringr** and **quanteda.tidy** packages: `mutate()` creates a new `speaker` variable, and the stringr functions remove empty whitespaces (`str_trim()`), the colon (`str_remove_all()`) and change the names from UPPER CASE to Title Case (`str_to_title()`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"stringr\")\nlibrary(\"quanteda.tidy\")\n\ndata_corpus_debatesseg <- data_corpus_debatesseg |> \n    mutate(speaker = stringr::str_trim(pattern),\n           speaker = stringr::str_remove_all(speaker, \":\"),\n           speaker = stringr::str_to_title(speaker)) \n```\n:::\n\n\n\nNext, we can use simple base R functions to inspect the count of utterances by speaker and debate. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$location,\n      data_corpus_debatesseg$speaker)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                    \n                                                     Biden Trump Wallace Welker\n  Belmont University in Nashville, Tennessee            84   122       0    146\n  Case Western Reserve University in Cleveland, Ohio   269   340     246      0\n```\n:::\n:::\n\n\n\nWe could further reshape the corpus to the level of sentences with `corpus_reshape()` if we are interested, for instance, in sentence-level sentiment or issue salience. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatessent <- corpus_reshape(data_corpus_debatesseg,\n                                          to = \"sentences\")\n\nndoc(data_corpus_debatessent)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3564\n```\n:::\n:::\n\n\nThe new text corpus moved from 1207 utterances to 3564 sentences. Using functions such as **quanteda.textstat**'s `textstat_summary()` we can retrieve summary statistics about each sentence.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"quanteda.textstats\")\ndat_summary_sents <- textstat_summary(data_corpus_debatessent)\n\n# aggregated summary statistics\nsummary(dat_summary_sents)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   document             chars            sents       tokens     \n Length:3564        Min.   :  3.00   Min.   :1   Min.   : 1.00  \n Class :character   1st Qu.: 24.00   1st Qu.:1   1st Qu.: 6.00  \n Mode  :character   Median : 41.00   Median :1   Median : 9.00  \n                    Mean   : 55.46   Mean   :1   Mean   :12.27  \n                    3rd Qu.: 73.00   3rd Qu.:1   3rd Qu.:16.00  \n                    Max.   :414.00   Max.   :1   Max.   :79.00  \n     types           puncts          numbers          symbols       \n Min.   : 1.00   Min.   : 0.000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 6.00   1st Qu.: 1.000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median : 9.00   Median : 1.000   Median :0.0000   Median :0.00000  \n Mean   :10.83   Mean   : 1.965   Mean   :0.0794   Mean   :0.01543  \n 3rd Qu.:14.00   3rd Qu.: 3.000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :56.00   Max.   :15.000   Max.   :4.0000   Max.   :3.00000  \n      urls                tags       emojis \n Min.   :0.0000000   Min.   :0   Min.   :0  \n 1st Qu.:0.0000000   1st Qu.:0   1st Qu.:0  \n Median :0.0000000   Median :0   Median :0  \n Mean   :0.0005612   Mean   :0   Mean   :0  \n 3rd Qu.:0.0000000   3rd Qu.:0   3rd Qu.:0  \n Max.   :1.0000000   Max.   :0   Max.   :0  \n```\n:::\n:::\n\n\n### Subset a Text Corpus\n\nThe last example in this chapter covers subsetting a text corpus based on document-level variables. Researchers who want to compare Trump and Biden may not be interested in the moderator's statements. We can exclude the moderators with `corpus_subset()`. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_corpus_debatescand <- corpus_subset(data_corpus_debatesseg,\n                                          speaker %in% c(\"Trump\", \"Biden\"))\n\n# check that subsetting worked as expected\ntable(data_corpus_debatescand$speaker,\n      data_corpus_debatescand$location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       \n        Belmont University in Nashville, Tennessee\n  Biden                                         84\n  Trump                                        122\n       \n        Case Western Reserve University in Cleveland, Ohio\n  Biden                                                269\n  Trump                                                340\n```\n:::\n:::\n\n\n\n\n## Issues\n\n### Identifying Patterns for Corpus Segmentation\n\nSegmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker's surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected. \n\n\n### Reshaping Corpora after Statistical Analyis of Texts\n\nIn many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, @castanhosilva22eu study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models. \n\n@mueller22temporal studied the temporal focus of parties' campaign communication. First, he reshaped party manifestos to the level of sentences before identifying the temporal focus of each sentence using supervised classification (@sec-ml-classifiers). The regression models use an aggregated dataset with only three observations per manifesto (sentiment in sections about the past, present, and future).\n\n\n::: {.callout-tip appearance=\"simple\"}\nApplying the `group_by()` in combination with the `summarise()` functions of the **dplyr** packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis.\n:::\n\n\n## Further Reading\n\n- Selecting document and considerations of \"found data\": @grimmer22textasdata [ch. 4]\n- Adjusting strings: @wickham17r4ds [ch. 14]\n\n## Exercises\n\nIn the exercises below, we use a corpus of speeches from the 2017 UN General Debates (`data_corpus_ungd2017`, included in the **quanteda.corpora** package).^[We will update the corpus before publishing the book and use the most recent UNGD speeches.]\n\n1. Identify the number of documents in `data_corpus_ungd2017`. \n2. Select only speeches delivered by representatives of African and Asian countries.\n2. Reshape this subsetted corpus to the level of sentences.\n3. Explore `textstat_summary()` of the **quanteda.textstats** package. Apply the function to `data_corpus_ungd2017` and assign it to an object called `tstat_sum_ungd`. \n4. What are the average, median, minimum, and maximum document lengths?\n5. Advanced: filter only speeches consisting of at least ?? tokens. \n6. Advanced: use `tstat_sum_ungd` and create a histogram of document length using the **ggplot2** package. \n7. Advanced: rerun the code for plotting the distribution, but add `facet_wrap()` and create small multiples for each continent.\n",
    "supporting": [
      "09-quanteda-corpus_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}