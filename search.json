[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Analysis Using R",
    "section": "",
    "text": "Welcome\nThis is the draft version of Text Analysis Using R. This book offers a comprehensive practical guide to text analysis and natural language processing using the R language. We have pitched the book at those already familiar with some R, but we also provide a gentle enough introduction that it is suitable for newcomers to R.\nYou’ll learn how to prepare your texts for analysis, how to analyse texts for insight using statistical methods and machine learning, and how to present those results using graphical methods. Each chapter covers a distinct topic, first presenting the methodology underlying each topic, and then providing practical examples using R. We also discuss advanced issues facing each method and its application. Finally, for those engaged in self-learning or wishing to use this book for instruction, we provide practical exercises for each chapter.\nThe book is organised into parts, starting with a review of R and especially the R packages and functions relevant to text analysis. If you are already comfortable with R you can skim or skip that section and proceed straight to part two.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Text Analysis Using R",
    "section": "",
    "text": "Note\n\n\n\nThis book is a work in progress. We will publish chapters as we write them, and open up the GitHub source repository for readers to make comments or even suggest corrections. You can view this at this https://github.com/quanteda/Text-Analysis-Using-R.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Text Analysis Using R",
    "section": "License",
    "text": "License\nWe will eventually seek a publisher for this book, but want to write it first. In the meantime we are retaining full copyright and licensing it only to be free to read.\n© Kenneth Benoit and Stefan Müller all rights reserved.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "00-preface.html",
    "href": "00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why another book on text mining in R?\nText mining tools for R have existed for many years, led by the venerable tm package (Feinerer, Hornik, and Meyer 2008). In 2015, the first version of quanteda (Benoit et al. 2018) was published on CRAN. Since then, the package has undergone three major versions, with each release improving its consistency, power, and usability. Starting with version 2, quanteda split into a series of packages designed around its different functions (such as plotting, statistics, or machine learning), with quanteda retaining the core text processing functions. Other popular alternatives exist, such as tidytext (Silge and Robinson 2016), although as we explain in Chapter 28  Integrating “tidy” approaches, quanteda works perfectly well with the R “tidyverse” and related approaches to text analysis.\nThis is hardly the only book covering how to work with text in R. Silge and Robinson (2017) introduced the tidytext package and numerous examples for mining text using the tidyverse approach to programming in R, including keyword analysis, mining word associations, and topic modelling. Kwartler (2017) provides a good coverage of text mining workflows, plus methods for text visualisations, sentiment scoring, clustering, and classification, as well as methods for sourcing and manipulating source documents. Earlier works include Turenne (2016) and Bécue-Bertaut (2019).\nSo why another book? First, the R ecosystem for text analysis is rapidly evolving, with the publication in recent years of massively improved, specialist text analysis packages such as quanteda. None of the earlier books covers this amazing family of packages. In addition, the field of text analysis methodologies has also advanced rapidly, including machine learning approaches based on artificial neural networks and deep learning. These and the packages that make use of them—for instance spacyr (Benoit and Matsuo 2020) for harnessing the power of the spaCy natural language processing library for Python (Honnibal et al. 2020)—have yet to be presented in a systematic, book-length treatment. Furthermore, as we are the authors and creators of many of the packages we cover in this book, we view this as the authoritative reference and how-to guide for using these packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#why-another-book-on-text-mining-in-r",
    "href": "00-preface.html#why-another-book-on-text-mining-in-r",
    "title": "Preface",
    "section": "",
    "text": "One of the two hard things in computer science: Naming things\n\n\n\nWhere does does the package name come from? quanteda is a portmanteau name indicating the purpose of the package, which is the quantitative analysis of textual data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#what-to-expect-from-this-book",
    "href": "00-preface.html#what-to-expect-from-this-book",
    "title": "Preface",
    "section": "What to Expect from This Book",
    "text": "What to Expect from This Book\nThis book is meant to be as a practical resource for those confronting the practical challenges of text analysis for the first time, focusing on how to do this in R. Our main focus is on the quanteda package and its extensions, although we also cover more general issues including a brief overview of the R functions required to get started quickly with practical text analysis We cover an introduction to the R language, for\nBenoit (2020) provides a detailed overview of the analysis of textual data, and what distinguishes textual data from other forms of data. It also clearly articulates what is meant by treating text “as data” for analysis. This book and the approaches it presents are firmly geared toward this mindset.\nEach chapter is structured so to provide a continuity across each topic. For each main subject explained in a chapter, we clearly explain the objective of the chapter, then describe the text analytic methods in an applied fashion so that readers are aware of the workings of the method. We then provide practical examples, with detailed working code in R as to how to implement the method. Next, we identify any special issues involved in correctly applying the method, including how to hand the more complicated situations that may arise in practice. Finally, we provide further reading for readers wishing to learn more, and exercises for those wishing for hands-on practice, or for assigning these when using them in teaching environment.\nWe have years of experience in teaching this material in many practical short courses, summer schools, and regular university courses. We have drawn extensively from this experience in designing the overall scope of this book and the structure of each chapter. Our goal is to make the book is suitable for self-learning or to form the basis for teaching and learning in a course on applied text analysis. Indeed, we have partly written this book to assign when teaching text analysis in our own curricula.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#who-this-book-is-for",
    "href": "00-preface.html#who-this-book-is-for",
    "title": "Preface",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nWe don’t assume any previous knowledge of text mining—indeed, the goal of this book is to provide that, from a foundation through to some very advanced topics. Getting use from this book does not require a pre-existing familiarity with R, although, as the slogan goes, “every little helps”. In Part I we cover some of the basics of R and how to make use of it for text analysis specifically. Readers will also learn more of R through our extensive examples. However, experience in teaching and presenting this material tells us that a foundation of R will enable readers to advance through the applications far more rapidly than if they were learning R from scratch at the same time that they take the plunge into the possibly strange new world of text analysis.\nWe are both academics, although we also have experience working in industry or in applying text analysis for non-academic purposes. The typical reader may be a student of text analysis in the literal sense (of being an student) or in the general sense of someone studying techniques in order to improve their practical and conceptual knowledge. Our orientation as social scientists, with a specialization in political text and political communications and media. But this book is for everyone: social scientists, computer scientists, scholars of digital humanities, researchers in marketing and management, and applied researchers working in policy, government, or business fields. This book is written to have the credibility and scholarly rigour (for referencing methods, for instance) needed by academic readers, but is designed to be written in a straightforward, jargon-free (as much as we were able!) manner to be of maximum practical use to non-academic analysts as well.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#how-the-book-is-structured",
    "href": "00-preface.html#how-the-book-is-structured",
    "title": "Preface",
    "section": "How the Book is Structured",
    "text": "How the Book is Structured\n\nSections\nThe book is divided into seven sections. These group topics that we feel represent common stages of learning in text analysis, or similar groups of topics that different users will be drawn too. By grouping stages of learning, we make it possible also for intermediate or advanced users to jump to the section that interests them most, or to the sections where they feel they need additional learning.\nOur sections are:\n\n(Working in R): This section is designed for beginners to learn quickly the R required for the techniques we cover in the book, and to guide them in learning a proper R workflow for text analysis.\nAcquiring texts: Often described (by us at least) as the hardest problem in text analysis, we cover how to source documents, including from Internet sources, and to import these into R as part of the quantitative text analysis pipeline.\nManaging textual data using quanteda: In this section, we introduce the quanteda package, and cover each stage of textual data processing, from creating structured corpora, to tokenisation, and building matrix representations of these tokens. We also talk about how to build and manage structured lexical resources such as dictionaries and stop word lists.\nExploring and describing texts: How to get overviews of texts using summary statistics, exploring texts using keywords-in-context, extracting target words, and identifying key words.\nStatistics for comparing texts: How to characterise documents in terms of their lexical diversity. readability, similarity, or distance.\nMachine learning for texts: How to apply scaling models, predictive models, and classification models to textual matrices.\nFurther methods for texts: Advanced methods including the use of natural language models to annotate texts, extract entities, or use word embeddings; integrating quanteda with “tidy” data approaches; and how to apply text analysis to “hard” languages such as Chinese (hard because of the high dimensional character set and the lack of whitespace to delimit words).\n\nFinally, in several appendices, we provide more detail about some tricky subjects, such as text encoding formats and working with regular expressions.\n\n\nChapter structure\nOur approach in each chapter is split into the following components, which we apply in every chapter:\n\nObjectives. We explain the purpose of each chapter and what we believe are the most important learning outcomes.\nMethods. We clearly explain the methodological elements of each chapter, through a combination of high-level explanations, formulas, and references.\nExamples. We use practical examples, with R code, demonstrating how to apply the methods to realise the objectives.\nIssues. We identify any special issues, potential problems, or additional approaches that a user might face when applying the methods to their text analysis problem.\nFurther Reading. In part because our scholarly backgrounds compel us to do so, and in part because we know that many readers will want to read more about each method, each chapter contains its own set of references and further readings.\nExercises. For those wishing additional practice or to use this text as a teaching resource (which we strongly encourage!), we provide exercises that can be assigned for each chapter.\n\nThroughout the book, we will demonstrate with examples and build models using a selection of text data sets. A description of these data sets can be found in Appendix Appendix A — Installing the Required Tools.\n\n\nConventions\nThroughout the book we use several kinds of info boxes to call your attention to information, cautions, and warnings.\n\n\n\n\n\n\nNote\n\n\n\nThe information icon signals a note or a reference to further information.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTips provide suggestions for better ways of doing things.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe exclamation mark icon signals an important point to consider.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning icons flag things you definitely want to avoid.\n\n\nAs you may already have noticed, we put names of R packages in boldface.\nCode blocks will be self-evident, and will look like this, with the output produced from executing those commands shown below the highlighted code in a mono-spaced font.\n\nlibrary(\"quanteda\")\n\nPackage version: 4.0.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\ndata_corpus_inaugural[1:3]\n\nCorpus consisting of 3 documents and 4 docvars.\n1789-Washington :\n\"Fellow-Citizens of the Senate and of the House of Representa...\"\n\n1793-Washington :\n\"Fellow citizens, I am again called upon by the voice of my c...\"\n\n1797-Adams :\n\"When it was first perceived, in early times, that no middle ...\"\n\n\nWe love the pipe operator in R (when used with discipline!) and built quanteda with a the aim making all of the main functions easily and logically pipeable. Since version 1, we have re-exported the %&gt;% operator from magrittr to make it available out of the box with quanteda. With the introduction of the |&gt; pipe in R 4.1.0, however, we prefer to use this variant, so will use that in all code used in this book.\n\n\nData used in examples\nAll examples and code are bundled as a companion R package to the book, available from our public GitHub repository.\n\n\n\n\n\n\nTip\n\n\n\nWe have written a companion package for this book called TAUR, which can be installed from GitHub using this command:\n\nremotes::install_github(\"quanteda/TAUR\")\n\n\n\nWe largely rely on data from three sources:\n\nthe built-in-objects from the quanteda package, such as the US Presidential Inaugural speech corpus;\n\nadded corpora from the book’s companion package, TAUR; and\nsome additional quanteda corpora or dictionaries from from the additional sources or packages where indicated.\n\nAlthough not commonly used, our scheme for naming data follows a very consistent scheme. The data objects being with data, have the object class as the second part of the name, such as corpus, and the third and final part of the data object name contains a description. The three elements are separated by the underscore (_) character. This means that any object is known by its name to be data, so that it shows up in the index of package objects (from the all-important help page, e.g. from help(package = \"quanteda\")) in one location, under “d”. It also means that its object class is known from the name, without further inspection. So data_dfm_lbgexample is a dfm, while data_corpus_inaugural is clearly a corpus. We use this scheme and others like with an almost religious fervour, because we think that learning the functionality of a programming framework for NLP and quantitative text analysis is complicated enough without having also to decipher or remember a mishmash of haphazard and inconsistently named functions and objects. The more you use our software packages and specifically quanteda, the more you will come to appreciate the attention we have paid to implementing a consistent naming scheme for objects, functions, and their arguments as well as to their consistent functionality.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#colophon",
    "href": "00-preface.html#colophon",
    "title": "Preface",
    "section": "Colophon",
    "text": "Colophon\nThis book was written in RStudio using Quarto. The website is hosted via GitHub Pages, and the complete source is available on GitHub.\nThis version of the book was built with R version 4.3.2 (2023-10-31) and the following packages:\n\n\n\n\n\n\n\nPackage\nVersion\nSource\n\n\n\n\nquanteda\n4.0.0\nlocal\n\n\nquanteda.textmodels\n0.9.6\nCRAN (R 4.3.0)\n\n\nquanteda.textplots\n0.94.3\nCRAN (R 4.3.0)\n\n\nquanteda.textstats\n0.96.5\nlocal\n\n\nreadtext\n0.90\nCRAN (R 4.3.0)\n\n\nstopwords\n2.3\nCRAN (R 4.3.0)\n\n\ntidyverse\n2.0.0\nCRAN (R 4.3.0)\n\n\n\n\n\n\n\nHow to Contact Us\nPlease address comments and questions concerning this book by filing an issue on our GitHub page, https://github.com/quanteda/Text-Analysis-Using-R/issues/. At this repository, you will also find instructions for installing the companion R package, TAUR.\nFor more information about the authors or the Quanteda Initiative, visit our website.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-preface.html#acknowledgements",
    "href": "00-preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nDeveloping quanteda has been a labour of many years involving many contributors. The most notable contributor to the package and its learning materials is Kohei Watanabe, without whom quanteda would not be the incredible package it is today. Kohei has provided a clear and vigorous vision for the package’s evolution across its major versions and continues to maintain it today. Others have contributed in major ways both through design and programming, namely Paul Nulty and Akitaka Matsuo, both who were present at creation and through the 1.0 launch which involved the first of many major redesigns. Adam Obeng made a brief but indelible imprint on several of the packages in the quanteda family. Haiyan Wang wrote versions of some of the core C++ code that makes quanteda so fast, as well as contributing to the R base. William Lowe and Christian Müller also contributed code and ideas that have enriched the package and the methods it implements.\nNo project this large could have existed without institutional and financial benefactors and supporters. Most notable of these is the European Research Council, who funded the original development under a its Starting Investigator Grant scheme, awarded to Kenneth Benoit in 2011 for a project entitled QUANTESS: Quantitative Text Analysis for the Social Sciences (ERC-2011-StG 283794-QUANTESS). Our day jobs (as university professors) have also provided invaluable material and intellectual support for the development of the ideas, methodologies, and software documented in this book. This list includes the London School of Economics and Political Science, which employs Ken Benoit and which hosted the QUANTESS grant and employed many of the core quanteda team and/or where they completed PhDs; University College Dublin where Stefan Müller is employed; Trinity College Dublin, where Ken Benoit was formerly a Professor of Quantitative Social Sciences and where Stefan completed his PhD in political science; and the Australian National University which provided a generous affiliation to Ken and where the outline for this book took first shape as a giant tableau of post-it notes on the wall of a visiting office in the former building of the School of Politics and International Relations.\nAs we develop this book, we hope that many readers of the work-in-progress will contribute feedback, comments, and even edits, and we plan to acknowledge you all. So, don’t be shy readers. You can suggest changes by clicking on “Edit this page” in the top-right corner, forking the GitHub repository, and making a pull request. More details on contributing and copyright are provided in a separate page on Contributing to Text Analysis Using R.\nWe also have to give a much-deserved shout-out to the amazing team at RStudio, many of whom we’ve been privileged to hear speak at events or meet in person. You made Quarto available at just the right time. That and the innovations in R tools and software that you have driven for the past decade have been rich beyond every expectation.\nFinally, no acknowledgements would be complete without a profound thanks to our partners, who have put up with us during the long incubation of this project. They have had to listen us talk about this book for years before we finally got around to writing it. They’ve tolerated us cursing software bugs, students, CRAN, each other, package users, and ourselves. But they’ve also seen they joy that we’ve experienced from creating tools and materials that empower users and students, and the excitement of the long intellectual voyage we have taken together and with our ever-growing base of users and students. Bina and Émeline, thank you for all of your support and encouragement. You’ll be so happy to know we finally have this book project well underway.\n\n\n\n\nBécue-Bertaut, Monica. 2019. Textual Data Science with R. CRC Press.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 461–97. Thousand Oaks: Sage.\n\n\nBenoit, Kenneth, and Akitaka Matsuo. 2020. Spacyr: Wrapper to the ’spaCy’ ’NLP’ Library. https://CRAN.R-project.org/package=spacyr.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An R Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nFeinerer, Ingo, Kurt Hornik, and David Meyer. 2008. “Text Mining Infrastructure in R.” Journal of Statistical Software 25 (5): 1–54. https://www.jstatsoft.org/v25/i05/.\n\n\nHonnibal, Matthew, Ines Montani, Sophie Van Landeghem, and Adriane Boyd. 2020. “spaCy: Industrial-Strength Natural Language Processing in Python.” https://doi.org/10.5281/zenodo.1212303.\n\n\nKwartler, Ted. 2017. Text Mining in Practice with R. Chichester: John Wiley & Sons.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in r.” Journal of Open Source Software 1 (3). https://doi.org/10.21105/joss.00037.\n\n\n———. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media, Inc.\n\n\nTurenne, Nicolas. 2016. Analyse de Données Textuelles Sous r. ISTE Group.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-authors.html",
    "href": "00-authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Kenneth Benoit is Professor of Computational Social Science in the Department of Methodology at the London School of Economics and Political Science. He is also (2020–present) Director of the LSE Data Science Institute.He has previously held positions in the Department of Political Science at Trinity College Dublin and at the Central European University (Budapest). He received his Ph.D. (1998) from Harvard University, Department of Government. His current research focuses on computational, quantitative methods for processing large amounts of textual data, mainly political texts and social media.\nStefan Müller is an Assistant Professor and Ad Astra Fellow in the School of Politics and International Relations at University College Dublin. Previously, he was a Senior Researcher at the University of Zurich. He received his Ph.D. (2019) from Trinity College Dublin, Department of Political Science. His current research focuses on political representation, party competition, political communication, public opinion, and quantitative text analysis. His work has been published in journals such as the American Political Science Review, The Journal of Politics, Political Communication, the European Journal of Political Research, and Political Science Research and Methods, among others.\nThe Quanteda Initiative is a non-profit company we founded in 2018 in London, devoted to the promotion of open-source text analysis software. It supports active development of these tools, in addition to providing training materials, training events, and sponsoring workshops and conferences. Its main product is the open-source quanteda package for R, but the Quanteda Initiative also supports a family of independent but interrelated packages for providing additional functionality for natural language processing and document management.\nIts core objectives, as stated in its charter, are to:\n\nSupport open-source, text analysis software developed for research and scientific analysis. These efforts focus mainly on the open-source software library quanteda, written for the R programming language, and its related family of extension packages.\nPromote interoperability between text analytic software libraries, including those written in other languages such as Python, Java, or C++.\nProvide ongoing user, technical, and development support for open-source text analysis software. Organize training and dissemination activities related to open-source text analysis software.",
    "crumbs": [
      "About the Authors"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction and Motivation",
    "section": "",
    "text": "1.1 Objectives\nTo dive right in to text analysis using R, by means of an extended demonstration of how to acquire, process, quantify, and analyze a series of texts. The objective is to demonstrate the techniques covered in the book and the tools required to apply them.\nThis overview is intended to serve more as a demonstration of the kinds of natural language processing and text analysis that can be done powerfully and easily in R, rather than as primary instructional material. Starting in Chapter 2, you learn about the R fundamentals and work our way gradually up to basic and then more advanced text analysis. Until then, enjoy the demonstration, and don’t be discouraged if it seems too advanced to follow at this stage. By the time you have finished this book, this sort of analysis will be very familiar.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Motivation</span>"
    ]
  },
  {
    "objectID": "01-intro.html#application-analyzing-candidate-debates-from-the-us-presidential-election-campaign-of-2020",
    "href": "01-intro.html#application-analyzing-candidate-debates-from-the-us-presidential-election-campaign-of-2020",
    "title": "1  Introduction and Motivation",
    "section": "1.2 Application: Analyzing Candidate Debates from the US Presidential Election Campaign of 2020",
    "text": "1.2 Application: Analyzing Candidate Debates from the US Presidential Election Campaign of 2020\nThe methods demonstrated in this chapter include:\n\nacquiring texts directly from the world-wide web;\ncreating a corpus from the texts, with associated document-level variables;\nsegmenting the texts by sections found in each document;\ncleaning up the texts further;\ntokenizing the texts into words;\nsummarizing the texts in terms of who spoke and how much;\nexamining keywords-in-context from the texts, and identifying keywords using statistical association measures;\ntransforming and selecting features using lower-casing, stemming, and removing punctuation and numbers;\nremoving selected features in the form of “stop words”;\ncreating a document-feature matrix from the tokens;\nperforming sentiment analysis on the texts;\nfitting topic models on the texts.\n\nFor demonstration, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.1\n\n1.2.1 Acquiring text directly from the world wide web.\nFull transcripts of both televized debates are available from The American Presidency Project. We start our demonstration by scraping the debates using the rvest package (Wickham 2021). You will learn more about scraping data from the internet in Chapter 7. We first need to install and load the packages required for this demonstration.\n\n# install packages\ninstall.packages(\"quanteda\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\ninstall.packages(\"rvest\")\ninstall.packages(\"stringr\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"quanteda/quanteda.tidy\")\n\n\n# load packages\nlibrary(\"quanteda\")\nlibrary(\"rvest\")\nlibrary(\"stringr\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.tidy\")\n\nNext, we identify the presidential debates for the 2020 period, assign the URL of the debates to an object, and load the source page. We retrieve metadata (the location and dates of the debates), store the information as a data frame, and get the URL debates. Finally, we scrape the search results and save the texts as a character vector, with one element per debate.\n\n# search presidential debates for the 2020 period\n# https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50\n\n# assign URL of debates to an object called url_debates\nurl_debates &lt;- \"https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50\"\n\nsource_page &lt;- read_html(url_debates)\n\n# get debate meta-data\n\nnodes_pres &lt;- \".views-field-title a\"\ntext_pres &lt;- \".views-field-field-docs-start-date-time-value.text-nowrap\"\n\ndebates_meta &lt;- data.frame(\n    location = html_text(html_nodes(source_page, nodes_pres)),\n    date =  html_text(html_nodes(source_page, text_pres)),\n    stringsAsFactors = FALSE\n)\n\n# format the date\ndebates_meta$date &lt;- as.Date(trimws(debates_meta$date), \n                             format = \"%b %d, %Y\")\n\n# get debate URLs\ndebates_links &lt;- source_page |&gt; \n    html_nodes(\".views-field-title a\") |&gt; \n    html_attr(name = \"href\") \n\n# add first part of URL to debate links\ndebates_links &lt;- paste0(\"https://www.presidency.ucsb.edu\", debates_links)\n\n# scrape search results\ndebates_scraped &lt;- lapply(debates_links, read_html)\n\n# get character vector, one element per debate\ndebates_text &lt;- sapply(debates_scraped, function(x) {\n    html_nodes(x, \"p\") |&gt; \n        html_text() |&gt;\n        paste(collapse = \"\\n\\n\")\n})\n\nHaving retrieved the text of the two debates, we clean up the character vector. More specifically, we clean up the details on the location using regular expressions and the stringr package (see Chapter C).\n\ndebates_meta$location &lt;- str_replace(debates_meta$location, \"^.* in \", \"\")\n\n\n\n1.2.2 Creating a text corpus\nNow we have two objects. The character vector debates_text contains the text of both debates, and the data frame debates_meta stores the date and location of both debates. These objects allow us to create a quanteda corpus, with the metadata. We use the corpus() function, specify the location of the document-level variables, and assign the corpus to an object called data_corpus_debates.\n\ndata_corpus_debates &lt;- corpus(debates_text, \n                              docvars = debates_meta)\n\n# check the number of documents included in the text corpus\nndoc(data_corpus_debates)\n\n[1] 2\n\n\nThe object data_corpus_debates contains two documents, one for each debate. While this unit of analysis may be suitable for some analyses, we want to identify all utterances by the moderator and the two candidates. With the function corpus_segment(), we can segment the corpus into statements. The unit of analysis changes from a full debate to a statement during a debate. Inspecting the page for one of the debates2 reveals that a new utterance starts with the speaker’s name in ALL CAPS, followed by a colon. We use this consistent pattern for segmenting the corpus to the level of utterances. The regular expression \"\\\\s*[[:upper:]]+:\\\\s+\" identifies speaker names in ALL CAPS (\\\\s*[[:upper:]]+), followed by a colon +: and a white space \\\\s+.\n\ndata_corpus_debatesseg &lt;- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\nsummary(data_corpus_debatesseg, n = 4)\n\nCorpus consisting of 1207 documents, showing 4 documents:\n\n    Text Types Tokens Sentences        location       date       pattern\n text1.1   139    251        13 Cleveland, Ohio 2020-09-29 \\n\\nWALLACE: \n text1.2     6      6         1 Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n text1.3     5      5         1 Cleveland, Ohio 2020-09-29   \\n\\nTRUMP: \n text1.4     3      3         1 Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n\n\nThe segmentation results in a new corpus consisting of 1,207 utterances. The summary() function provides a useful overview of each utterance. The first text, for example, contains 251 tokens, 139 types (i.e., unique tokens) and 13 sentences.\nHaving segmented the corpus, we improve the document-level variables since such meta information on each document is crucial for subsequent analyses like subsetting the corpus or grouping the corpus to the level of speakers. We create a new document-level variable called “speaker” based on the pattern extracted above.\n\n# str_trim() removes empty whitespaces, \n# str_remove_all() removes the colons,\n# and str_to_title() changes speaker names \n# from UPPER CASE to Title Case\n\ndata_corpus_debatesseg &lt;- data_corpus_debatesseg |&gt; \n    rename(speaker = pattern) |&gt; \n    mutate(speaker = str_trim(speaker),\n           speaker = str_remove_all(speaker, \":\"),\n           speaker = str_to_title(speaker)) \n\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$speaker,\n      data_corpus_debatesseg$location)\n\n         \n          Cleveland, Ohio Nashville, Tennessee\n  Biden               269                   84\n  Trump               340                  122\n  Wallace             246                    0\n  Welker                0                  146\n\n\nThe cross-table reports the number of statement by each speaker in each debate. The first debate in Cleveland seems to be longer: the number of Trump’s and Biden’s statements during the Cleveland debates are three times higher than those in Tennessee. The transcript reports 246 utterances by Chris Wallace during the first and 146 by Kristen Welker during the second debate. We can inspect all cleaned document-level variables in this text corpus.\n\ndata_corpus_debatesseg |&gt; \n    docvars() |&gt; \n    glimpse()\n\nRows: 1,207\nColumns: 3\n$ location &lt;chr&gt; \"Cleveland, Ohio\", \"Cleveland, Ohio\", \"Cleveland, Ohio\", \"Cle…\n$ date     &lt;date&gt; 2020-09-29, 2020-09-29, 2020-09-29, 2020-09-29, 2020-09-29, …\n$ speaker  &lt;chr&gt; \"Wallace\", \"Biden\", \"Trump\", \"Biden\", \"Wallace\", \"Trump\", \"Wa…\n\n\n\n\n1.2.3 Tokenizing a corpus\nNext, we tokenize our text corpus. Typically, tokenization involves separating texts by white spaces. We tokenize the text corpus without any pre-processing using tokens().\n\ntoks_usdebates2020 &lt;- tokens(data_corpus_debatesseg)\n\n# let's inspect the first six tokens of the first four documents\nprint(toks_usdebates2020, max_ndoc = 4, max_ntoken = 6)\n\nTokens consisting of 1,207 documents and 3 docvars.\ntext1.1 :\n[1] \"Good\"      \"evening\"   \"from\"      \"the\"       \"Health\"    \"Education\"\n[ ... and 245 more ]\n\ntext1.2 :\n[1] \"How\"   \"you\"   \"doing\" \",\"     \"man\"   \"?\"    \n\ntext1.3 :\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"?\"    \n\ntext1.4 :\n[1] \"I'm\"  \"well\" \".\"   \n\n[ reached max_ndoc ... 1,203 more documents ]\n\ntokens(data_corpus_debatesseg)\n\nTokens consisting of 1,207 documents and 3 docvars.\ntext1.1 :\n [1] \"Good\"       \"evening\"    \"from\"       \"the\"        \"Health\"    \n [6] \"Education\"  \"Campus\"     \"of\"         \"Case\"       \"Western\"   \n[11] \"Reserve\"    \"University\"\n[ ... and 239 more ]\n\ntext1.2 :\n[1] \"How\"   \"you\"   \"doing\" \",\"     \"man\"   \"?\"    \n\ntext1.3 :\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"?\"    \n\ntext1.4 :\n[1] \"I'm\"  \"well\" \".\"   \n\ntext1.5 :\n [1] \"Gentlemen\" \",\"         \"a\"         \"lot\"       \"of\"        \"people\"   \n [7] \"been\"      \"waiting\"   \"for\"       \"this\"      \"night\"     \",\"        \n[ ... and 137 more ]\n\ntext1.6 :\n [1] \"Thank\" \"you\"   \"very\"  \"much\"  \",\"     \"Chris\" \".\"     \"I\"     \"will\" \n[10] \"tell\"  \"you\"   \"very\" \n[ ... and 282 more ]\n\n[ reached max_ndoc ... 1,201 more documents ]\n\n# check number of tokens and types\ntoks_usdebates2020 |&gt; \n    ntoken() |&gt; \n    sum()\n\n[1] 43614\n\ntoks_usdebates2020 |&gt; \n    ntype() |&gt; \n    sum()\n\n[1] 28137\n\n\nWithout any pre-processing, the corpus consists of 43,742 tokens and 28,174 types. We can easily check how these numbers change when transforming all tokens to lowercase and removing punctuation characters.\n\ntoks_usdebates2020_reduced &lt;- toks_usdebates2020 |&gt; \n    tokens(remove_punct = TRUE) |&gt; \n    tokens_tolower()\n\n# check number of tokens and types\ntoks_usdebates2020_reduced |&gt; \n    ntoken() |&gt; \n    sum()\n\n[1] 36801\n\ntoks_usdebates2020_reduced |&gt; \n    ntype() |&gt; \n    sum()\n\n[1] 25049\n\n\nThe number of tokens and types decreases to 36,740 and 28,174, respectively, after removing punctuation and harmonizing all terms to lowercase.\n\n\n1.2.4 Keywords-in-context\nIn contrast to a document-feature matrix (covered below), tokens objects still preserve the order of words. We can use tokens objects to identify the occurrence of keywords and their immediate context.\n\nkw_america &lt;- kwic(toks_usdebates2020, \n                   pattern = c(\"america\"),\n                   window = 2)\n\n# number of mentions\nnrow(kw_america)\n\n[1] 18\n\n# print first 6 mentions of America and the context of ±2 words\nhead(kw_america, n = 6)\n\nKeyword-in-context with 6 matches.                                                     \n [text1.284, 66]        towns in | America | , how   \n [text1.290, 80]       states in | America | with a  \n [text1.290, 96]       States of | America | , and   \n  [text1.335, 5] worst president | America | has ever\n [text1.489, 37]        whole of | America | . But   \n [text1.500, 53]      applied in | America | .\"      \n\n\n\n\n1.2.5 Text processing\nThe keywords-in-context analysis above reveals that all terms are still in upper case and that very frequent, uninformative words (so-called stopwords) and punctuation are still part of the text. In most applications, we remove very frequent features and transform all words to lowercase. The code below shows how to adjust the object accordingly.\n\ntoks_usdebates2020_processed &lt;- data_corpus_debatesseg |&gt; \n    tokens(remove_punct = TRUE) |&gt; \n    tokens_remove(pattern = stopwords(\"en\")) |&gt; \n    tokens_tolower()\n\nLet’s inspect if the changes have been implemented as we expect by calling kwic() on the new tokens object.\n\nkw_america_processed &lt;- kwic(toks_usdebates2020_processed, \n                             pattern = c(\"america\"),\n                             window = 2)\n\n# print first 6 mentions of America and the context of ±2 words\nhead(kw_america_processed, n = 6)\n\nKeyword-in-context with 6 matches.                                                                 \n [text1.284, 32]     class towns | america | well guy            \n [text1.290, 29]     half states | america | significant increase\n [text1.290, 38]   united states | america | wants open          \n  [text1.335, 3] worst president | america | ever come           \n [text1.489, 15]  equality whole | america | never accomplished  \n [text1.500, 25] equally applied | america | believe separate    \n\n# test: print as table+\nlibrary(kableExtra)\nkw_america_processed |&gt; data.frame() |&gt; \n  dplyr::select(Pre = pre, Keyword = keyword, Post = post, Pattern = pattern) |&gt;  \n  kbl(booktabs = T) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"), html_font = \"Source Sans Pro\", full_width = F)\n\n\n\n\nPre\nKeyword\nPost\nPattern\n\n\n\n\nclass towns\namerica\nwell guy\namerica\n\n\nhalf states\namerica\nsignificant increase\namerica\n\n\nunited states\namerica\nwants open\namerica\n\n\nworst president\namerica\never come\namerica\n\n\nequality whole\namerica\nnever accomplished\namerica\n\n\nequally applied\namerica\nbelieve separate\namerica\n\n\ndefeat racism\namerica\n\namerica\n\n\nincrease homicides\namerica\nsummer particularly\namerica\n\n\nless violence\namerica\ntoday president\namerica\n\n\nfired plant\namerica\none's going\namerica\n\n\nfire plant\namerica\ngoing move\namerica\n\n\nunited states\namerica\nsituation thousands\namerica\n\n\nevery company\namerica\nblow away\namerica\n\n\nunited states\namerica\n\namerica\n\n\nunited states\namerica\nanybody seeking\namerica\n\n\nsection race\namerica\nwant talk\namerica\n\n\ninstitutional racism\namerica\nalways said\namerica\n\n\ngrowing industry\namerica\nare--is electric--excuse\namerica\n\n\n\n\n\n\n\nThe processing of the tokens object worked as expected. Let’s imagine we want to group the documents by debate and speaker, resulting in two documents for Trump, two for Biden, and one for each moderator. tokens_group() allows us to change the unit of analysis. After aggregating the documents, we can use the function textplot_xray() to observe the occurrences of specific keywords during the debates.\n\n# new document-level variable with date and speaker\ntoks_usdebates2020$speaker_date &lt;- paste(\n    toks_usdebates2020$speaker,\n    toks_usdebates2020$date,\n    sep = \", \")\n\n# reshape the tokens object to speaker-date level \n# and keep only Trump and Biden\ntoks_usdebates2020_grouped &lt;- toks_usdebates2020 |&gt; \n    tokens_subset(speaker %in% c(\"Trump\", \"Biden\")) |&gt; \n    tokens_group(groups = speaker_date)\n\n# check number of documents\nndoc(toks_usdebates2020_grouped)\n\n[1] 4\n\n# use absolute position of the token in the document\ntextplot_xray(\n    kwic(toks_usdebates2020_grouped, pattern = \"america\"), \n    kwic(toks_usdebates2020_grouped, pattern = \"tax*\")\n)\n\n\n\n\n\n\n\n\nThe grouped document also allows us to check how often each candidate spoke during the debate.\n\nntoken(toks_usdebates2020_grouped)\n\nBiden, 2020-09-29 Biden, 2020-10-22 Trump, 2020-09-29 Trump, 2020-10-22 \n             7977              8060              8956              8981 \n\n\nThe number of tokens between Trump and Biden do not differ substantively in both debates. Trump’s share of speech is only slightly higher than Biden’s (7996 v 8093 tokens; 8973 v 9016 tokens).\n\n\n1.2.6 Identifying multiword expressions\nMany languages build on multiword expressions. For instance, “income” and “tax” as separate unigrams have a different meaning than the bigram “income tax”. The package quanteda.textstats includes the function textstat_collocation() that automatically retrieves common multiword expressions.\n\ntstat_coll &lt;- data_corpus_debatesseg |&gt; \n    tokens(remove_punct = TRUE) |&gt; \n    tokens_remove(pattern = stopwords(\"en\"), padding = TRUE) |&gt; \n    textstat_collocations(size = 2:3, min_count = 5)\n\n# for illustration purposes select the first 20 collocations\nhead(tstat_coll, 20)\n\n       collocation count count_nested length   lambda        z\n1  president trump    69           42      2 6.898135 21.87073\n2        make sure    30            4      2 7.463828 21.53833\n3  president biden    51           51      2 6.434178 20.53644\n4     mr president    33           20      2 5.358261 19.55120\n5      health care    20           15      2 7.578059 19.31951\n6       number one    18           16      2 5.593899 17.93762\n7        right now    18           13      2 4.814084 16.84894\n8       number two    15           11      2 5.470417 16.68685\n9     half million    15           13      2 6.850521 16.55165\n10         mr vice    15           15      2 5.345112 16.54794\n11 american people    22            4      2 4.971608 16.45284\n12     two minutes    28           19      2 8.502087 16.08709\n13      four years    19           15      2 7.592875 16.04832\n14       come back    12            7      2 5.535399 15.45982\n15      one number    12           12      2 5.120179 14.65388\n16  climate change    11            8      2 8.961748 14.65273\n17     three years    11            5      2 5.346560 14.55198\n18  million people    18           18      2 4.075772 14.42822\n19  vice president    99           81      2 9.074332 14.06393\n20 million dollars    10           10      2 5.285594 13.99695\n\n\nWe can use tokens_compound() to compound certain multiword expressions before creating a document-feature matrix which does not consider word order. For illustration purposes, we compound climate change ,social securit*, and health insurance*. By default, compounded tokens are concatenated by _.\n\ntoks_usdebates2020_comp &lt;- toks_usdebates2020 |&gt; \n    tokens(remove_punct = TRUE) |&gt; \n    tokens_compound(pattern = phrase(c(\"climate change\",\n                                       \"social securit*\",\n                                       \"health insuranc*\"))) |&gt; \n    tokens_remove(pattern = stopwords(\"en\"))\n\n\n\n1.2.7 Document-feature matrix\nWe have come a long way already. We downloaded debate transcripts, segmented the texts to utterances, added document-level variables, tokenized the corpus, inspected keywords, and compounded multiword expressions. Next, we transform our tokens object into a document-feature matrix (dfm). A dfm counts the occurrences of tokens in each document. We can create a document feature matrix, print the structure, and get the most frequent words.\n\ndfmat_presdebates20 &lt;- dfm(toks_usdebates2020_comp)\n\n# most frequent features\ntopfeatures(dfmat_presdebates20, n = 10)\n\npresident     going    people      said      know      want       get       say \n      287       278       258       162       139       128       118       114 \n     look      vice \n      105       101 \n\n# most frequent features by speaker\n\ntopfeatures(dfmat_presdebates20, groups = speaker, n = 10)\n\n$Biden\n    going    people      fact      said       get      make      know president \n      126       118        72        62        54        46        46        45 \n     sure       can \n       44        43 \n\n$Trump\npeople  going   know   said   look   want    joe   done    say   just \n   111     92     85     75     65     61     55     55     54     52 \n\n$Wallace\npresident       sir     going  question        mr      vice     trump        go \n      110        56        44        39        38        36        33        30 \n      two     right \n       28        25 \n\n$Welker\npresident      vice     trump     biden       let     right  question      move \n       99        52        37        34        28        27        26        24 \n     talk        -- \n       21        19 \n\n\nMany methods build on document-feature matrices and the “bag-of-words” approach. In this section, we introduce textstat_keyness(), which identifies features that occur differentially across different categories – in our case, Trump’s and Biden’s utterances. The function textplot_keyness() provides a straightforward way of visualize the results of the keyness analysis (Figure (fig:keynesspres).)\n\ntstat_key &lt;- dfmat_presdebates20 |&gt;\n    dfm_subset(speaker %in% c(\"Trump\", \"Biden\")) |&gt; \n    dfm_group(groups = speaker) |&gt; \n    textstat_keyness(target = \"Trump\")\n\ntextplot_keyness(tstat_key)\n\n\n\n\n\n\n\n\n\n\n1.2.8 Bringing it all together: Sentiment analysis\nBefore introducing the R Basics in the next chapter, we show how to conduct a dictionary-based sentiment analysis using the Lexicoder Sentiment Dictionary (Young and Soroka 2012). The dictionary, included in quanteda as data_dictionary_LSD2015 contains 1709 positive and 2858 negative terms (as well as their negations). We discuss advanced sentiment analyses with measures of uncertainty in Chapter ??.\n\ntoks_sent &lt;- toks_usdebates2020 |&gt; \n    tokens_group(groups = speaker) |&gt; \n    tokens_lookup(dictionary = data_dictionary_LSD2015,\n                  nested_scope = \"dictionary\")\n\n# create a dfm with the count of matches,\n# transform object into a data frame,\n# and add document-level variables\ndat_sent &lt;- toks_sent |&gt; \n    dfm() |&gt; \n    convert(to = \"data.frame\") |&gt; \n    cbind(docvars(toks_sent))\n\n# select Trump and Biden and aggregate sentiment\ndat_sent$sentiment = with(\n    dat_sent, \n    log((positive + neg_negative + 0.5) /  \n            (negative + neg_positive + 0.5)))\n\ndat_sent\n\n   doc_id negative positive neg_positive neg_negative speaker   sentiment\n1   Biden      357      465           33            8   Biden  0.19272394\n2   Trump      430      509            7            2   Trump  0.15627088\n3 Wallace      136      185            0            4 Wallace  0.32806441\n4  Welker      106      100            0            0  Welker -0.05798726",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Motivation</span>"
    ]
  },
  {
    "objectID": "01-intro.html#issues",
    "href": "01-intro.html#issues",
    "title": "1  Introduction and Motivation",
    "section": "1.3 Issues",
    "text": "1.3 Issues\nSome issues raised from the example, where we might have done things differently or added additional analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Motivation</span>"
    ]
  },
  {
    "objectID": "01-intro.html#further-reading",
    "href": "01-intro.html#further-reading",
    "title": "1  Introduction and Motivation",
    "section": "1.4 Further Reading",
    "text": "1.4 Further Reading\nSome studies of debate transcripts? Or issues involved in analyzing interactive or “dialogical” documents.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Motivation</span>"
    ]
  },
  {
    "objectID": "01-intro.html#exercises",
    "href": "01-intro.html#exercises",
    "title": "1  Introduction and Motivation",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nAdd some here.\n\n\n\n\nWickham, Hadley. 2021. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\nYoung, Lori, and Stuart N. Soroka. 2012. “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication 29 (2): 205–31.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Motivation</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Introduction and Motivation",
    "section": "",
    "text": "The transcripts are available at https://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio and https://www.presidency.ucsb.edu/documents/presidential-debate-belmont-university-nashville-tennessee-0.↩︎\nhttps://www.presidency.ucsb.edu/documents/presidential-debate-case-western-reserve-university-cleveland-ohio↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Motivation</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html",
    "href": "02-r-basics.html",
    "title": "2  R Basics",
    "section": "",
    "text": "2.1 Objectives\nTo provide a targeted introduction to R, for those needing an introduction or a review.\nWe will cover:\nThe objective is to introduce how these core object types behave, and use them to store basic quantities required in text analysis.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html#objectives",
    "href": "02-r-basics.html#objectives",
    "title": "2  R Basics",
    "section": "",
    "text": "core object types (atomic, vectors, lists, complex)\ndetailed survey of characters and “strings”\ndetailed coverage of list types\ndetailed coverage of matrix types\nintroduction to the data.frame\noutput data in the form of a data.frame\noutput data in the form of a plot (ggplot2 object)",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html#methods",
    "href": "02-r-basics.html#methods",
    "title": "2  R Basics",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nThe methods are primarily about how to use R, including:\nWe will build up the basic objects needed for understand the core structures needed in R to hold texts (character), meta-data (data.frame), tokens (lists), dictionaries (lists), and document-feature matrices (matrices).",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html#examples",
    "href": "02-r-basics.html#examples",
    "title": "2  R Basics",
    "section": "2.3 Examples",
    "text": "2.3 Examples\nExamples working through the construction of each text object container using the types above.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html#issues",
    "href": "02-r-basics.html#issues",
    "title": "2  R Basics",
    "section": "2.4 Issues",
    "text": "2.4 Issues\nCould have used “tidy” approaches.\nSparsity.\nIndexing.\nWouldn’t it be nice if we could nest some objects inside others?",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html#further-reading",
    "href": "02-r-basics.html#further-reading",
    "title": "2  R Basics",
    "section": "2.5 Further Reading",
    "text": "2.5 Further Reading\nSome resources on R.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02-r-basics.html#exercises",
    "href": "02-r-basics.html#exercises",
    "title": "2  R Basics",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\nAdd some here.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "03-r-workflow.html",
    "href": "03-r-workflow.html",
    "title": "3  R Workflow",
    "section": "",
    "text": "3.1 Objectives\nHow R works: - passing by value, compared to other languages - using “pipes” - object orientation: object classes and methods (functions)\nWorkflow issues: - clear code - reproducible workflow - R markdown and R notebooks\nUsing packages",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Workflow</span>"
    ]
  },
  {
    "objectID": "03-r-workflow.html#methods",
    "href": "03-r-workflow.html#methods",
    "title": "3  R Workflow",
    "section": "3.2 Methods",
    "text": "3.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Workflow</span>"
    ]
  },
  {
    "objectID": "03-r-workflow.html#examples",
    "href": "03-r-workflow.html#examples",
    "title": "3  R Workflow",
    "section": "3.3 Examples",
    "text": "3.3 Examples\nObjects that work on character and lists.\nThings that work for data.frames. Non-standard evaluation for named columns in a data.frame.\nggplot2",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Workflow</span>"
    ]
  },
  {
    "objectID": "03-r-workflow.html#issues",
    "href": "03-r-workflow.html#issues",
    "title": "3  R Workflow",
    "section": "3.4 Issues",
    "text": "3.4 Issues\nAdditional issues.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Workflow</span>"
    ]
  },
  {
    "objectID": "03-r-workflow.html#further-reading",
    "href": "03-r-workflow.html#further-reading",
    "title": "3  R Workflow",
    "section": "3.5 Further Reading",
    "text": "3.5 Further Reading\nAdvanced reading on R. Packages.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Workflow</span>"
    ]
  },
  {
    "objectID": "03-r-workflow.html#exercises",
    "href": "03-r-workflow.html#exercises",
    "title": "3  R Workflow",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\nAdd some here.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Workflow</span>"
    ]
  },
  {
    "objectID": "04-r-strings.html",
    "href": "04-r-strings.html",
    "title": "4  Working with Text in R",
    "section": "",
    "text": "4.1 Objectives\nA deeper dive into how text is represented in R. - character vectors vesus “strings” - factors - base package string handling - the stringi and stringr packages - regular expressions - encoding, briefly.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Text in R</span>"
    ]
  },
  {
    "objectID": "04-r-strings.html#methods",
    "href": "04-r-strings.html#methods",
    "title": "4  Working with Text in R",
    "section": "4.2 Methods",
    "text": "4.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Text in R</span>"
    ]
  },
  {
    "objectID": "04-r-strings.html#examples",
    "href": "04-r-strings.html#examples",
    "title": "4  Working with Text in R",
    "section": "4.3 Examples",
    "text": "4.3 Examples\nLots of the above.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Text in R</span>"
    ]
  },
  {
    "objectID": "04-r-strings.html#issues",
    "href": "04-r-strings.html#issues",
    "title": "4  Working with Text in R",
    "section": "4.4 Issues",
    "text": "4.4 Issues\nAdditional issues.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Text in R</span>"
    ]
  },
  {
    "objectID": "04-r-strings.html#further-reading",
    "href": "04-r-strings.html#further-reading",
    "title": "4  Working with Text in R",
    "section": "4.5 Further Reading",
    "text": "4.5 Further Reading\nString handling in R. More regular expressions. More on encoding.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Text in R</span>"
    ]
  },
  {
    "objectID": "04-r-strings.html#exercises",
    "href": "04-r-strings.html#exercises",
    "title": "4  Working with Text in R",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\nAdd some here.",
    "crumbs": [
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working with Text in R</span>"
    ]
  },
  {
    "objectID": "05-acquiring-files.html",
    "href": "05-acquiring-files.html",
    "title": "5  Working with Files",
    "section": "",
    "text": "5.1 Objectives\nReally basic issues for working with files, such as: - Where files typically live on your computer; - Paths, the “current working directory” and how to change it; - Input formats and methods to convert them; - saving and naming files; - text editors; - cleaning texts; - detecting and converting text file encodings.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Files</span>"
    ]
  },
  {
    "objectID": "05-acquiring-files.html#methods",
    "href": "05-acquiring-files.html#methods",
    "title": "5  Working with Files",
    "section": "5.2 Methods",
    "text": "5.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Files</span>"
    ]
  },
  {
    "objectID": "05-acquiring-files.html#examples",
    "href": "05-acquiring-files.html#examples",
    "title": "5  Working with Files",
    "section": "5.3 Examples",
    "text": "5.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Files</span>"
    ]
  },
  {
    "objectID": "05-acquiring-files.html#issues",
    "href": "05-acquiring-files.html#issues",
    "title": "5  Working with Files",
    "section": "5.4 Issues",
    "text": "5.4 Issues\nAdditional issues.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Files</span>"
    ]
  },
  {
    "objectID": "05-acquiring-files.html#further-reading",
    "href": "05-acquiring-files.html#further-reading",
    "title": "5  Working with Files",
    "section": "5.5 Further Reading",
    "text": "5.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Files</span>"
    ]
  },
  {
    "objectID": "05-acquiring-files.html#exercises",
    "href": "05-acquiring-files.html#exercises",
    "title": "5  Working with Files",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\nAdd some here.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working with Files</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html",
    "href": "06-acquiring-packages.html",
    "title": "6  Using Text Import Tools",
    "section": "",
    "text": "6.1 Objectives",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html#objectives",
    "href": "06-acquiring-packages.html#objectives",
    "title": "6  Using Text Import Tools",
    "section": "",
    "text": "The readtext package\nUsing API gateway packages: twitteR and other social media packages\nDealing with JSON formats\nNewspaper formats, LexisNexis, etc.\nOCR tools",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html#methods",
    "href": "06-acquiring-packages.html#methods",
    "title": "6  Using Text Import Tools",
    "section": "6.2 Methods",
    "text": "6.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html#examples",
    "href": "06-acquiring-packages.html#examples",
    "title": "6  Using Text Import Tools",
    "section": "6.3 Examples",
    "text": "6.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html#issues",
    "href": "06-acquiring-packages.html#issues",
    "title": "6  Using Text Import Tools",
    "section": "6.4 Issues",
    "text": "6.4 Issues\nAdditional issues.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html#further-reading",
    "href": "06-acquiring-packages.html#further-reading",
    "title": "6  Using Text Import Tools",
    "section": "6.5 Further Reading",
    "text": "6.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "06-acquiring-packages.html#exercises",
    "href": "06-acquiring-packages.html#exercises",
    "title": "6  Using Text Import Tools",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\nAdd some here.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Using Text Import Tools</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html",
    "href": "07-acquiring-internet.html",
    "title": "7  Obtaining Texts from the Internet",
    "section": "",
    "text": "7.1 Objectives",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html#objectives",
    "href": "07-acquiring-internet.html#objectives",
    "title": "7  Obtaining Texts from the Internet",
    "section": "",
    "text": "Web scraping\nMarkup formats\nChallenges of removing tags\nAPIs, and JSON",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html#methods",
    "href": "07-acquiring-internet.html#methods",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.2 Methods",
    "text": "7.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html#examples",
    "href": "07-acquiring-internet.html#examples",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.3 Examples",
    "text": "7.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html#issues",
    "href": "07-acquiring-internet.html#issues",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.4 Issues",
    "text": "7.4 Issues\nAdditional issues.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html#further-reading",
    "href": "07-acquiring-internet.html#further-reading",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.5 Further Reading",
    "text": "7.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "07-acquiring-internet.html#exercises",
    "href": "07-acquiring-internet.html#exercises",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.6 Exercises",
    "text": "7.6 Exercises\nAdd some here.",
    "crumbs": [
      "Acquiring Texts",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Obtaining Texts from the Internet</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html",
    "href": "08-quanteda-overview.html",
    "title": "8  Introducing the quanteda Package",
    "section": "",
    "text": "8.1 Objectives",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html#objectives",
    "href": "08-quanteda-overview.html#objectives",
    "title": "8  Introducing the quanteda Package",
    "section": "",
    "text": "Purpose\nBasic object types in quanteda\nInter-related classes\nWorkflow\nKey differences between quanteda and other packages\nWorking with other packages",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html#methods",
    "href": "08-quanteda-overview.html#methods",
    "title": "8  Introducing the quanteda Package",
    "section": "8.2 Methods",
    "text": "8.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html#examples",
    "href": "08-quanteda-overview.html#examples",
    "title": "8  Introducing the quanteda Package",
    "section": "8.3 Examples",
    "text": "8.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html#issues",
    "href": "08-quanteda-overview.html#issues",
    "title": "8  Introducing the quanteda Package",
    "section": "8.4 Issues",
    "text": "8.4 Issues\nAdditional issues.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html#further-reading",
    "href": "08-quanteda-overview.html#further-reading",
    "title": "8  Introducing the quanteda Package",
    "section": "8.5 Further Reading",
    "text": "8.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "08-quanteda-overview.html#exercises",
    "href": "08-quanteda-overview.html#exercises",
    "title": "8  Introducing the quanteda Package",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\nAdd some here.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introducing the **quanteda** Package</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html",
    "href": "09-quanteda-corpus.html",
    "title": "9  Creating and Managing Corpora",
    "section": "",
    "text": "9.1 Objectives\nIn this chapter, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html#methods",
    "href": "09-quanteda-corpus.html#methods",
    "title": "9  Creating and Managing Corpora",
    "section": "9.2 Methods",
    "text": "9.2 Methods\nEvery text analysis project begins with a set of texts, grouped in a collection known in text analysis as a corpus. A corpus is a body of documents generally collected for common purpose. Each corpus usually consists of three elements: the documents containing text, variables specific to each document, and metadata about the corpus as a whole.\nCreating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. A text corpus could include all articles published on immigration in Irish newspapers, with each article recorded as one document in the text corpus. A corpus could also be the reviews about one specific hotel, or a random sample of reviews about hotels in Europe. Researchers need to consider and justify why certain texts are (not) included in a corpus, since the generalisability and validity of findings can depend on the documents selected for analysis.\nThe principles of your specific project or research design should guide your decisions to include or exclude of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in televised debates and campaign speeches, the corpus will contain debate transcripts and speeches. Thus, the research question should drive document selection.\nIn some text analysis applications, the sample could constitute the entirety of available text documents. Analysing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information not transcribed or published cannot be included in the analysis. When analysing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions (Herzog and Benoit 2015). The positions of politicians not selected for speaking at a budget debate cannot be considered in the textual analysis. Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.\n\n\n\n\n\n\nIncluding and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches (Benoit, Munger, and Spirling 2019). Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in Chapter 17. Chapter 24 shows how to identify variation in topic prevalence for two or more groups of documents.\n\n\n\nBesides the raw text, corpora (usually) include attributes that distinguish texts. We call these attributes document-level variables or, in the language of quanteda, “docvars”. Docvars contain additional information on each document and allow researchers to differentiate between texts in their analysis. Examples of document-level variables are the name of the author of a text, the newspaper in which an article was published, the hotel which was reviewed on TripAdvisor, or the date when the document was created.\nA text corpus also typically contains metadata recording important information about the corpus as a whole. Metadata that is generally useful to record in a corpus include the source where the corpus was obtained, including possibly the URL; the author of the corpus (if from a single source); a title for the corpus; and possibly important keywords that might be used later in categorising the corpus. Corpus metadata can be quite general, for instance potentially including a codebook, instructions how to cite the corpus, or a license for the use of the corpus or copyright information.\n\n\n\n\n\n\nWhile no universal standard exists for which metadata to record, there are guidelines for corpus objects that are followed in the quanteda packages that provide example corpora, including those in the package that accompanies this book. These all record the following metadata fields: title and description, providing a title and short text description of the corpus respectively; source and url, documenting in plain text and as a web address where the corpus was obtained; author, even when there was no single author of the documents; and keywords, a vector of keywords categorising the corpus. We recommend using this scheme for new corpus object that you create.\n\n\n\nThus far we have been using the terminology of “document” and “text” interchangeably, but the definition of what constitutes a document is more deliberate and more consequential, since documents define the unit of analysis. Just as in making the decision to select which documents should be included in the corpus, the text analyst must also think carefully about what will define a document. Sometimes, this decision is natural, such as in collecting individual product reviews, individual speeches (as in the Presidential inaugural speech corpus), or individual social media posts. Because the definition of a document can be fluid, however, not all such decisions are so clear cut. A user might wish to cut some longer documents into sub-sections (like chapters of a book, or paragraphs of a speech) that will form documents. Or, when facing lots of smaller texts such as posts on Twitter, a user might wish to aggregate these by user, or by day or week, to define new documents.\nAs with the decision to select texts for a corpus, the decision as to precisely how documents should be defined will depend on a user’s needs for a specific project. Sometimes, this will produce a need to redefine the document units in which texts were collected, into document units that resemble the units that the user will analyse for the purposes of a broader study. In their study about the association between emotions and real-time voter reactions, for instance, Boussalis et al. (2021) redefine the speech unit of documents in which campaign speeches where found, into specific utterances that were later analysed as specific statements associated with variable levels of sentiment. In reviews of hotels, the unit could be an individual review of a hotel, the mention of the hotel and its immediate context, or all texts that review a hotel. The point is that the document units in which texts are collected may not be the same as the document units that a text researcher will need for the purposes of their analysis. In the section that follows, we will show to reshape, split, and combine document units from a corpus that allow a flexible redefinition of document-level units to meet specific analytic needs. We also cover how to access and modify all elements of the corpus mentioned in this section.\n\n\n\n\n\n\nSome tools for text analysis call for “cleaning” a corpus, sometimes by removing elements that are deemed to be unwanted, such as punctuation. We strongly discourage this, because such radical interventions lessen the generality of a corpus. We take the same approach to defining documents: We prefer that a corpus contain natural units of analysis, such a individual reviews, rather than analytic units of analysis such as combined or aggregated reviews. Chapter 11 and Chapter 12 show to combine documents to the analytic unit of analysis using tokens_group() or dfm_group(). Cleaning should be limited to removing textual cruft, such as page numbers if a document was converted from pdf format, and these are not of interest.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html#applications",
    "href": "09-quanteda-corpus.html#applications",
    "title": "9  Creating and Managing Corpora",
    "section": "9.3 Applications",
    "text": "9.3 Applications\nIn this section, we will demonstrate how to create a corpus from different input sources, how to access and assign docvars and corpus metadata, how to subset documents from a corpus based on document-level variables, and how to draw a random sample of documents from a text corpus.\n\n9.3.1 Creating a Corpus from Different Input Sources\nWe can create a quanteda corpus from different input sources, for example a data frame, a tm corpus object, or a keyword-in-context object (kwic).\n\n\n\n\n\n\nIn Chapter 5 we show how to use the readtext package for importing texts stored in various formats, e.g., PDF documents, Word documents, or spreadsheets. The output of the readtext() function is always a data frame, which can be easily transformed to a corpus object using corpus().\n\n\n\nCreating a corpus from a data frame or tibble object is straightforward. We use corpus() and determine the text_field that contains the text. By default, all remaining variables of the data frame will be added as docvars.\n\nlibrary(\"quanteda\")\n\n# create data frame for illustration purposes\ndat &lt;- data.frame(\n    letter_factor = factor(rep(letters[1:3], each = 2)),\n    some_ints = 1L:6L,\n    some_text = paste0(\"This is text number \", 1:6, \".\"),\n    stringsAsFactors = FALSE,\n    row.names = paste0(\"from_df_\", 1:6)\n)\n\nhead(dat)\n\n          letter_factor some_ints              some_text\nfrom_df_1             a         1 This is text number 1.\nfrom_df_2             a         2 This is text number 2.\nfrom_df_3             b         3 This is text number 3.\nfrom_df_4             b         4 This is text number 4.\nfrom_df_5             c         5 This is text number 5.\nfrom_df_6             c         6 This is text number 6.\n\n# create corpus\ncorp_dataframe &lt;- corpus(dat, text_field = \"some_text\")\n\nsummary(corp_dataframe)\n\nCorpus consisting of 6 documents, showing 6 documents:\n\n      Text Types Tokens Sentences letter_factor some_ints\n from_df_1     6      6         1             a         1\n from_df_2     6      6         1             a         2\n from_df_3     6      6         1             b         3\n from_df_4     6      6         1             b         4\n from_df_5     6      6         1             c         5\n from_df_6     6      6         1             c         6\n\n\nWe can also create a quanteda corpus from a tm corpus object. The following example uses the corpus crude corpus from the tm package which contains 20 exemplary news articles.\n\n# load in a tm example VCorpus object\ndata(crude, package = \"tm\")\n\n# create quanteda corpus\ncorp_newsarticles &lt;- corpus(crude)\n\nIn addition, we can create a text corpus from a kwic() object. Keywords-in-context is covered extensively in Chapter 15.\nFor example, we could extract all mentions of “freedom”, “war”, and “economy”, and the immediate context of 20 words from our corpus of hotel reviews, and convert the output to a new text corpus.\n\n# create keyword-in-context object for three terms\n# and a window of ±20 tokens\nkw_inaugural &lt;- data_corpus_inaugural |&gt;\n    tokens(remove_separators = FALSE) |&gt;\n    kwic(pattern = c(\"freedom\", \"war\",\"econom*\"),\n         window = 20, separator = \" \")\n\n# check number of matches\nnrow(kw_inaugural)\n\n[1] 478\n\n# check how often each pattern was matched\ntable(kw_inaugural$pattern)\n\n\nfreedom     war econom* \n    185     181     112 \n\n# convert kwic object to a new text corpus\ncorp_kwic &lt;- corpus(kw_inaugural, split_context = FALSE)\n\nprint(corp_kwic, max_ndoc = 5, max_nchar = 40)\n\nCorpus consisting of 478 documents and 1 docvar.\n1789-Washington.L1841 :\n\"truth   more   thoroughly   established ...\"\n\n1797-Adams.L367 :\n\"The   zeal   and   ardor   of   the   pe...\"\n\n1801-Jefferson.L2619 :\n\"best   reliance   in   peace   and   for...\"\n\n1801-Jefferson.L2652 :\n\"  the   supremacy   of   the   civil   o...\"\n\n1801-Jefferson.L2756 :\n\"  all   abuses   at   the   bar   of   t...\"\n\n[ reached max_ndoc ... 473 more documents ]\n\n\n\n\n9.3.2 Inspecting Document-Level Variables and Metadata of a Corpus\nDocument-level variables are crucial for many text analysis projects. For instance, if we want to compare differences in issue emphasis in inaugural before and after World War II, we need a document-level variable specifying the year when a speech was delivered. We may also want to create a binary variable indicating whether a speech was delivered before or after World War II. Below, we provide examples on how to inspect document-level variables and how to create new docvars.\nFirst, we load the packages and inspect the text corpus using summary().\n\n# provide summary of corpus and print document-level variables for the first six documents\nsummary(data_corpus_inaugural, n = 6)\n\nCorpus consisting of 59 documents, showing 6 documents:\n\n            Text Types Tokens Sentences Year  President FirstName\n 1789-Washington   625   1537        23 1789 Washington    George\n 1793-Washington    96    147         4 1793 Washington    George\n      1797-Adams   826   2577        37 1797      Adams      John\n  1801-Jefferson   717   1923        41 1801  Jefferson    Thomas\n  1805-Jefferson   804   2380        45 1805  Jefferson    Thomas\n    1809-Madison   535   1261        21 1809    Madison     James\n                 Party\n                  none\n                  none\n            Federalist\n Democratic-Republican\n Democratic-Republican\n Democratic-Republican\n\n\nThe corpus consists of 59 documents. Each document is one inaugural speech delivered between 1789 and 2021.\nThe meta() function returns a named list containing the corpus-level information stored in the corpus, reflecting the standard set of metadata fields that we have chosen to use for quanteda objects. The “keywords” element of this list is a character vector containing five keywords.\n\nmeta(data_corpus_inaugural)\n\n$description\n[1] \"Transcripts of all inaugural addresses delivered by United States Presidents, from Washington 1789 onward.  Data compiled by Gerhard Peters.\"\n\n$source\n[1] \"Gerhard Peters and John T. Woolley. The American Presidency Project.\"\n\n$url\n[1] \"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses\"\n\n$author\n[1] \"(various US Presidents)\"\n\n$keywords\n[1] \"political\"     \"US politics\"   \"United States\" \"presidents\"   \n[5] \"presidency\"   \n\n$title\n[1] \"US presidential inaugural address speeches\"\n\n\nWe can also access the names of our docvars.\n\nnames(docvars(data_corpus_inaugural))\n\n[1] \"Year\"      \"President\" \"FirstName\" \"Party\"    \n\n\nWe see that the Year variable stores the year when each speech was delivered. We could create a binary variable PrePostWW2 which distinguishes between speeches held before and after 1945. T\n\n# use $ operator\ndata_corpus_inaugural$PrePostWW2 &lt;-\n    ifelse(data_corpus_inaugural$Year &gt; 1945,\n           \"Post World War II\", \"Pre World War II\"\n    )\n\n# equivalent to\ndocvars(data_corpus_inaugural, \"PrePostWW\") &lt;-\n    ifelse(docvars(data_corpus_inaugural, \"Year\") &gt; 1945,\n           \"Post World War II\", \"Pre World War II\"\n    )\n\n\n\n\n\n\n\nThere are multiple ways to access docvars in a quanteda object—here a corpus, but also other objects derived from a corpus such as tokens object, or separate objects such as dictionaries. These can be docvars(data_corpus_inaugural, \"Party\") or, for individual docvars, using the $ operator known from lists or data.frames, i.e. data_corpus_inaugural$Party.\n\n\n\n\n\n9.3.3 Subsetting a Corpus\nApplying corpus_subset() to a corpus allows us to subset a text corpus based on values of document-level variables. We can filter, for instance, only speeches delivered by Democratic Presidents, speeches delivered after World War II, or speeches from selected presidents.\n\n# subset only Republican Presidents\ncorp_democrats &lt;- corpus_subset(data_corpus_inaugural, Party == \"Republican\")\n\n# subset speeches delivered after WW2\ncorp_postww2 &lt;- corpus_subset(data_corpus_inaugural,\n                              PrePostWW2 == \"Post World War II\")\n\ncorp_postww2 &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1945)\n\n# subset speeches delivered by Clinton, Obama, and Biden\ncorp_cob &lt;- corpus_subset(data_corpus_inaugural,\n                          President %in% c(\"Clinton\", \"Obama\", \"Biden\"))\n\n# remove Biden's 2021 speech\ncorp_nobiden &lt;- corpus_subset(data_corpus_inaugural,\n                              President != \"Biden\")\n\n\n\n\n\n\n\nUsually, we use logical operators for subsetting a text corpus or creating a new document-level variable based on certain conditions. The most relevant logical operators are:\n\n&lt;: less than\n&lt;=: less than or equal to\n&gt;: greater than\n&gt;=: greater than or equal to\n==: equal\n!=: not equal\n!x: not x (negation)\nx | y: x OR y\nx & y: x AND y\n\n\n\n\n\n\n9.3.4 Randomly Sampling Documents From a Corpus\nOften, users may be interested in taking a random sample of documents from the text corpus. For example, a user may implement and test the workflow for a small, random subset of the documents, and only later run the analysis on the full text corpus.\nThe corpus_sample() function allows for sampling documents from a specified size (size) with or without replacement (replace), optionally stratified by grouping variables (by) or with probability weights (prob).\n\n# sample 30 inaugural speeches without replacement\nset.seed(10987)\ncorp_sample30 &lt;- corpus_sample(data_corpus_inaugural, size = 30, \n                               replace = FALSE)\n\n# check that the corpus consists of 30 documents\nndoc(corp_sample30)\n\n[1] 30\n\ncorp_sample_prepost &lt;- corpus_sample(data_corpus_inaugural,\n                                     size = 15,\n                                     replace = FALSE, by = PrePostWW2)\n\n# check that corpus contains 15 pre- and 15 post-WW II documents\ntable(corp_sample_prepost$PrePostWW2)\n\n\nPost World War II  Pre World War II \n               15                15",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html#advanced",
    "href": "09-quanteda-corpus.html#advanced",
    "title": "9  Creating and Managing Corpora",
    "section": "9.4 Advanced",
    "text": "9.4 Advanced\n\n9.4.1 Corpus “Cleaning”\nIt’s common to speak about “cleaning” a corpus, by removing unwanted elements such as numbers or punctuation characters. Our approach is more conservative, however, since we prefer to preserve the entirety of a document in our corpus, leaving the decision as to which elements to keep or remove to downstream processing. Why do we advocate this approach, and what do we mean by it?\nIf we recall of the text analysis workflow from Chapter 8, the construction of the corpus is the first step, and tokenisation the second. Most of what is “cleaned” from a corpus in some forms of natural language processing consist of removing categories of tokens, such as whitespace, punctuation, numbers, or even specific words (for instance “stopwords”). But this step cannot even take place until a document has been processed by being tokenised. (We view it as ironic, therefore, that decisions on which tokens to remove is often called “pre-processing”.) But the decision as to what should be removed and what to be kept is very unlikely to be universally agreed. Indeed, the decision as to what even constitutes a token may differ (something we cover more in the next two chapters). If there is no universal agreement, then a corpus should preserve the entire text of each document, including all of its linguistic elements, and leave the decision as to what to remove to be open-ended and adaptable to whatever use to which the corpus is being put.\nTo put this another way, we think that the language itself of “cleaning” texts is misguided. If we think of decisions about what to retain from texts as one of cleaning, it implies that some elements are a form of dirt to be scrubbed away. But what to one text analyst might be dirt to be wiped away might be rich topsoil to another. In most stopword lists, for instance, pronouns are removed, but permanently removing these from a corpus would make it impossible to analyse gendered language, where gendered pronouns are markers for substantive differences — for instance, Monroe, Quinn, and Colaresi (2008) found that gendered pronouns in the Senate floor were far more often used by Democratic than by Republic Party speakers. In their classic study of the authorship of the unsigned Federalist Papers, Mosteller and Wallace (1963) discovered that it was the statistical distribution of common words such as articles, conjunctions, and prepositions that differentiated the articles penned by Hamilton instead of Madison. The lesson is that it is far better then to leave the original documents intact rather than presuppose that any specific form of surgery will always improve them.\n\n\n\n\n\n\nPunctuation and capitalisation are often used by natural language processing tools to detect boundaries between sentences or clauses. In the dataset that accompanies Pang, Lee, and Vaithyanathan (2002)’s classic study of the Naive Bayes classifier to detect sentiment from movie reviews, the reviews have been converted to lowercase and the punctuation characters separated from the words they would normally follow. Once this step has been taken, it is very difficult or impossible to reconstruct the original text as it was originally written.\n\n\n\nBy relegating the decision to “downstream” processing, we mean that decisions on “cleaning” up documents become issues of feature selection. Feature selection is a better framing of the decision to intervene in the content of our documents prior to analysis, since feature selection underscores that the goal is to support a particular type of analysis, not to scrub away elements that are innately undesirable.\nAn exception to this rule where we do feel it is appropriate to use the language of cleaning up texts, or least tidying them, is when they have been converted from other formats that generate content that is not part of the text itself, but rather artefacts of the converted format. When converting from pdf for instance — a format designed for the printed page — the resulting plain text may frequently contain page numbers, headers, or footers that are not part of a document’s core text, and say more about the printed page and font sizes than about the document content. We would always remove these.\nAnother exception is when a document contains information at the beginning or end that are more usefully treated as metadata, such as a title, author, or date. These we would remove and put into document variables. All Project Gutenberg books, for instance, contain header information about the project and its license, metadata about the title, author, release date, and language, among other fields. We would always trim this part, and separate it into document-level metadata (or corpus-level metadata, if the documents are chapters).\n\n\n9.4.2 The Structure of a quanteda Corpus Object\nAs we will see with most of the core object classes in quanteda, its text analytic objects tend to be special versions of common R object types. In the R language, the atomic data type for recording textual data is the object type known as “character”. A character data type is a an atomic vector that has a length and attributes. In quanteda, a corpus is simply a special type of character object, with its attributes recording the extra information that make it special, including its metadata, its document-level variables, and other hidden information that are used by quanteda functions.\nThe advantage of building a corpus on the character data type is a property known as inheritance: functions that already work on a character data object will also work on a quanteda corpus. As a language built entirely around vectorised operations, furthermore, having a corpus built around a type of atomic vector is highly efficient. The other advantage is simplicity: to convert a character vector to a corpus, we only need to add special attributes. To convert a corpus back to a character vector, we can simply drop these attributes (using the coercion method as.character(), which we detail in the next section).\n\n\n\n\n\n\nThere are some operations that requires a bit of additional care in order to preserve a corpus object’s special attributes. Some low-level functions, especially those that call underlying functions in another language (C or C++) may revert a corpus to a character type if not handled carefully. There is a way around this, however: Use the subset operator [.\nTo replace a character in a corpus with another, for instance, will typically strip its corpus attributes. We can get around this by using the subset operator:\n\n# not a corpus any longer\ncorp &lt;- corpus(c(letters[1:3]))\ncorp &lt;- stringr::str_replace_all(corp, \"b\", \"B\")\ncorp\n\n[1] \"a\" \"B\" \"c\"\n\n# still a corpus\ncorp &lt;- corpus(c(letters[1:3]))\ncorp[seq_along(corp)] &lt;- stringr::str_replace_all(corp, \"b\", \"B\")\ncorp\n\nCorpus consisting of 3 documents.\ntext1 :\n\"a\"\n\ntext2 :\n\"B\"\n\ntext3 :\n\"c\"\n\n\n\n\n\n\n\n9.4.3 Conversion from and to Other Formats\nBelow, we provide examples of converting a quanteda corpus to other objects. Converting a corpus object to a data frame could be useful, for instance, if you want to reshape a corpus to sentences and hand-code a sample of these sentences for validation purposes or a training set for supervised classifiers (see Chapter 23). The example below shows how to sample 1,000 hotel reviews, and converting this corpus to a data frame.\n\nlibrary(\"TAUR\")\n\n# set seed for reproducibility\nset.seed(35)\n\ncorp_TAhotels_sample &lt;- corpus_sample(data_corpus_TAhotels,\n                                      size = 1000,\n                                      replace = FALSE)\n\n# convert corpus to data frame\ndat_TAhotels_sample &lt;- convert(corp_TAhotels_sample, to = \"data.frame\")\n\nwhich will consist of the columns doc_id, containing the document name; text, containing the actual text of the document; and any docvars that were in the corpus, which in this example consist of location and date.\nThe resulting data frame consists of 1000 randomly sampled reviews and 3 columns: doc_id, containing the document name; text, containing the actual text of the document; and any docvars that were in the corpus (in our case Rating).\nWe can easily store this data frame as a spreadsheet, which can be useful when hand-code a selection of reviews (for instance, based on their sentiment).\n\n# store data frame as csv file with UTF-8 encoding and remove row names\nwrite.csv(dat_TAhotels_sample_base_r,\n          file = \"dat_TAhotels_sample.csv\",\n          fileEncoding = \"UTF-8\",\n          row.names = FALSE)\n\n# the rio package allows us to store\n# the data frame in many different file formats\nlibrary(\"rio\")\n\nexport(dat_TAhotels_sample_base_r,\n       file = \"dat_TAhotels_sample.xlsx\")\n\n\n\n9.4.4 Converting or Coercing to Other Object Types\nWe can coerce an object to a text corpus (as.corpus()) or extract the texts from a corpus (as.character()). as.corpus() can only be applied to a quanteda corpus object and upgrades it to the newest format. This function can be handy for researchers who work with older quanteda objects. For transforming data frames or tm corpus objects into a quanteda corpus, you should use the corpus() function. The function as.character() returns the corpus text as a plain character vector.\n\n# retrieve texts from corpus object\nchars_TAhotels &lt;- as.character(data_corpus_TAhotels)\n\n# inspect character object\nstr(chars_TAhotels)\n\n Named chr [1:20491] \"nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous re\"| __truncated__ ...\n - attr(*, \"names\")= chr [1:20491] \"text1\" \"text2\" \"text3\" \"text4\" ...\n\n\n\n\n9.4.5 Changing the Unit of Analysis\nIn a corpus, the units are documents, and each document contains text. But what defines a “document”? We have already discussed how we might split a naturally found found document into smaller elements, for example splitting a book into new documents consisting of single chapters. Going in the other direction, we might also group naturally occurring documents, such as social media posts, into a user’s combined weekly or daily posts, especially when these are short format posts such as Tweets.\nThis ability to redefine document units in terms of split or grouped textual units points to a curious feature of textual data that other data, such as numerical data, do not possess. If we think of the processed elements of documents as features, such as tokens that might be tabulated into a table of token counts by document, then the units of analysis of that table—the documents—are defined in terms of collections of its columns—the token features. Since a document is just an arbitrary collection of terms, however, it means that the more we segment our document into smaller collections, the more it approaches being a features unit defined by the column dimension of the data. Conversely, grouping documents together does the opposite. Redefining the boundaries of what constitutes a “document”, therefore, involves shifting data from columns into rows or vice versa. This ability to reshape our data matrix because one dimension is defined in terms of a collection of the other is unique to text analysis. We could not perform a similar reshaping operation on say, a survey dataset where by spreading an individual’s observed responses across additional rows, because we cannot split an individual as a unit and because that individual is defined in terms of a sampled, physical individual, not as an arbitrary collection of survey questions.\nUltimately, how we reshape our documentary units by grouping or splitting them will depend on our research question and the needs of our method for analysing the data. Knowing how the sampling procedure for the textual data selection relates to the sampling units and the units of analysis may have implications for subsequent inference, given that the units of analysis are not randomly sampled textual data, irrespective of the sampling units. Determining which are most suitable will depend on the nature of the analytical technique and the insight it is designed to yield, and sometimes the length and nature of the texts themselves.\nTo illustrate how we can reshape a corpus into smaller units and then re-aggregate them into larger units, but different from the original units, we will demonstrate the construction of a corpus that requires some reshaping. For this purpose, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.1 The corpus TAUR package contains this text corpus as data_corpus_debates.\n\n# inspect text corpus\nsummary(data_corpus_debates)\n\nCorpus consisting of 2 documents, showing 2 documents:\n\n              Text Types Tokens Sentences             location       date\n Debate 2020-09-29  2454  24836      2039      Cleveland, Ohio 2020-09-29\n Debate 2020-10-22  2403  21726      1488 Nashville, Tennessee 2020-10-22\n\n\nThe output of summary(data_corpus_debates) reveals that the corpus contains only two documents, i.e. the full debate transcript. The first document contains the transcript of the debate in Cleveland, consisting of 24,836 tokens, 2,454 types (i.e., unique tokens), and 2,039 sentences. The second debate in Nashville is slightly shorter (21,726 tokens and 2,403 types).\n\n\n\n\n\n\n“Types” and “tokens” are specific linguistic terms that refer to the quantity and quality of the linguistic units found in a text. A type is a unique lexical unit, and a token is any lexical unit. Lexical units are most often words, but may also include punctuation characters, numerals, emoji, or even spaces. We cover this in greater detail in Chapter 10.\n\n\n\nThe debate corpus contains only two documents, each a transcript of a length debate that included many different statements by the presidential candidates and the moderator. The corpus recorded these “natural” speech units as a document, but in the case of transcripts, that is often not the “analytical” document unit that will meet the analyst’s needs.\nTo make the document unit align with our analytical purposes, we will need to reshape the corpus by segmenting it into individual statements, recording the speaker and the sequence in which the statement occurred, as new document-level variables.\nThe tool for this segmentation in quanteda is corpus_segment(). As discussed in Chapter 8, the name of this function reflects the object that it takes as an input and produces as an output (a corpus) and the verb element describes the operation it will perform (segmentation). To perform this segmentation, we will rely on the presence in the transcript of regular patterns marking the introduction of a new statement. quanteda can take multiple forms of patterns, including a fixed match, a “glob” match, and a full regular expression match. Because we need to match a very specific pattern with more elements than a simple glob pattern can handle, we will need to use the regular expression “valuetype”. (For a brief primer on pattern matching and regular expressions see Appendix C.)\nIn the transcript, a statement starts with the speaker’s name in ALL CAPS, followed by a colon. To match this marker of a new statement, we will use the regular expression \"\\\\s*[[:upper:]]+:\\\\s+\". This identifies speaker names in ALL CAPS (\\\\s*[[:upper:]]+), indicating no or more spaces followed by one or more upper case words, followed by a colon (the literal:), followed by one or more white spaces \\\\s+. The case_insensitive = FALSE tells the pattern matcher to pay attention to case (how a word’s letters are capitalised or not).\nBy default, corpus_segment() will split the original documents into smaller documents corresponding to the segments preceded by the pattern, and extract the pattern found to a new variable for the split document (a docvar named pattern). It is also smart enough to record the original, longer document from which each split document originally came.\n\n# segment text corpus to level of utterances\ndata_corpus_debatesseg &lt;- corpus_segment(data_corpus_debates,\n                                         pattern = \"\\\\s*[[:upper:]]+:\\\\s+\",\n                                         valuetype = \"regex\",\n                                         case_insensitive = FALSE\n)\n\n# overview of text corpus; n = 4 prints only the first four documents\nsummary(data_corpus_debatesseg, n = 4)\n\nCorpus consisting of 1208 documents, showing 4 documents:\n\n                Text Types Tokens Sentences        location       date\n Debate 2020-09-29.1   139    251        13 Cleveland, Ohio 2020-09-29\n Debate 2020-09-29.2     6      6         1 Cleveland, Ohio 2020-09-29\n Debate 2020-09-29.3     5      5         1 Cleveland, Ohio 2020-09-29\n Debate 2020-09-29.4     3      3         1 Cleveland, Ohio 2020-09-29\n     pattern\n   WALLACE: \n \\n\\nBIDEN: \n \\n\\nTRUMP: \n \\n\\nBIDEN: \n\n\nThe new, split corpus, has turned the 2 original documents into 1,208 new documents. Note also that we have kept the quanteda naming conventions in assigning the new object a name that identifies it as data, as a corpus, and describes what it is. Because any function in quanteda that begins with corpus_ will always take in and output a corpus, we know that this object will be a corpus.\nIn the split document, the new docvar pattern contains the pattern that we matched. Because this also included additional elements such as the spaces and the colon, however, it’s not as clean as we would prefer. Let’s turn this into a new speaker document-level variable by cleaning it up and renaming it. To make this easy, we will use the stringr for string manipulation, and the quanteda.tidy package for applying some dplyr functions from the “tidyverse” to manipulate the variables. Specifically, we will use mutate() to create a new speaker variable, and the stringr functions to remove empty whitespace (str_trim()) and the colon (str_remove_all()), and to change the names from UPPER CASE to Title Case (str_to_title()).\n\nlibrary(\"stringr\")\nlibrary(\"quanteda.tidy\")\n\ndata_corpus_debatesseg &lt;- data_corpus_debatesseg |&gt;\n    mutate(speaker = stringr::str_trim(pattern),\n           speaker = stringr::str_remove_all(speaker, \":\"),\n           speaker = stringr::str_to_title(speaker))\n\nNext, we can use simple base R functions to inspect the count of utterances by speaker and debate.\n\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$location,\n      data_corpus_debatesseg$speaker)\n\n                      \n                       Biden Trump Wallace Welker\n  Cleveland, Ohio        269   341     246      0\n  Nashville, Tennessee    84   122       0    146\n\n\nWe could further reshape the corpus to the level of sentences with corpus_reshape() if we are interested, for instance, in sentence-level sentiment or issue salience.\n\ndata_corpus_debatessent &lt;- corpus_reshape(data_corpus_debatesseg,\n                                          to = \"sentences\")\n\nndoc(data_corpus_debatessent)\n\n[1] 3560\n\n\nThe new text corpus moved from 1208 utterances to 3560 sentences. Using functions such as quanteda.textstat’s textstat_summary() we can retrieve summary statistics of the sentence-level corpus.\n\nlibrary(\"quanteda.textstats\")\ndat_summary_sents &lt;- textstat_summary(data_corpus_debatessent)\n\n# aggregated summary statistics\nsummary(dat_summary_sents)\n\n   document             chars            sents       tokens         types      \n Length:3560        Min.   :  3.00   Min.   :1   Min.   : 1.0   Min.   : 1.00  \n Class :character   1st Qu.: 24.00   1st Qu.:1   1st Qu.: 6.0   1st Qu.: 6.00  \n Mode  :character   Median : 41.00   Median :1   Median :10.0   Median : 9.00  \n                    Mean   : 55.32   Mean   :1   Mean   :12.4   Mean   :10.85  \n                    3rd Qu.: 73.00   3rd Qu.:1   3rd Qu.:16.0   3rd Qu.:14.00  \n                    Max.   :414.00   Max.   :1   Max.   :79.0   Max.   :56.00  \n     puncts          numbers           symbols             urls        tags  \n Min.   : 0.000   Min.   :0.00000   Min.   :0.00000   Min.   :0   Min.   :0  \n 1st Qu.: 1.000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0   1st Qu.:0  \n Median : 2.000   Median :0.00000   Median :0.00000   Median :0   Median :0  \n Mean   : 2.074   Mean   :0.07893   Mean   :0.01376   Mean   :0   Mean   :0  \n 3rd Qu.: 3.000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0   3rd Qu.:0  \n Max.   :15.000   Max.   :4.00000   Max.   :2.00000   Max.   :0   Max.   :0  \n     emojis \n Min.   :0  \n 1st Qu.:0  \n Median :0  \n Mean   :0  \n 3rd Qu.:0  \n Max.   :0  \n\n\n\n\n\n\n\n\nSegmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker’s surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected.\n\n\n\n\n\n9.4.6 Reshaping Corpora after Statistical Analysis of Texts\nIn many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, Castanho Silva and Proksch (2021) study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models.\n\n\n\n\n\n\nApplying the group_by() in combination with the summarise() functions of the dplyr packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html#further-reading",
    "href": "09-quanteda-corpus.html#further-reading",
    "title": "9  Creating and Managing Corpora",
    "section": "9.5 Further Reading",
    "text": "9.5 Further Reading\n\nSelecting document and considerations of “found data”: Grimmer, Roberts, and Stewart (2022, ch. 4)\nDefinitions of document units and the reasons for and implications of redefining these: Benoit (2020, pp480–481, “Defining Documents and Choosing the Unit of Analysis”)\nAdjusting strings: Wickham, Çetinkaya-Rundel, and Grolemund (2023, ch. 15)",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html#exercises",
    "href": "09-quanteda-corpus.html#exercises",
    "title": "9  Creating and Managing Corpora",
    "section": "9.6 Exercises",
    "text": "9.6 Exercises\nIn the exercises below, we use a corpus of TripAdvisor hotel reviews, data_corpus_TAhotels, included in the TAUR package).\n\nIdentify the number of documents in data_corpus_TAhotels.\nSubset the corpus by selecting only reviews with the maximum rating of 5.\nReshape this subsetted corpus to the level of sentences.\nExplore textstat_summary() of the quanteda.textstats package. Apply the function to data_corpus_TAhotels and assign it to an object called tstat_sum_TAhotels.\nWhat are the average, median, minimum, and maximum document lengths?\nAdvanced: use data_corpus_TAhotels filter only speeches consisting of at least 300 tokens by combining ntoken() and corpus_subset().\nCreate a new document-level variable RankingRecoded that which splits up the rankings into three categories: Negative (Ranking = 1 and 2), Neutral (Ranking = 3), and Positive (Ranking = 4 and 5). Which of the three categories is the most frequent one?\n\n\n\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 461–97. Thousand Oaks: Sage.\n\n\nBenoit, Kenneth, Kevin Munger, and Arthur Spirling. 2019. “Measuring and Explaining Political Sophistication Through Textual Complexity.” American Journal of Political Science 63 (2): 491–508. https://doi.org/10.1111/ajps.12423.\n\n\nBoussalis, Constantine, Travis G. Coan, Mirya R. Holman, and Stefan Müller. 2021. “Gender, Candidate Emotional Expression, and Voter Reactions During Televised Debates.” American Political Science Review 115 (4): 1242–57. https://doi.org/10.1017/S0003055421000666.\n\n\nCastanho Silva, Bruno, and Sven-Oliver Proksch. 2021. “Politicians Unleashed? Political Communication on Twitter and in Parliament in Western Europe.” Political Science Research and Methods published ahead of print (doi: 10.1017/psrm.2021.36). https://doi.org/10.1017/psrm.2021.36.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts: A New Framework for Machine Learning and the Social Sciences. Princeton: Princeton University Press.\n\n\nHerzog, Alexander, and Kenneth Benoit. 2015. “The Most Unkindest Cuts: Speaker Selection and Expressed Goverment Dissent During Economic Crisis.” The Journal of Politics 77 (4): 1157–75. https://doi.org/10.1086/682670.\n\n\nMonroe, B. L., K. M. Quinn, and M. P. Colaresi. 2008. “Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict.” Political Analysis 16 (4): 372–403.\n\n\nMosteller, F., and D. L. Wallace. 1963. “Inference in an Authorship Problem.” Journal of the American Statistical Assocation 58 (302): 275–309.\n\n\nPang, B., L. Lee, and S. Vaithyanathan. 2002. “Thumbs up? Sentiment Classification Using Machine Learning Techniques.” Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 79–86.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. Sebastopol: O’Reilly. https://r4ds.hadley.nz.",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "09-quanteda-corpus.html#footnotes",
    "href": "09-quanteda-corpus.html#footnotes",
    "title": "9  Creating and Managing Corpora",
    "section": "",
    "text": "The transcripts are available from the American Presidency Project, specifically here and here.↩︎",
    "crumbs": [
      "Creating and Managing Corpora",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Creating and Managing Corpora</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html",
    "href": "10-quanteda-tokens.html",
    "title": "10  Creating and Managing Tokens",
    "section": "",
    "text": "10.1 Objectives\nIn this chapter, we cover the basics of tokenisation and the quanteda tokens object. You will learn what to pay attention to when tokenizing texts, and how to select, keep, and remove tokens. We explain methods for selecting tokens to remove or to modify, for instance removing “stopwords” or removing suffixes through stemming and lemmatisation, methods for reducing words to their base or root form. Finally, we show how to manage metadata in a tokens object, which largely mirrors the way metadata is managed in a corpus object. At the end of this chapter, the reader will have a solid understanding of how to create and manage tokens objects in quanteda that will serve as a foundation for more advanced tokens manipulation methods in later chapters.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#methods",
    "href": "10-quanteda-tokens.html#methods",
    "title": "10  Creating and Managing Tokens",
    "section": "10.2 Methods",
    "text": "10.2 Methods\nAfter having collected the text for analysis and collecting these texts in a corpus (see Chapter 9), the next most common step is tokenise our texts. Tokenisation is the process of segmenting longer texts into individual units known as tokens, based on semantic, linguistic, or lexographic distinctions. The most common variety of tokens consists of distinct words, but tokens can also be punctuation characters, numeric or alphabetic characters, emoji, or speaces. Tokens can also consist of sequences of these, such as sentences, paragraphs, or word sequences of arbitrary length.\nTokenization is the process of splitting a text into its constituent tokens (which includes punctuation characters as tokens). Tokenization usually happens by recognising the delimiters between words, which in most languages takes the form of a space. In more technical language, inter-word delimiters are known as whitespace, and include additional machine characters such as newlines, tabs, and space variants. Most languages separate words by whitespace, but some major ones such as Chinese, Japanese, and Korean do not. Tokenizing these languages requires a set of rules to recognise word boundaries, usually from a listing of common word endings. Smart tokenizers will also separate punctuation characters that occur immediately following a word, such as the comma after word in this sentence.\nIn quanteda, the built-in tokenizer provides the option to tokenise our texts to different levels: words, sentences, or individual characters. Most of the time, we tokenise our documents to the level of words, although the default “word” tokeniser also separates out punctuation characters and numerals. Each tokenised document will consist of the list of tokens found in that document, but always still organised into the same document units that define the corpus.\nThroughout all tokenisation steps, we know the position of each tokens in the document, which we can use to identify and compound multiword-expressions or apply a dictionary with multiword expressions. These aspects will be covered in much more detail in Chapter 11. For now, it is important to keep in mind the main difference between tokens objects and a document-feature matrix: while we know the relative position of each feature in a tokens object, a document-feature matrix reports the counts of features (which can be words, punctuation characters, numbers, or multiword expressions) in each document, but does not allow us to identify where a certain feature appeared in the document.\nThe next step after the tokenization of our documents is often described as pre-processing, but we prefer “processing.” Processing does not precede the analysis, but is an integral part of the workflow and can influence subsequent results (Denny and Spirling 2018). The most common types is the lower-casing of text (e.g., “Party” will be change to “party”); the removal of punctuation characters and symbols; the removal of so-called stopwords which appear frequently throughout all documents but do not add specific meaning; stemming or lemmatisation; or compounding phrases/multiword expressions to a single token. All of these decisions can influence our results. In this chapter, we focus on lower-casing, the removal of punctuation characters, and stopwords. The subsequent chapter covers more advanced tokenisation approaches, including phrases, tokens replacement, and chunking.\nLower-casing words is a standard procedure in many text analysis projects. The rationale behind this is that “Income” and “income” should be interpreted as the same textual feature due to their shared meaning. Furthermore, it’s a common practice to remove punctuation characters like commas, colons, semi-colons, question marks, and exclamation marks. Though these characters appear prolifically across texts, they often don’t significantly contribute to a quantitative text analysis. However, in certain contexts, punctuation can carry significant weight. For instance, the frequency of question marks can differentiate between positive and negative reviews of movies or hotels. They can also demarcate the rhetoric of opposition versus governing parties in parliamentary debates. Negative reviews might employ more question marks than positive ones, while opposition parties might employ rhetorical questions to criticise the ruling party. Symbols are another category often pruned during text processing.\nThe removal of stopwords prior to quantitative analysis is another frequent step. The rationale behind removing stopwords might be to shrink the vector space, condense the size of document-feature matrices, or prevent common words from inflating document similarities. It’s pivotal to understand that there’s no one-size-fits-all stopwords list. These lists are usually developed by researchers and tend to be domain-specific. Some words might be redundant for specific research topics but invaluable for others. For instance, feminine pronouns like “she” and “her” are integral when scrutinising partisan bias in abortion debates (Monroe and Schrodt 2008), even though they might appear in many stopwords lists. In another case, the word “will” plays a pivotal role in discerning the temporal direction of a sentence (Müller 2022). Applying stopword lists without close inspection may lead to the removal of essential terms, undermining subsequent analysis. It is imperative that researchers critically evaluate which words to retain or exclude.\n\n\n\n\n\n\nStopword lists often originate from two primary methodologies. The first method involves examining frequent words in text corpora and manually pinpointing non-essential features. The second method leverages automated techniques, like term-frequency-inverse-document-frequency (tf-idf), to detect stopwords (Sarica and Luo 2021; Wilbur and Sirotkin 1992). Refer to Chapter 17 for an in-depth exploration of strategies to discern both informative and non-informative features.\n\n\n\nStemming and lemmatisation serve as strategies to consolidate features. Stemming truncates tokens to their stems. In contrast, lemmatisation transforms a word into its fundamental form. Most stemming methodologies use predefined lists of suffixes and associated rules governing suffix removal. Many languages have these lists readily available. An exemplary rule-based stemming algorithm is the Snowball stemmer, developed by Martin F. Porter (Porter 2001). Lemmatisation, being more nuanced than stemming, ensures that tokens align with their root form. For example, a stemmer might truncate “easily” to “easili” and leave “easier” untouched. In contrast, a lemmatiser would convert both “easily” and “easier” to their root form: “easy”. While stemming in particular, and lemmatisation to a lower degree, are very popular processing step, reducing features to their base forms often does not change substantive results. Schofield and Mimno (2016) compare and apply various stemmers before running topic models (Chapter 24). Their careful validation reveals that “stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability” (Schofield and Mimno 2016: 287).",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#applications",
    "href": "10-quanteda-tokens.html#applications",
    "title": "10  Creating and Managing Tokens",
    "section": "10.3 Applications",
    "text": "10.3 Applications\nIn this section, we apply the processing steps described above. The examples in this chapter are limited to tokenizing short texts. In practice and in most other chapters, you will be working with much larger text data sets. We always recommend creating a corpus object first and then tokenizing the corpus, rather than moving directly from a character vector or data frame to a tokens object.\n\nNOTE: We could use a diagram here.\n\n\n10.3.1 Tokenizing and Lowercasing Texts\nLet’s start with exploring the tokens() function.\n\n# texts for examples\ntxt &lt;- c(\n    doc1 = \"A sentence, showing how tokens() works.\",\n    doc2 = \"@quantedainit and #textanalysis https://quanteda.org\")\n\n# tokenisation without any processing\ntokens(txt)\n\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n\n\nThe tokens() function includes several arguments for changing the tokenisation.\n\n# tokenise to sentences (rarely used)\ntokens(txt, what = \"sentence\")\n\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"A sentence, showing how tokens() works.\"\n\ndoc2 :\n[1] \"@quantedainit and #textanalysis https://quanteda.org\"\n\n# tokenise to character-level\ntokens(txt, what = \"character\")\n\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\" \"s\" \"e\" \"n\" \"t\" \"e\" \"n\" \"c\" \"e\" \",\" \"s\" \"h\"\n[ ... and 22 more ]\n\ndoc2 :\n [1] \"@\" \"q\" \"u\" \"a\" \"n\" \"t\" \"e\" \"d\" \"a\" \"i\" \"n\" \"i\"\n[ ... and 37 more ]\n\n\nWe can lowercase our tokens object by applying the function tokens_tolower().\n\ntokens(txt) |&gt;\n    tokens_tolower()\n\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"a\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n\n\n\n\n10.3.2 Removing Punctuation, Separators, Symbols\nWe can remove several tokens with inbuilt functions or adjust how hyphens are tokenised.\n\n# remove punctuation\ntokens(txt, remove_punct = TRUE)\n\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"A\"        \"sentence\" \"showing\"  \"how\"      \"tokens\"   \"works\"   \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n\n# remove numbers, symbols, and separators\ntokens(txt,\n       remove_numbers = TRUE,\n       remove_separators = TRUE,\n       remove_symbols = TRUE)\n\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"\n\n# split tags and hyphens\ntokens(txt,\n       split_tags = TRUE,\n       split_hyphens = TRUE)\n\nTokens consisting of 2 documents.\ndoc1 :\n [1] \"A\"        \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"  \n [7] \"(\"        \")\"        \"works\"    \".\"       \n\ndoc2 :\n [1] \"@\"            \"quantedainit\" \"and\"          \"#\"            \"textanalysis\"\n [6] \"https\"        \":\"            \"/\"            \"/\"            \"quanteda.org\"\n\n\nDetails on processing steps are provided in the documentation of the tokens function, which can be accessed through the document for tokens() (accessed by typing ?tokens into the R console).\n\n\n\n\n\n\nWith large text corpora, it might be difficult to assess whether the tokenisation works as expected. We therefore encourage researchers to work with minimal working examples, e.g., one or two sentences that contain certain features you want to tokenise, remove, keep, or compound. You can run your code on this small example and test whether the tokenisation worked as expected before applying the code to the entire corpus.\n\n\n\n\n\n10.3.3 Inspecting and Removing Stopwords\nThe quanteda package contains several functions that process tokens. You start with tokenizing your text corpus, possibly apply some of the processing options included in the tokens() function, and proceed by applying more advanced processing steps, which always start with tokens_.\nLet’s start examining pre-existing stopword lists. We use quanteda’s default Showball stopword list.\n\n# number of stopwords in the English Snowball stopword list\nlength(quanteda::stopwords(\"en\"))\n\n[1] 175\n\n# first 5 stopwords of of English Snowball stopword list\nhead(quanteda::stopwords(\"en\"), 5)\n\n[1] \"i\"      \"me\"     \"my\"     \"myself\" \"we\"    \n\n# default German Snowball stopword list\nlength(quanteda::stopwords(\"de\"))\n\n[1] 231\n\n# first 5 stopwords of German Snowball stopword list\nhead(quanteda::stopwords(\"de\"), 5)\n\n[1] \"aber\"  \"alle\"  \"allem\" \"allen\" \"aller\"\n\n\nBecause quanteda’s stopwords() function is merely a re-export from the same function in the stand-alone stopwords package, we can access the additional stopwords lists defined in that package.\n\n# check the first ten stopwords from an expanded English stopword list\n# (note that list includes numbers)\nhead(stopwords(\"en\", source = \"stopwords-iso\"), 10)\n\n [1] \"'ll\"       \"'tis\"      \"'twas\"     \"'ve\"       \"10\"        \"39\"       \n [7] \"a\"         \"a's\"       \"able\"      \"ableabout\"\n\n\nFinally, you can create your own list of stopwords adding stopwords in a character vector. The short my_stopwords list below is for illustration purposes only since many custom lists will be considerably longer.\n\nmy_stopwords &lt;- c(\"a\", \"an\", \"the\")\n\nIn the next step, we apply various stopword lists to our tokens object using tokens_select(x, selection = \"remove\") and the wrapper function tokens_remove().\n\n# remove English stopwords and inspect output\ntokens(txt) |&gt;\n    tokens_select(\n        pattern = quanteda::stopwords(\"en\"),\n        selection = \"remove\"\n    )\n\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"tokens\"   \"(\"        \")\"        \"works\"   \n[8] \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"#textanalysis\"        \"https://quanteda.org\"\n\n# the following code is equivalent\ntokens(txt) |&gt;\n    tokens_remove(pattern = quanteda::stopwords(\"en\"))\n\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"tokens\"   \"(\"        \")\"        \"works\"   \n[8] \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"#textanalysis\"        \"https://quanteda.org\"\n\n# remove patterns that match the custom stopword list\ntokens(txt) |&gt;\n    tokens_remove(pattern = my_stopwords)\n\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"sentence\" \",\"        \"showing\"  \"how\"      \"tokens\"   \"(\"        \")\"       \n[8] \"works\"    \".\"       \n\ndoc2 :\n[1] \"@quantedainit\"        \"and\"                  \"#textanalysis\"       \n[4] \"https://quanteda.org\"",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#pattern-matching",
    "href": "10-quanteda-tokens.html#pattern-matching",
    "title": "10  Creating and Managing Tokens",
    "section": "10.4 Pattern Matching",
    "text": "10.4 Pattern Matching\nPattern matching is central when compounding or selecting tokens. Let’s consider the following example: we might want to keep only “president”, “president’s” and “presidential” in our tokens object. One option is to use fixed pattern matching and only keep the exact matches. We specify the patternand valuetype in the tokens_select() function and determine whether to treat patterns case-sensitive or case-insensitive.\nLet’s go through this trio systematically. The pattern can be one ore more unigrams or multi-word sequences. When including multi-word sequences, make sure to use the phrase() function as described above. case_insensitive specifies whether or not to ignore the case of terms when matching a pattern. The valuetype can take one of three arguments: \"glob\" for “glob”-style wildcard expressions; \"regex\" for regular expressions; or \"fixed\" for exact matching.\nWe start explaining fixed pattern matching and the the behaviour of case_insensitive before moving to “glob”-style pattern matching and matching based on regular expressions. We refer readers to Chapter 4 and Appendix C for details about regular expressions.\n\n# create tokens object\ntoks_president &lt;- tokens(\"The President attended the presidential gala\n                         where the president's policies were applauded.\")\n\n# fixed (literal) pattern matching\ntokens_keep(toks_president, pattern = c(\"president\", \"presidential\",\n                                        \"president's\"),\n            valuetype = \"fixed\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"President\"    \"presidential\" \"president's\" \n\n\nThe default pattern match is case_insentitive = TRUE. Therefore, President remains part of the tokens object even though the pattern includes president in lower-case. We could change this behaviour by setting tokens_keep(x, case_insensitive = FALSE).\n\n# fixed (literal) pattern matching: case-sensitive\ntokens_keep(toks_president, pattern = c(\"president\", \"presidential\",\n                                        \"president's\"),\n            valuetype = \"fixed\",\n            case_insensitive = FALSE)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"presidential\" \"president's\" \n\n\nNow only presidential and president's are kept in the tokens object while the term President is not capture since it does not match the term “president” when selecting tokens in a case-sensitive way.\n\n\n\n\n\n\n* and ?: two “glob”-style matches to rule them all\n\n\n\nPattern matching in quanteda defaults to “glob”-style because it’s simpler than regular expression matching and suffices for the majority of user requirements. Moreover, it aligns with fixed pattern matching when wildcard characters (* and ?) aren’t utilised. The implementation in quanteda uses * to match any number of any characters including none, and ? to match any single character.\n\n\nLet’s take a look at a few examples to explain the behaviour of “glob”-style pattern matching.\n\n# match the token \"president\" and all terms starting with \"president\"\ntokens_keep(toks_president, pattern = \"president*\",\n            valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"President\"    \"presidential\" \"president's\" \n\n# match tokens ending on \"ing*\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"*ing\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"buying\"  \"paying\"  \"playing\" \"laying\" \n\n# match tokens starting with \"p\" and ending on \"ing\"\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"p*ing\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"paying\"  \"playing\"\n\n# match tokens starting with a character followed by \"ay\"\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"?ay\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"pay\" \"lay\"\n\n# match tokens starting with a character, followed \"ay\" and none or more characters\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"?ay*\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"paying\" \"pay\"    \"laying\" \"lay\"   \n\n\nIf you want to have more control over pattern matches, we recommend regular expressions (valuetype = \"regex\"), which we explain in more detail in Appendix C.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#stemming",
    "href": "10-quanteda-tokens.html#stemming",
    "title": "10  Creating and Managing Tokens",
    "section": "10.5 Stemming",
    "text": "10.5 Stemming\nThe quanteda packages includes the function tokens_wordstem(), a wrapper around wordStem() from the SnowballC package. The function uses Martin Porter’s (Porter 2001) algorithm described above. The example below shows how tokens_wordstem() adjust various words.\n\n# example applied to tokens\ntxt &lt;- c(\n    one = \"eating eater eaters eats ate\",\n    two = \"taxing taxis taxes taxed my tax return\"\n)\n\n# create tokens object\ntokens(txt)\n\nTokens consisting of 2 documents.\none :\n[1] \"eating\" \"eater\"  \"eaters\" \"eats\"   \"ate\"   \n\ntwo :\n[1] \"taxing\" \"taxis\"  \"taxes\"  \"taxed\"  \"my\"     \"tax\"    \"return\"\n\n# create tokens object and stem tokens\ntxt |&gt;\n    tokens() |&gt;\n    tokens_wordstem()\n\nTokens consisting of 2 documents.\none :\n[1] \"eat\"   \"eater\" \"eater\" \"eat\"   \"ate\"  \n\ntwo :\n[1] \"tax\"    \"taxi\"   \"tax\"    \"tax\"    \"my\"     \"tax\"    \"return\"\n\n\nLemmatisation is more complex than stemming since it does not rely on pre-defined rules. The spacyr package allows you to lemmatise a text corpus. We describe lemmatisation in the Advanced section below.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#advanced",
    "href": "10-quanteda-tokens.html#advanced",
    "title": "10  Creating and Managing Tokens",
    "section": "10.6 Advanced",
    "text": "10.6 Advanced\n\n10.6.1 Applying Different Tokenisers\nquanteda contains several tokenisers, which can be applied in tokens(). Moreover, you can apply tokenisers included in other packages.\nThe current default tokeniser is word3 included in quanteda version 3 and above. For forward compatibility including use of a more advanced tokeniser that will be used in major version 4, there is also a word4 tokeniser that is even smarter than the defaults. You can apply the different tokenisers by specifying the word argument in tokens().\nThe tokenizers package includes additional tokenisers (Mullen et al. 2018). These tokenisers can also be applied and transformed to a quanteda tokens object.\n\n# load the tokenizers package\nlibrary(tokenizers)\n\n# tokenisation without processing\ntokenizers::tokenize_words(txt) %&gt;%\n    tokens()\n\nTokens consisting of 2 documents.\none :\n[1] \"eating\" \"eater\"  \"eaters\" \"eats\"   \"ate\"   \n\ntwo :\n[1] \"taxing\" \"taxis\"  \"taxes\"  \"taxed\"  \"my\"     \"tax\"    \"return\"\n\n# tokenisation with processing in both functions\ntokenizers::tokenize_words(txt, lowercase = FALSE, strip_punct = FALSE) %&gt;%\n    tokens(remove_symbols = TRUE)\n\nTokens consisting of 2 documents.\none :\n[1] \"eating\" \"eater\"  \"eaters\" \"eats\"   \"ate\"   \n\ntwo :\n[1] \"taxing\" \"taxis\"  \"taxes\"  \"taxed\"  \"my\"     \"tax\"    \"return\"\n\n\n\n\n10.6.2 Lemmatisation\nWhile stemming works directly in quanteda using tokens_wordstem(), lemmatisation, i.e., changing tokens to its base form, requires different packages. You can use the spacyr package, a wrapper around the spaCy Python library, to lemmatise a quanteda tokens object. Note that you will need to install Python and a virtual environment to use the spaCy package.1\n\n# load spacyr package\nlibrary(\"spacyr\")\n\n# use spacy_install() to install spacy in a new or existing\n# virtual environment. Check ?spacy_install() for details\n\n# initialise and use English language model\nspacy_initialize(model = \"en_core_web_sm\")\n\nsuccessfully initialized (spaCy Version: 3.7.2, language model: en_core_web_sm)\n\ntxt_compare &lt;- c(\n    one = \"The cats are running quickly.\",\n    two = \"The geese were flying overhead.\"\n)\n\n# parse texts, return, part-of-speech and lemma\ntoks_spacy &lt;- spacy_parse(txt_compare, pos = TRUE, lemma = TRUE)\n\n# show first 10 tokens, which are stored as a data frame\nhead(toks_spacy, 10)\n\n   doc_id sentence_id token_id   token   lemma   pos entity\n1     one           1        1     The     the   DET       \n2     one           1        2    cats     cat  NOUN       \n3     one           1        3     are      be   AUX       \n4     one           1        4 running     run  VERB       \n5     one           1        5 quickly quickly   ADV       \n6     one           1        6       .       . PUNCT       \n7     two           1        1     The     the   DET       \n8     two           1        2   geese   geese  NOUN NORP_B\n9     two           1        3    were      be   AUX       \n10    two           1        4  flying     fly  VERB       \n\n# transform object to a quanteda tokens object and use lemma\nas.tokens(toks_spacy, use_lemma = TRUE)\n\nTokens consisting of 2 documents.\none :\n[1] \"the\"     \"cat\"     \"be\"      \"run\"     \"quickly\" \".\"      \n\ntwo :\n[1] \"the\"      \"geese\"    \"be\"       \"fly\"      \"overhead\" \".\"       \n\n# compare with Snowball stemmer\ntxt_compare |&gt;\n    tokens() |&gt;\n    tokens_wordstem()\n\nTokens consisting of 2 documents.\none :\n[1] \"The\"   \"cat\"   \"are\"   \"run\"   \"quick\" \".\"    \n\ntwo :\n[1] \"The\"      \"gees\"     \"were\"     \"fli\"      \"overhead\" \".\"       \n\n# finalise spaCy and terminate Python process to free up memory\nspacy_finalize()\n\nThe code above highlights the differences between stemming and lemmatisation. Stemming can truncate words, resulting in non-real words. Lemmatisation reduces words to their canonical, valid form. The word flying is stemmed to fli, while the lemmatiser changes the word to its base form, fly.\n\n\n10.6.3 Modifying Stopword Lists\nIn many cases, you might want to use an existing stopword list but remove or add certain features. You can use quanteda’s char_remove() and base R’s c() function to remove or add features. The examples below show how to remove features from the default English stopword list.\n\n# check if \"will\" is included in default stopword list\n\"will\" %in% stopwords(\"en\")\n\n[1] TRUE\n\n# remove \"will\" and store output as new stopword list\nstopw_reduced &lt;- char_remove(stopwords(\"en\"), pattern = \"will\")\n\n# check whether \"will\" was removed\n\"will\" %in% stopw_reduced\n\n[1] FALSE\n\n\nWe use c() from base R to add words to stopword lists. For example, the feature further is included in the default English stopword list, but furthermore and therefore are not included. Let’s add both terms.\n\n# check if terms are included in stopword list\nc(\"furthermore\", \"therefore\") %in% stopwords(\"en\")\n\n[1] FALSE FALSE\n\n# extend stopword list\nstop_extended &lt;- c(stopwords(\"en\"), \"furthermore\", \"therefore\")\n\n# check the last parts of the character vector\ntail(stop_extended)\n\n[1] \"than\"        \"too\"         \"very\"        \"will\"        \"furthermore\"\n[6] \"therefore\"  \n\n\nAs discussed above, tokenisation and processing involves many steps, and we can combine these steps using the base R pipe (|&gt;). The example below shows a typical workflow.\n\n# tokenise data_corpus_inaugural,\n# remove punctuation and numbers,\n# remove stopwords,\n# stem the tokens,\n# and transform object to lowercase\n\ntoks_inaugural &lt;- data_corpus_inaugural |&gt;\n    tokens(remove_punct = TRUE, remove_numbers = TRUE) |&gt;\n    tokens_remove(pattern = stopwords(\"en\")) |&gt;\n    tokens_wordstem() |&gt;\n    tokens_tolower()\n\n# inspect first tokens from the first two speeches\nhead(toks_inaugural, 2)\n\nTokens consisting of 2 documents and 4 docvars.\n1789-Washington :\n [1] \"fellow-citizen\" \"senat\"          \"hous\"           \"repres\"        \n [5] \"among\"          \"vicissitud\"     \"incid\"          \"life\"          \n [9] \"event\"          \"fill\"           \"greater\"        \"anxieti\"       \n[ ... and 640 more ]\n\n1793-Washington :\n [1] \"fellow\"   \"citizen\"  \"call\"     \"upon\"     \"voic\"     \"countri\" \n [7] \"execut\"   \"function\" \"chief\"    \"magistr\"  \"occas\"    \"proper\"  \n[ ... and 50 more ]\n\n\n\n\n\n\n\n\nThe sequence of processing steps during the tokensation is important. For example, if we first stem our tokens and remove stopwords or specific patterns afterwards, we might not remove all desired features. Consider the following example:\n\ntxt &lt;- \"During my stay in London I visited the museum\nand attended a very good concert.\"\n\n# remove stopwords before stemming tokens\ntokens(txt, remove_punct = TRUE) %&gt;%\n    tokens_remove(stopwords(\"en\")) %&gt;%\n    tokens_wordstem()\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"stay\"    \"London\"  \"visit\"   \"museum\"  \"attend\"  \"good\"    \"concert\"\n\n# stem tokens before removing stopwords\ntokens(txt, remove_punct = TRUE) %&gt;%\n    tokens_wordstem() %&gt;%\n    tokens_remove(stopwords(\"en\"))\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"Dure\"    \"stay\"    \"London\"  \"visit\"   \"museum\"  \"attend\"  \"veri\"   \n[8] \"good\"    \"concert\"\n\n\nThe first example produces what most users want: it removes all terms from our stopword list (during, my, I, the, and, a, very), while the second example first stems During to dure and very to veri, which changes the terms to tokens that are not included in stopwords(\"en\") (and therefore remain in the tokens object).\n\n\n\n\n\n10.6.4 Managing Document-Level Variables and Metadata\nBy default, tokens object contain the document-level variables and the metadata assigned to your corpus. You can access or modify these variables in the same way as we did in Chapter 9.\n\n# tokenise US inaugural speeches\ntoks_inaugural &lt;- tokens(data_corpus_inaugural)\n\n# add document level variable\ntoks_inaugural$post_1990 &lt;- ifelse(\n    toks_inaugural$Year &gt; 1990, \"Post-1990\", \"Pre-1990\"\n)\n\n# inspect new document-level variable\ntable(toks_inaugural$post_1990)\n\n\nPost-1990  Pre-1990 \n        8        51",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#further-reading",
    "href": "10-quanteda-tokens.html#further-reading",
    "title": "10  Creating and Managing Tokens",
    "section": "10.7 Further Reading",
    "text": "10.7 Further Reading\n\nThe concept of tokenisation and how to build a custom tokeniser: Hvitfeldt and Silge (2021, ch. 2)\nThe intuition behind processing and tokenising texts: Grimmer, Roberts, and Stewart (2022, ch. 5.3)\nIntroduction to the tokenizers package: Mullen et al. (2018)\nHow processing decisions can influence results: Denny and Spirling (2018)",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#exercises",
    "href": "10-quanteda-tokens.html#exercises",
    "title": "10  Creating and Managing Tokens",
    "section": "10.8 Exercises",
    "text": "10.8 Exercises\nAdd some here.\n\n\n\n\nDenny, Matthew W., and Arthur Spirling. 2018. “Text Preprocessing for Unsupervised Learning: Why It Matters, When It Misleads, and What to Do about It.” Political Analysis 26 (2): 168–89. https://doi.org/10.1017/pan.2017.44.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts: A New Framework for Machine Learning and the Social Sciences. Princeton: Princeton University Press.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in r. Boca Raton: CRC Press. https://smltar.com.\n\n\nMonroe, Burt L., and Philip A. Schrodt. 2008. “Introduction to the Special Issue: The Statistical Analysis of Political Text.” Political Analysis 16 (4): 351–55. https://doi.org/10.1093/pan/mpn017.\n\n\nMullen, Lincoln A., Kenneth Benoit, Os Keyes, Dmitry Selivanov, and Jeffrey Arnold. 2018. “Fast, Consistent Tokenization of Natural Language Text.” Journal of Open Source Software 3: 655. https://doi.org/10.21105/joss.00655.\n\n\nMüller, Stefan. 2022. “The Temporal Focus of Campaign Communication.” The Journal of Politics 84 (1): 585–90. https://doi.org/10.1086/715165.\n\n\nPorter, Martin F. 2001. “Snowball: A Language for Stemming Algorithms.” In.\n\n\nSarica, Serhad, and Jianxi Luo. 2021. “Stopwords in Technical Language Processing.” PLoS One 16 (8): e0254937. https://doi.org/10.1371/journal.pone.0254937.\n\n\nSchofield, Alexandra, and David Mimno. 2016. “Comparing Apples to Apple: The Effects of Stemmers on Topic Models.” Transactions of the Association for Computational Linguistics 4: 287–300. https://doi.org/10.1162/tacl_a_00099.\n\n\nWilbur, W. John, and Karl Sirotkin. 1992. “The Automatic Identification of Stop Words.” Journal of Information Science 18 (1): 45–55. https://doi.org/10.1177/01655515920180010.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "10-quanteda-tokens.html#footnotes",
    "href": "10-quanteda-tokens.html#footnotes",
    "title": "10  Creating and Managing Tokens",
    "section": "",
    "text": "Detailed instructions are provided at http://spacyr.quanteda.io and Appendix A.↩︎",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Creating and Managing Tokens</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html",
    "href": "11-quanteda-tokensadvanced.html",
    "title": "11  Advanced Token Manipulation",
    "section": "",
    "text": "11.1 Objectives\nThe previous chapter introduced you to the basics of tokenisation and processing of tokens objects. Now we move to advanced token manipulations. We show how to replace tokens and introduce n-grams and skip-grams. We explain why we often want to compound multi-word expressions into a single token. We also outline why you might want to keep specific tokens and a context window around these tokens, and close the chapter with a brief introduction to lookup functions, which we cover much more extensively in Chapter 16.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#methods",
    "href": "11-quanteda-tokensadvanced.html#methods",
    "title": "11  Advanced Token Manipulation",
    "section": "11.2 Methods",
    "text": "11.2 Methods\nOccasionally, you might want to replace certain tokens. For example, in political texts, Labour spelled usually implies a mention to the named-entity Labour Party, while labour refers to the noun. Country names is another example: if we want to understand mentions of the United States of America in UN General Debates, we could replace US, USA, and United States with united_states_of_america. Dictionaries, discussed in Chapter 16, are an effective tool for replacing tokens with an overarching “key”, such as united_states.\nIn some applications, tokens sequences might reveal more information than individual tokens. Before transforming a tokens object to a dfm, many existing studies create n-grams or skipgrams. N-grams are sequences of “n” items. Skip-grams are variations of n-grams which pairs non-consecutive tokens. N-grams capture local word patterns, whereas skip-grams capture broader contexts within texts. The sentence I loved my stay in New York. would result in the following bi-grams (sequence of two tokens): \"I_loved\"  \"loved_my\" \"my_stay\"  \"stay_in\"  \"in_New\" \"New_York\" \"York_.\". Skip-grams of size 2 with a distance of 0 and 1 would change the object to: \"I_loved\" \"I_my\" \"loved_my\" \"loved_time\" \"my_time\" \"my_in\"  \"time_in\" \"time_New\" \"in_New\" \"in_York\" \"New_York\" \"New_.\" \"York_.\".\nThese examples highlight advantages and shortcomings of n-grams and skip-grams. On the one hand, both approaches provide information about the context of each token. On the other hand, n-grams and skip-grams increase the number of types (i.e., unique tokens) in our corpus. For example, the number of types in the corpus of US inaugural speeches more than doubles when creating bi-grams rather than uni-grams and triples when creating tri-grams instead of uni-grams. Instead of creating bi-grams or tri-grams, manual or automated identification of meaningful multi-word expressions is often sufficient or even preferred over n-grams.\nSo far, we treated all tokens as so-called unigrams. We separated tokens by spaces and did not combine two or more tokens that might form a multi-word expression. In languages with compound words, we do not need to pay much attention to multi-word expresions. For example, the German term “Mehrwertsteuer” is a single word in German, but its English equivalent consists of three words: “value added tax”. Suppose we are interested in companies’ or politicians’ focus on different forms of taxation. In that case, we want to treat value added tax as a multi-word expression rather than three separate tokens value, added, tax. Identifying multi-word expressions is especially important for document-feature matrices (dfm) (Chapter 12), which contain the counts of features in each document. If we do not explicitly compound value added tax, the words will be included as three separate tokens in our dfm. Compounding the expression during the tokenisation process, will ensure that the dfm contains the compound noun value_added_tax.\nIn many cases, we know multi-word expressions through our domain knowledge. For example, in reviews about hotels in New York, we might want to compound New_York, Madison_Square_Garden, and Wall_Street. In parliamentary speeches, we want to compound party names: instead of treating the combination green party as separate tokens, we might prefer the multi-word expression green party before proceeding with our statistical analysis.\nUsers need to discover relevant multi-word expressions. We can use approaches such as keywords-in-context (Chapter 15) to explore the context of specific words or conduct a collocation analysis to identify terms that tend to co-occur together automatically. We introduce these methods in add reference to new chapter. Having identified multi-word expressions, you can compound these collocations before continuing your textual analysis.\nKeeping tokens and their context windows is another effective—and sometimes underused—tokenisation operation. We can keep specific tokens and the words around these patterns to refine our research question and focus on specific aspects of our text corpus. Let’s imagine the following example: we are working with all speeches delivered in parliament over a period of three decades and want to understand how parties’ focus and positions about climate change have evolved. Most speeches in our corpus will focus on different policies or contain procedural language, but we could create a list of words and phrases relating to the environment, and keep these terms with a context of several words. This approach would allow for a “targeted” analysis. Instead of analysing the full text corpus, we narrowed down our documents to the parts relevant to our research question. For example, Lupia, Soroka, and Beatty (2020) limit U.S Congressional speeches to sentences mentioning the National Science Foundation (NSF). Afterwards, the authors identify which of these context words distinguish Democrats from Republicans, and how the topics (Chapter 24) mentioned in these sentences are moderated by the party of a speaker. Rauh, Bes, and Schoonvelde (2020) extract mentions of European institutions and a context window of three sentences from speeches delivered by European prime ministers. In the next step, the authors measure speech complexity and sentiment in these statements on European institutions. Their results reveals that prime minister tend to speak more favourably about the European Union when they face a strong Eurosceptic challenger party.\nFinally, we briefly introduce the concept of looking up tokens. We match tokens against a predefined list. This approach requires users to develop “dictionaries”, consisting of one or more “keys” or categories. Each of these keys, in turn, contains various patterns, usually words or multi-word expressions. A sentiment analysis, covered in Chapter 16, often relies on lists of terms and phrases scored as “positive” and “negative” and involves looking up these tokens.\nClassifying topics, policy areas, or concepts can also be conducted with a “lookup approach.” For example, Gessler and Hunger (2022) create a dictionary of keywords and phrases related to immigration. Afterwards, the authors apply this dictionary to party press releases. The authors keep all documents containing keywords from their immigration dictionary. Their rule-based approach is more computationally efficient than supervised classification and produced valid results. Subsequent analyses apply scaling methods to this subset of immigration-related press releases to understand how the 2015 “refugee crisis” in Europe changed party positions on migration policy. Chapter 16 provides more details on creating and applying dictionaries.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#examples",
    "href": "11-quanteda-tokensadvanced.html#examples",
    "title": "11  Advanced Token Manipulation",
    "section": "11.3 Examples",
    "text": "11.3 Examples\nIn this section, we rely on short sentences and text corpora of political speeches and hotel reviews to explain how to replace tokens, how to create n-grams and skip-grams, how to compound multi-word expressions, and how to select tokens and their context.\n\n11.3.1 Replacing and Looking Up Tokens\nIn some cases, users may want to substitute tokens. Reasons to replace tokens include standardising terms, accounting for synonyms, acronyms, or fixing typographical errors. For example, it may be reasonable to harmonise “EU” and “European Union” in political texts. The function tokens_replace() allows us to conduct one-to-one matching and replace EU with European Union.\n\ntoks_eu_uk &lt;- tokens(\"The European Union negotiated with the UK.\")\n\n# important: use phrase if you want to detect a multi-word expression\ntokens_replace(toks_eu_uk, \n               pattern = phrase(\"European Union\"),\n               replacement = \"EU\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"        \"EU\"         \"negotiated\" \"with\"       \"the\"       \n[6] \"UK\"         \".\"         \n\n# we can also replace \"UK\" with a multi-word expression \"United Kingdom\"\ntokens_replace(toks_eu_uk, \n               pattern = \"UK\", \n               replacement = phrase(\"United Kingdom\"))\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"        \"European\"   \"Union\"      \"negotiated\" \"with\"      \n[6] \"the\"        \"United\"     \"Kingdom\"    \".\"         \n\n# if we want to treat United Kingdom and European Union \n# as multi-word expressions across all texts,\n# we can compound it after the replacement\ntoks_eu_uk |&gt; \n    tokens_replace(pattern = \"UK\", replacement = phrase(\"United Kingdom\")) |&gt; \n    tokens_compound(pattern = phrase(c(\"United Kingdom\",\n                                       \"European Union\")))\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"The\"            \"European_Union\" \"negotiated\"     \"with\"          \n[5] \"the\"            \"United_Kingdom\" \".\"             \n\n\n\n\n\n\n\n\nHow to declare multi-word expressions\n\n\n\nWe need to declare explicitly when we work with multi-word expressions. The phrase() function declares a pattern to be a sequence of separate patterns. By using phrase() you make explicit that the elements should be used for matching multi-word sequences rather than individual matches to single words. It is vital to use phrase() in all functions involving multi-word expressions, including tokens_compound().1\n\n# make phrases from characters\nphrase(c(\"natural language processing\"))\n\n[[1]]\n[1] \"natural\"    \"language\"   \"processing\"\n\n# show that replacements of multi-word expressions\n# require phrase()\ntokens(\"quantitative text analysis with quanteda\") |&gt; \n    tokens_replace(pattern = phrase(c(\"quantitative text analysis\")),\n                   replacement = \"QTA\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"QTA\"      \"with\"     \"quanteda\"\n\n# replacement does not work without phrase()\ntokens(\"quantitative text analysis with quanteda\") |&gt; \n    tokens_replace(pattern = \"quantitative text analysis\",\n                   replacement = \"QTA\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"quantitative\" \"text\"         \"analysis\"     \"with\"         \"quanteda\"    \n\n\n\n\nMore common than one-to-one replacements is the conversation of tokens into equivalence classes defined by values of a dictionary object. Dictionaries, covered in much greater detail in chapter Chapter 16, allow us to look up uni-grams or multi-word expressions and replace these terms with the dictionary “key”. We introduce the intuition behind tokens_lookup() with a simple example. The example below replaces selected European institutions with its dictionary key eu_institution.\n\n# create a dictionary (covered more extensively in the next chapter)\ndict_euinst &lt;- dictionary(\n    list(eu_institution = c(\"european commission\", \"ecb\")))\n\n# tokenise a sentence \ntoks_eu &lt;- tokens(\"The European Commission is based in Brussels \n                  and the ECB in Frankfurt.\")\n\n# look up institutions (default behaviour)\ntokens_lookup(toks_eu, dictionary = dict_euinst)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"eu_institution\" \"eu_institution\"\n\n# show unmatched tokens\ntokens_lookup(toks_eu, dictionary = dict_euinst,\n              nomatch = \"_UNMATCHED\")\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"_UNMATCHED\"     \"eu_institution\" \"_UNMATCHED\"     \"_UNMATCHED\"    \n [5] \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"    \n [9] \"eu_institution\" \"_UNMATCHED\"     \"_UNMATCHED\"     \"_UNMATCHED\"    \n\n\nBy default, unmatched tokens are omitted, but we can assign a custom term to unmatched tokens. What is more, we can use tokens_lookup() as a more sophisticated form of tokens_replace(): setting exclusive = FALSE in tokens_lookup() replaces dictionary matches but leaves the other features unaffected.\n\n# replace dictionary matches and keep other features\ntokens_lookup(toks_eu, \n              dictionary = dict_euinst,\n              exclusive = FALSE)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"The\"            \"EU_INSTITUTION\" \"is\"             \"based\"         \n [5] \"in\"             \"Brussels\"       \"and\"            \"the\"           \n [9] \"EU_INSTITUTION\" \"in\"             \"Frankfurt\"      \".\"             \n\n\n\n\n11.3.2 Pattern Matching: pattern, valuetype, and case_insensitive\nPattern matching is central when compounding or selecting tokens. Let’s consider the following example: we might want to keep only “president”, “president’s” and “presidential” in our tokens object. One option is to use fixed pattern matching and only keep the exact matches. We specify the patternand valuetype in the tokens_select() function and determine whether to treat patterns case-sensitive or case-insensitive.\nLet’s go through this trio systematically. The pattern can be one ore more unigrams or multi-word sequences. When including multi-word sequences, make sure to use the phrase() function as described above. case_insensitive specifies whether or not to ignore the case of terms when matching a pattern. The valuetype can take one of three arguments: \"glob\" for “glob”-style wildcard expressions; \"regex\" for regular expressions; or \"fixed\" for exact matching.\nWe start explaining fixed pattern matching and the the behaviour of case_insensitive before moving to “glob”-style pattern matching and matching based on regular expressions. We refer readers to Chapter 4 and Appendix C for details about regular expressions.\n\n# create tokens object\ntoks_president &lt;- tokens(\"The President attended the presidential gala\n                         where the president's policies were applauded.\")\n\n# fixed (literal) pattern matching\ntokens_keep(toks_president, pattern = c(\"president\", \"presidential\",\n                                        \"president's\"),\n            valuetype = \"fixed\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"President\"    \"presidential\" \"president's\" \n\n\nThe default pattern match is case_insentitive = TRUE. Therefore, President remains part of the tokens object even though the pattern includes president in lower-case. We could change this behaviour by setting tokens_keep(x, case_insensitive = FALSE).\n\n# fixed (literal) pattern matching: case-sensitive\ntokens_keep(toks_president, pattern = c(\"president\", \"presidential\",\n                                        \"president's\"),\n            valuetype = \"fixed\",\n            case_insensitive = FALSE)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"presidential\" \"president's\" \n\n\nNow only presidential and president's are kept in the tokens object while the term President is not capture since it does not match the term “president” when selecting tokens in a case-sensitive way.\n\n\n\n\n\n\n* and ?: two “glob”-style matches to rule them all\n\n\n\nPattern matching in quanteda defaults to “glob”-style because it’s simpler than regular expression matching and suffices for the majority of user requirements. Moreover, it aligns with fixed pattern matching when wildcard characters (* and ?) aren’t utilised. The implementation in quanteda uses * to match any number of any characters including none, and ? to match any single character. Let’s take a look at a few examples to explain the behaviour of wildcard pattern matches.\n\n# match the token \"president\" and all terms starting with \"president\"\ntokens_keep(toks_president, pattern = \"president*\",\n            valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"President\"    \"presidential\" \"president's\" \n\n# match tokens ending on \"ing*\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"*ing\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"buying\"  \"paying\"  \"playing\" \"laying\" \n\n# match tokens starting with \"p\" and ending on \"ing\"\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"p*ing\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"paying\"  \"playing\"\n\n# match tokens starting with a character followed by \"ay\"\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"?ay\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"pay\" \"lay\"\n\n# match tokens starting with a character, followed \"ay\" and none or more characters\ntokens(\"buying buy paying pay playing laying lay\") |&gt; \n    tokens_keep(pattern = \"?ay*\", valuetype = \"glob\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"paying\" \"pay\"    \"laying\" \"lay\"   \n\n\nIf you want to have more control over pattern matches, we recommend regular expressions (valuetype = \"regex\").\n\n\n\n\n11.3.3 N-Grams and Skip-Grams\nYou can create n-grams and skip-grams in various lengths using tokens_ngrams() and tokens_skipgrams(). While using these functions is fairly straightforward, users need to make decisions about removing patterns before concatenating tokens and need to determine the size of n-grams and/or skips. We describe these options below. First, we create n-grams and skip-grams of various sizes. Then, we combine skip-grams and n-grams in the same function, and finally show how the output changes if we process a tokens object before constructing n-grams.\n\n# tokenise a sentence\ntoks_social &lt;- tokens(\"We should consider increasing social welfare payments.\")\n\n# form n-grams of size 2\ntokens_ngrams(toks_social, n = 2)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_should\"           \"should_consider\"     \"consider_increasing\"\n[4] \"increasing_social\"   \"social_welfare\"      \"welfare_payments\"   \n[7] \"payments_.\"         \n\n# form n-grams of size 3\ntokens_ngrams(toks_social, n = 3)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_should_consider\"         \"should_consider_increasing\"\n[3] \"consider_increasing_social\" \"increasing_social_welfare\" \n[5] \"social_welfare_payments\"    \"welfare_payments_.\"        \n\n# form n-grams of size 2 and 3\ntokens_ngrams(toks_social, n = 2:3)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"We_should\"                  \"should_consider\"           \n [3] \"consider_increasing\"        \"increasing_social\"         \n [5] \"social_welfare\"             \"welfare_payments\"          \n [7] \"payments_.\"                 \"We_should_consider\"        \n [9] \"should_consider_increasing\" \"consider_increasing_social\"\n[11] \"increasing_social_welfare\"  \"social_welfare_payments\"   \n[ ... and 1 more ]\n\n# form skip-grams of size 2 and skip 1 token\ntokens_skipgrams(toks_social, n = 2, skip = 1)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_consider\"        \"should_increasing\"  \"consider_social\"   \n[4] \"increasing_welfare\" \"social_payments\"    \"welfare_.\"         \n\n# form skip-grams of size 2 and skip 1 and 2 tokens\ntokens_skipgrams(toks_social, n = 2, skip = 1:2)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"We_consider\"         \"We_increasing\"       \"should_increasing\"  \n [4] \"should_social\"       \"consider_social\"     \"consider_welfare\"   \n [7] \"increasing_welfare\"  \"increasing_payments\" \"social_payments\"    \n[10] \"social_.\"            \"welfare_.\"          \n\n# form n-grams of size 1 and skip-grams with a skip of 1 token\ntokens_ngrams(toks_social, n = 2, skip = 1)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"We_consider\"        \"should_increasing\"  \"consider_social\"   \n[4] \"increasing_welfare\" \"social_payments\"    \"welfare_.\"         \n\n# remove stopwords and punctuation before creating n-grams\ntoks_social |&gt; \n    tokens(remove_punct = TRUE) |&gt; \n    tokens_remove(pattern = stopwords(\"en\")) |&gt; \n    tokens_ngrams(n = 2)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"consider_increasing\" \"increasing_social\"   \"social_welfare\"     \n[4] \"welfare_payments\"   \n\n\nThe example above underscore that several combinations do not add much value to the context in which words appear. Many types are simply combinations of tokens and stopwords. From our experience, creating skip-grams or n-grams for all documents without any processing decisions in advance does not improve our analysis or results.\nIt is worth keeping in mind that n-grams applied to larger corpora inflate the number of types. We showcase the increase in tokens based on our corpus of 59 US inaugural speeches.\n\n# number of types with uni-grams and no processing\ndata_corpus_inaugural |&gt; \n    tokens() |&gt; \n    ntype() |&gt; \n    sum()\n\n[1] 47494\n\n# number of types with n-grams of size 2 \n# after removing stopwords and punctuation characters\ndata_corpus_inaugural |&gt; \n    tokens(remove_punct = TRUE) |&gt; \n    tokens_remove(pattern = stopwords(\"en\")) |&gt; \n    tokens_ngrams(n = 2) |&gt; \n    ntype() |&gt; \n    sum()\n\n[1] 63971\n\n# number of types with n-grams of size 2 and no processing\ndata_corpus_inaugural |&gt; \n    tokens() |&gt; \n    tokens_ngrams(n = 2) |&gt; \n    ntype() |&gt; \n    sum()\n\n[1] 118939\n\n# number of types with n-grams of size 3 and no processing\ndata_corpus_inaugural |&gt; \n    tokens() |&gt; \n    tokens_ngrams(n = 3) |&gt; \n    ntype() |&gt; \n    sum()\n\n[1] 144825\n\n\n\n\n\n\n\n\nWhen to pay attention to very sparse objects\n\n\n\nAn increase in types through n-grams increases the sparsity of a document-feature matrix, i.e., the proportion of cells that have zero counts. (Chapter 12). The sparsity of US inaugural debates (data_corpus_inaugural) increases from 92% to 96.9% when using bi-grams instead of uni-grams. While quanteda handles sparse document-feature matrices very efficiently, a very high sparsity might result in convergence issues for unsupervised scaling models (Chapter 22) or topic models (Chapter 24). Therefore, n-grams or skip-grams may be counterproductive for some research questions.\n\n\n\n\n11.3.4 Compounding Tokens\nBefore transforming a tokens object into a document-feature matrix (Chapter 12), we often want or need to compound multi-word expressions. Compounded phrases will be treated as a single feature in subsequent analyses. Let’s explore how to compound the multi-word expressions “social welfare” and “social security.” As mentioned above, we need to explicitly declare multi-word expressions with the pattern() function.\n\n# create tokens object for examples\ntoks_social &lt;- tokens(\"We need to increase social welfare payments \n                      and improve social security.\")\n\n# compound the pattern \"social welfare\"\ntoks_social |&gt; \n    tokens_compound(pattern = phrase(\"social welfare\"))\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"             \"need\"           \"to\"             \"increase\"      \n [5] \"social_welfare\" \"payments\"       \"and\"            \"improve\"       \n [9] \"social\"         \"security\"       \".\"             \n\n# note: compounding does not work without phrase()\ntoks_social |&gt; \n    tokens_compound(pattern = \"social welfare\")\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"       \"need\"     \"to\"       \"increase\" \"social\"   \"welfare\" \n [7] \"payments\" \"and\"      \"improve\"  \"social\"   \"security\" \".\"       \n\n\nWe can include several patterns in our character vector containing multi-word expressions.\n\n# compound two patterns\ntokens_compound(toks_social,\n                pattern = phrase(c(\"social welfare\",\n                                   \"social security\")))\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"We\"              \"need\"            \"to\"              \"increase\"       \n [5] \"social_welfare\"  \"payments\"        \"and\"             \"improve\"        \n [9] \"social_security\" \".\"              \n\n\n\n\n\n\n\n\nSetting the concatenator\n\n\n\nBy default, compounded tokens are concatenated using an underscore (_). The default is recommended since underscores will not be removed during normal cleaning and tokenisation. Using an underscore as a separator also allows you to check whether compounding worked as expected.\n\n# check whether compounding worked as expected \n# by extracting patterns containing underscores\ntoks_social |&gt; \n    tokens_compound(pattern = phrase(c(\"social welfare\", \n                                       \"social security\"))) |&gt; \n    tokens_keep(pattern = \"*_*\") # keep patterns with underscores\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"social_welfare\"  \"social_security\"\n\n\n\n\nYou can also compound terms based on regular expressions (Chapter 4 and Appendix C) or “wild card” pattern matches. Below, we use the glob-style wildcard expression * to compound all multi-word expressions starting with “social” in US State of the Union speeches.\n\n# tokenise SOTU speeches, remove punctuation and numbers\n# before removing stopwords\ntoks_sotu &lt;- TAUR::data_corpus_sotu |&gt; \n    tokens(remove_punct = TRUE,\n           remove_numbers = TRUE) |&gt; \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE)\n\n# compound all phrases starting with \"social\"\ntoks_sotu_comp &lt;- tokens_compound(toks_sotu, pattern = phrase(\"social *\"))\n\n# spot-check results by keeping all tokens starting \n# with social using \"glob\"-style wildcard pattern match\n# and create dfm to check compounded terms\ntokens_keep(toks_sotu_comp, pattern = \"social_*\") |&gt; \n    dfm() |&gt; \n    topfeatures(n = 15) # get 15 most frequent compounded tokens\n\n    social_security     social_problems     social_services        social_needs \n                229                  16                  16                  13 \n        social_life        social_order     social_progress      social_welfare \n                 10                   9                   9                   8 \n    social_programs       social_system  social_intercourse    social_condition \n                  8                   7                   6                   6 \n      social_change    social_insurance social_institutions \n                  6                   6                   5 \n\n\n\n\n11.3.5 Selecting Tokens within Windows\nIsolating specific tokens within a defined range of words can refine many research questions. For example, we could keep the term room and the context of ±4 tokens in the corpus of hotel reviews. This approach might provide a first descriptive insights into aspects the customers really (dis-)liked about their hotel room.\n\n# tokenize and process the corpus of hotel reviews\ntoks_hotels &lt;- tokens(TAUR::data_corpus_TAhotels,\n                      remove_punct = TRUE,\n                      remove_numbers = TRUE,\n                      padding = TRUE)\n\n# keep \"room*\" and its context of ±3 tokens\ntoks_room &lt;- toks_hotels |&gt; \n    tokens_remove(pattern = stopwords(\"en\"),\n                  padding = TRUE) |&gt; \n    tokens_keep(pattern = \"room*\", \n                window = 4, padding = TRUE)\n\n# inspect the first three hotel reviews\nprint(toks_room, max_ndoc = 4)\n\nTokens consisting of 20,491 documents and 1 docvar.\ntext1 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 86 more ]\n\ntext2 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 260 more ]\n\ntext3 :\n [1] \"nice\"       \"rooms\"      \"\"           \"\"           \"\"          \n [6] \"experience\" \"\"           \"\"           \"\"           \"\"          \n[11] \"\"           \"\"          \n[ ... and 226 more ]\n\ntext4 :\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[ ... and 92 more ]\n\n[ reached max_ndoc ... 20,487 more documents ]\n\n# transform tokens object into a document-feature matrix (dfm) and \n# get 30 most frequent words surrounding \"room*\" using topfeatures()\ntoks_room |&gt; \n    dfm() |&gt;\n    dfm_remove(pattern = \"\") |&gt; # remove padding placeholder\n    topfeatures(n = 30)\n\n       room       rooms       hotel       clean        nice       small \n      34404       12061        5520        4766        2908        2694 \n      great       staff         n't     service       floor        view \n       2632        2474        2414        2343        2181        2153 \n       good comfortable         bed    bathroom       large   breakfast \n       2107        1961        1675        1596        1595        1593 \n      night      stayed    spacious        stay    location         got \n       1426        1415        1333        1314        1273        1267 \n        day        just        size        beds      booked    friendly \n       1216        1180        1160        1050        1002         995 \n\n\n\n\n\n\n\n\nTo pad or not to pad?\n\n\n\nPadding implies leaving an empty string where removed tokens previously existed. Padding can be useful when we want to remove certain patterns, but (1) still know the position of tokens that remain in the corpus or (2) if we select tokens and their context window. The examples below highlight the differences.\n\ntoks &lt;- tokens(\"We're having a great time at the\n               pool and lovely food in the restaurant.\")\n\n# keep great, lovely and a context window of ±1 tokens\n# without padding\ntokens_keep(toks, pattern = c(\"great\", \"lovely\"),\n            window = 1,\n            padding = FALSE)\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"a\"      \"great\"  \"time\"   \"and\"    \"lovely\" \"food\"  \n\n# keep great, lovely and a context window of ±1 tokens\n# with padding\ntokens_keep(toks, pattern = c(\"great\", \"lovely\"),\n            window = 1,\n            padding = TRUE)\n\nTokens consisting of 1 document.\ntext1 :\n [1] \"\"       \"\"       \"a\"      \"great\"  \"time\"   \"\"       \"\"       \"\"      \n [9] \"and\"    \"lovely\" \"food\"   \"\"      \n[ ... and 3 more ]",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#advanced",
    "href": "11-quanteda-tokensadvanced.html#advanced",
    "title": "11  Advanced Token Manipulation",
    "section": "11.4 Advanced",
    "text": "11.4 Advanced\nNext, we provide an overview of advanced tokens operations: splitting and chunking tokens. Both can be useful in some contexts, but tend to be used less frequently than the operations discussed so far.\n\n11.4.1 Splitting\nSplitting tokens implies that we split one token into multiple replacements. The function tokens_split() splits a tokens by a separator pattern, effectively reversing the operation of tokens_compound(). The example below shows how to undo a compounding operation.\n\ntoks &lt;- tokens(\"Value added tax is a multi-word expression.\")\n\n# compound value added tax\ntoks_vat &lt;- tokens_compound(toks, \n                            pattern = phrase(\"value added tax*\"), \n                            concatenator = \"_\")\ntoks_vat\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"Value_added_tax\" \"is\"              \"a\"               \"multi-word\"     \n[5] \"expression\"      \".\"              \n\n# reverse compounding using \"_\" as the separator \n# for splitting tokens\ntokens_split(toks_vat, separator = \"_\")\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"Value\"      \"added\"      \"tax\"        \"is\"         \"a\"         \n[6] \"multi-word\" \"expression\" \".\"         \n\n\n\n\n11.4.2 Chunking\nIn some applications, we may be interested by dividing our texts into equally-sized segments or chunks. You might be working with a set of very long documents, which cannot be segmented into smaller units such as paragraphs or sentences due to missing delimiters (see Chapter 9 and using corpus_reshape() or corpus_segment()). Some methods, such as topic models (Chapter 24) work better when the documents have similar lengths. The function tokens_chunk() can be used to segment a tokens object by chunks of a given size. The overlap argument allows you to specify whether to take over tokens from the preceding chunk. We use the first hotel review of data_corpus_TAhotels and divide the reviews up into smaller chunks with and without overlaps.\n\n# tokenize the first hotel review\ntoks_r1 &lt;- tokens(TAUR::data_corpus_TAhotels[1])\n\n# print first 15 tokens\nprint(toks_r1, max_ntoken = 15)\n\nTokens consisting of 1 document and 1 docvar.\ntext1 :\n [1] \"nice\"        \"hotel\"       \"expensive\"   \"parking\"     \"got\"        \n [6] \"good\"        \"deal\"        \"stay\"        \"hotel\"       \"anniversary\"\n[11] \",\"           \"arrived\"     \"late\"        \"evening\"     \"took\"       \n[ ... and 83 more ]\n\n# chunk into chunks of 5 tokens without overlap\ntoks_r1_chunk &lt;- tokens_chunk(toks_r1, size = 5)\n\n# inspect chunked tokens object\nprint(toks_r1_chunk, max_ndoc = 3)\n\nTokens consisting of 20 documents and 1 docvar.\ntext1.1 :\n[1] \"nice\"      \"hotel\"     \"expensive\" \"parking\"   \"got\"      \n\ntext1.2 :\n[1] \"good\"        \"deal\"        \"stay\"        \"hotel\"       \"anniversary\"\n\ntext1.3 :\n[1] \",\"       \"arrived\" \"late\"    \"evening\" \"took\"   \n\n[ reached max_ndoc ... 17 more documents ]\n\n# chunk into chunks of 5 tokens with overlap and inspect result\ntokens_chunk(toks_r1, size = 5, overlap = 2) |&gt; \n    print(max_ndoc = 3)\n\nTokens consisting of 33 documents and 1 docvar.\ntext1.1 :\n[1] \"nice\"      \"hotel\"     \"expensive\" \"parking\"   \"got\"      \n\ntext1.2 :\n[1] \"parking\" \"got\"     \"good\"    \"deal\"    \"stay\"   \n\ntext1.3 :\n[1] \"deal\"        \"stay\"        \"hotel\"       \"anniversary\" \",\"          \n\n[ reached max_ndoc ... 30 more documents ]\n\n\nAs always, this example serves only for illustration purposes. Usually, the selected chunks would be larger than five documents to mirror the length of “typical” documents, such as sentences or paragraphs.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#further-reading",
    "href": "11-quanteda-tokensadvanced.html#further-reading",
    "title": "11  Advanced Token Manipulation",
    "section": "11.5 Further Reading",
    "text": "11.5 Further Reading\n\nExamples of targeted analyses\nPart-of-speech tagging\nspacy",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#exercises",
    "href": "11-quanteda-tokensadvanced.html#exercises",
    "title": "11  Advanced Token Manipulation",
    "section": "11.6 Exercises",
    "text": "11.6 Exercises\nAdd some here.\n\n\n\n\nGessler, Theresa, and Sophia Hunger. 2022. “How the Refugee Crisis and Radical Right Parties Shape Party Competition on Immigration.” Political Science Research and Methods 10 (3): 524–44. https://doi.org/10.1017/psrm.2021.64.\n\n\nLupia, Arthur, Stuart N. Soroka, and Alison Beatty. 2020. “What Does Congress Want from the National Science Foundation? A Content Analysis of Remarks from 1995 to 2018.” Science Advances 6 (33): eaaz6300. https://doi.org/10.1126/sciadv.aaz6300.\n\n\nRauh, Christopher, Bart Joachim Bes, and Martijn Schoonvelde. 2020. “Undermining, Defusing or Defending European Integration? Assessing Public Communication of European Executives in Times of EU Politicisation.” European Journal of Political Research 59 (2): 397–423. https://doi.org/10.1111/1475-6765.12350.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#footnotes",
    "href": "11-quanteda-tokensadvanced.html#footnotes",
    "title": "11  Advanced Token Manipulation",
    "section": "",
    "text": "tokens_lookup(), which handles phrases internally, is an exception to the rule.↩︎",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Token Manipulation</span>"
    ]
  },
  {
    "objectID": "12-quanteda-dfms.html",
    "href": "12-quanteda-dfms.html",
    "title": "12  Building Document-Feature Matrices",
    "section": "",
    "text": "12.1 Objectives\nCollecting documents in a text corpus, changing the unit of analysis in this corpus (if necessary), tokenising and processing texts are central aspects of each text analysis project. Having processed the tokens object, many statistical analysis of texts require a document-feature matrix (dfm), a mathematical matrix that describes the frequency of terms (e.g., words or phrases) that occur in a collection of documents.\nIn this chapter, we introduce you to the the assumptions we make when creating and analysing texts through document-feature matrices. We describe the nature of a dfm object and the importance of sparsity. You will learn how to create a dfm, how to subset or group a dfm, how to select features, and how to weight, trim, and smooth a dfm. We also show how to match dfms for machine learning approaches, and how to convert dfm objects for further use in other R packages. Finally, we explain the intuition behind feature co-occurrence matrices (fcm), when and why to use them, and how to construct an fcm from a quanteda tokens object. At the end of the chapter, readers will have a solid understanding of creating dfms and fcms, the workhorses for most statistical analyses of textual data.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building Document-Feature Matrices</span>"
    ]
  },
  {
    "objectID": "12-quanteda-dfms.html#methods",
    "href": "12-quanteda-dfms.html#methods",
    "title": "12  Building Document-Feature Matrices",
    "section": "12.2 Methods",
    "text": "12.2 Methods\nThe document-feature matrix is the core object of many analysis of texts. It is a structured representation of textual data in the form of a matrix. After tokenising texts, we treat “text as data” by tabulating the counts of features by documents. Each row in a dfm represents a document, while each column represents a unique feature from across all the documents. The cells of the matrix indicate the frequency of a word in each document. After converting raw texts into a matrix, we move textual data into the same format as many forms of quantitative data. This format allows us to apply various statistical techniques and machine learning tools. Many of these methods have well-understood properties that allows us to generate probability statements and calculate measures of uncertainty (Benoit 2020).\n\n\n\n\n\n\nWhy document-feature matrix rather than document-term matrix?\n\n\n\nMany textbooks and software packages speak of “document-term matrices”. We prefer the name “document-feature matrix” since a dfm does not only contain terms, but can also contain features such as punctuation characters, numbers, symbols, or emojis. “Document-term matrix” implies that such features are uninformative. However, as we describe in Chapter 10, punctuation characters, numbers, or emojis can provide relevant information about texts and should remain in the matrix representation of the text corpus.\n\n\nDocument-feature matrices allow us to analyse texts systematically. Somewhat ironically, we first need to destroy the structure of texts and make it impossible to “read” the documents. While we still know the order and context of words in tokens objects, document-feature matrix only provide information about the frequencies of features in a document. We no longer know at what position in a given document a certain feature appeared. Yet, only this oversimplification of texts allows us to apply statistical methods and analyse “text as data.”\nNote: Maybe add a few sentences on numerical representations of real-world phenomena, such as survey responses, economic indicators, or conflict data (see Benoit 2020: 464-465)?\nLet us explain the structure of a dfm with a simple example. Consider two short documents:\n\nDocument 1: “I love quanteda. I love text analysis.”\nDocument 2: “Text analysis with quanteda is fun.”\n\nA matrix representation of these documents would look as follows:\n\n\n\nTable 12.1: A document-feature matrix without tokens removal, but lower-casing of all features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nlove\nquanteda\n.\ntext\nanalysis\nwith\nis\nfun\n\n\n\n\nDoc 1\n2\n2\n1\n2\n1\n1\n0\n0\n0\n\n\nDoc 2\n0\n0\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\nTable 12.1 shows a dfm consisting of two rows (one per document) and each feature has its own column. The numbers report how often a feature appeared a document. We can treat this dfm like any other quantitative data set. We cannot tell the precise position “love” appeared in Document 1 anymore, but we can immediately say that this feature appeared twice in the Document 1, and that love was not included in Document 2. Since we only know feature frequencies across documents but not their precise positions in a document, we call the process of transforming texts into a matrix “bag-of-words” approach.\nMaybe add a few more general sentences about dfms here.\nLet’s clarify a few key terms by inspecting the Table 12.1. First, the dfm consists of 2 document and 9 features. Document 1 contains 6 types (i.e., unique tokens). The types are i, love, quanteda, text, analysis, .. Document 1 consists of 9 tokens since the words i, love and . appear twice while love, quanteda, text, and analysis appear only once. In Document 2, the number of types is identical to the number of tokens: the processed document contains 7 tokens, and each token appears only once. The dfm has a sparsity of 27.7% since 5 out of the 18 cells have zero counts.\nThe number of types, tokens, features, and a dfm’s sparsity depends on the processing steps we implemented during tokenisation (Chapter 10). For example, if we conduct the common steps of removing stopwords and punctuation characters, the document feature matrix will be considerably smaller. Table 12.2 shows after removing punctuation characters and the words “i”, “with”, and “is”, which are included in most stopword lists. Grimmer, Roberts, and Stewart (2022, 57–59) provide more examples of why the “default” processing steps might be problematic.\n\n\n\nTable 12.2: A document-feature matrix after removing punctuation characters and stopwords\n\n\n\n\n\n\nlove\nquanteda\ntext\nanalysis\nfun\n\n\n\n\nDoc 1\n2\n1\n1\n1\n0\n\n\nDoc 2\n0\n1\n1\n1\n1\n\n\n\n\n\n\nThe sparsity was reduced from 28% to 20%, and the number of columns changed from nine to five. At the same time, removing these stopwords and punctuation did not result in a considerable loss of information about the content of each document. It is important to repeat, however, that stopword removal is not always recommended and that you should inspect the features included in a stopword list. We removed the pronoun “I”. In many cases, the removal of pronouns will not be an issue, but in other cases pronouns can tell us a lot about a text. For instance, Crisp et al. (2021) show Japanese candidates mention first-person pronouns more often in their personal manifesto if they face stronger intra-party competition. The usage of pronouns is a signal of candidates’ campaign strategies and their degree of personalisation.\n\n\n\n\n\n\nSelecting features in your tokens object rather than the dfm\n\n\n\nWe strongly recommend to conduct all processing steps involving the removal of features during the tokenisation step rather than applying the same functions to a dfm. Multi-word expressions can be easily detected in tokens object, but this information is not available in a dfm anymore. Similarly, we strongly recommend applying dictionaries (Chapter 16) to the tokens object to identify possible multi-word expressions correctly.\nThere are, however, a few processing steps that rely on feature frequencies. Users might want to remove word appearing in less or more than a certain number of percentage of documents. These so-called trimming operations, covered in detail in the Applications section, only work dfm objects since we need to the word counts to filter by frequncies.\n\n\nThe dfm in Table 12.1 reports counts of features. This is the default when a dfm is created. Oftentimes, we might prefer prefer weighting feature frequencies. For example, many users might want to normalise a dfm by calculating the proportion of feature counts within each document. This process is called document normalisation as it homogenises the counts for each document. As such, it addresses the (potential) issue of varied document length in a corpus, allowing for comparing relative frequencies as opposed to raw counts. For example, two mentions of “terrible” in a short hotel review consisting of 100 words have a larger weight than three mentions of “terrible” in review of 500 words. When normalising the document, the relative frequency of “terrible” would be 0.02 (or 2%) in the short review, and 0.006 (0.6%) in the longer review, highlighting the much higher prevalence of this negative term in the first review.\nBoolean weighting is another type of weighting. Applying boolean weights implies that all non-zero counts are recoded as 1. This weighting scheme is useful when users simply want to know whether or not a feature appeared in a document without being intersted in the absolute frequencies of the term.\nA popular, and slightly more complex, weighting scheme is called tf-idf or term frequency-inverse document frequency. Tf-idf is a very popular approach in information retrieval and often used in machine learning (Chapter 23). Tf-idf up-weights terms that appear more often in a given document and down-weight terms that are common across documents. As a term appears in more documents, its weight approaches zero. While tf-idf is a very useful approach for identifying “unique” words that define a given document, tf-idf may be problematic when working with a topic-specific texts corpus. Tf-idf will assign low weights to words appearing across documents. When we apply td-idf to a corpus of debates on climate protection, td-idf will eliminate climate-related features even if they appear in different frequencies across documents. To sum up, while tf-idf can be useful for certain classification tasks, users should proceed with great caution when applying tf-idf weighting.\nLet’s go through a simple example to understand how to calculate tf-idf scores. Going back to the example above, Document 1 contains the word “horrible” 2 times and consists of 100 tokens. Document 2 contains the word horrible 3 times but consists of 500 tokens. Let’s use quanteda’s default calculation for an example.\nTo calculate tf-idf we need to know term frequencies and inverse-docuemnt frequencies.\n\\[\n\\text{tf-idf} = \\text{tf} \\times \\text{idf}\n\\]\nWhen using the counts of the term frequencies for “horrible”, Document 1 has a \\(tf = 2\\), while the \\(tf\\) of “horrible” for Document 2 equals \\(3\\).\nNote that we can also calculate tf with normalised frequencies, i.e.,\n\\[\n\\text{tf} = \\frac{\\text{Number of times term appears in the document}}{\\text{Total number of terms in the document}}\n\\]\nIn this case, the \\(tf\\) for Document 1 would be \\(for \\frac{2}{100}\\) and \\(\\frac{3}{500}\\) for Document 2. The scheme_tf argument in quanteda’s dfm_tfidf() allows you to select count or prop.\nNext we calculate the inverse document frequency (idf) as:\n\\[\n\\text{idf} = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents with the term}}\\right)\n\\]\nSince “horrible” appears in both Document 1 and Document 2:\n\\[\n\\text{idf for \"horrible\"} = \\log\\left(\\frac{2}{2}\\right) = \\log(1) = 0\n\\]\nIn the last step, we multiply each document’s \\(tf\\) for “horrible” with the \\(idf\\) of the same term.\n\\[\n\\text{tf-idf} = \\text{tf} \\times \\text{idf}\n\\]\nFor Document 1: \\(\\text{tf-idf for \"horrible\" in Document 1} = 2 \\times 0 = 0\\)\nFor Document 2: \\(\\text{tf-idf for \"horrible\" in Document 2} = 3 \\times 0 = 0\\)\nIf Document 1 contained “terrible” 2 times, but Document 2 did not mention “horrible” at all, \\(idf\\) would change to: \\(\\text{idf for \"horrible\"} = \\log\\left(\\frac{2}{1}\\right)\\) because “terrible” is included in only one of our two documents. tf-idf for Document 2 would remain 0, but tf-idf for “horrible” in Document 1 changes to \\(2 \\times \\log(2) = 1.386\\). The example shows highights that terms appearing in all documents always receive a tf-idf score of 0, no matter how (in)frequent they are. Keyness analysis (Chapter 17) is another approach of identifying words more “unique” to one group of documents, and–in contrast to tf-idf–features appearing across all documents are not down-weighted to 0 if their expected frequencies differ.\nThe trimming of a document-feature matrix is another important step to remove the number of features and sparsity of the dfm. The dfm is reduced in size based on the document frequency or term frequency. Usually, we trim features based on minimum frequencies, but we can also exclude features in terms of maximum frequencies. Combining trimming operations for minimum and maximum frequencies will return features that fall within this range. Let’s consider the following example. If we have a large text corpus and want to remove very infrequent terms we could trim based on the minimum term frequencies and document frequencies. We could, for instance only keep terms occurring at least 10 times across all documents and also appear in at least 2 documents. In this case, the minimum term frequency equals \\(10\\) and the minimum document frequency equals \\(2\\). Of course, we can also trim features based on relative frequencies. The Application section below provides several examples of trimming dfms.\nSmoothing is another frequently used form of weighting a dfm. It implies that zero counts are changed to a constant other than zero. If we set a smoothing parameter of 1, the constant of 1 is added to all cells in the dfm: zero counts are changed to 1, cells with the value 1 get the value 2 etc.\nSmoothing a dfm is required for statistical models that struggle with zero probabilities. For example, a zero probability of a feature in a Naive Bayes classifier (Chapter 23) would change the entire document’s probably to 0, even if other terms indicate it belongs to that class.\nNext, we move to the feature co-occurrence matrix, which measure the co-occurrence of features. Feature co-occurrence matrices are the input for many word embedding models, which will be covered extensively in Chapter 26. The intuition behind fcms is simple: instead of counting how often a feature appears in a document (the intuition behind dfms) we want to know which feature appear together within a user-defined context window.\nAdd content on fcm here.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building Document-Feature Matrices</span>"
    ]
  },
  {
    "objectID": "12-quanteda-dfms.html#applications",
    "href": "12-quanteda-dfms.html#applications",
    "title": "12  Building Document-Feature Matrices",
    "section": "12.3 Applications",
    "text": "12.3 Applications\nIn this section, you will learn how to create a dfm, how to get identify the number of types and tokens, and assess it sparsity. You will learn how to weight, trim, group, and subset a document-feature matrix, and how to create and modify a feature co-occurrence matrix.\n\n12.3.1 Creating a Document-Feature Matrix from a Tokens Object\nWe start with replicating the example mentioned above. Suppose we have two documents in a text corpus. We first tokenise the corpus and then create a dfm().\n\n# create example corpus\ncorp &lt;- corpus(\n    c(doc1 = \"I love quanteda. I love text analysis.\",\n      doc2 = \"Text analysis with quanteda is fun.\"))\n\n# tokenize corpus without making processing \ntoks &lt;- tokens(corp)\n\n# inspect tokens object\nprint(toks)\n\nTokens consisting of 2 documents.\ndoc1 :\n[1] \"I\"        \"love\"     \"quanteda\" \".\"        \"I\"        \"love\"     \"text\"    \n[8] \"analysis\" \".\"       \n\ndoc2 :\n[1] \"Text\"     \"analysis\" \"with\"     \"quanteda\" \"is\"       \"fun\"      \".\"       \n\n# create a document-feature matrix\ndfmat &lt;- dfm(toks)\n\n# inspect dfm: first three documents, first four features\nprint(dfmat, max_ndoc = 3, max_nfeat = 4)\n\nDocument-feature matrix of: 2 documents, 9 features (27.78% sparse) and 0 docvars.\n      features\ndocs   i love quanteda .\n  doc1 2    2        1 2\n  doc2 0    0        1 1\n[ reached max_nfeat ... 5 more features ]\n\n\n\n\n12.3.2 Types, Tokens, Features, and Sparsity\nWe can access the number of features, tokens, types, and the sparsity of a dfm with in-built quanteda functions. We convert the corpus of US inaugural speeches to a document-feature matrix retrieve these statistics.\n\ndfmat_inaug &lt;- data_corpus_inaugural |&gt; \n    tokens() |&gt; \n    dfm()\n\n# sparsity() and nfeat() are reported on the dfm-level\nsparsity(dfmat_inaug)\n\n[1] 0.9183668\n\nnfeat(dfmat_inaug)\n\n[1] 9437\n\n# ntype() and ntoken() are reported on the document-level\n# show the number of types for first five documents\nntype(dfmat_inaug) |&gt; head(n = 5)\n\n1789-Washington 1793-Washington      1797-Adams  1801-Jefferson  1805-Jefferson \n            603              95             801             687             781 \n\n# show the number of tokens for first five documents\nntoken(dfmat_inaug) |&gt; head(n = 5)\n\n1789-Washington 1793-Washington      1797-Adams  1801-Jefferson  1805-Jefferson \n           1537             147            2577            1923            2380 \n\n\n\n\n\n\n\n\ndfm()’s default behaviour\n\n\n\nWhen converting a tokens object into a document-feature matrix, the dfm() considers all features included in the tokens object. It does not remove any features, but, by default, all tokens are transformed to lower case, even if you have not used tokens_tolower().\n\ntoks_example &lt;- tokens(\"Transforming a tokens object to a dfm.\")\n\n# by default, dfm() transforms all tokens to lowercase\ndfm(toks_example)\n\nDocument-feature matrix of: 1 document, 7 features (0.00% sparse) and 0 docvars.\n       features\ndocs    transforming a tokens object to dfm .\n  text1            1 2      1      1  1   1 1\n\n# can change this behaviour by using dfm(x, tolower = FALSE)\ndfm(toks_example, tolower = FALSE)\n\nDocument-feature matrix of: 1 document, 7 features (0.00% sparse) and 0 docvars.\n       features\ndocs    Transforming a tokens object to dfm .\n  text1            1 2      1      1  1   1 1\n\n\n\n\nIn almost all applications we can think of, lower-casing tokens is a reasonable and recommended processing choice. The sparsity of a dfm and the number of types (unique tokens) increases drastically when you treat upper- and lower-case tokens separately. If you want to make sure certain tokens (e.g., Labour vs. labour) are treated separately, we suggest replacing these tokens during the tokenisation process (Chapter 11) rather than preserving the original case of all tokens.\n\n# transform all features to lowercase\ndfmat_lower &lt;- data_corpus_inaugural |&gt; \n    tokens() |&gt; \n    dfm(tolower = TRUE) # default\n\n# sum of features\nnfeat(dfmat_lower)\n\n[1] 9437\n\n# do not change tokens to lowercase\ndfmat_unchanged &lt;- data_corpus_inaugural |&gt; \n    tokens() |&gt; \n    dfm(tolower = FALSE) # keep original case \n\n# sum of features\nnfeat(dfmat_unchanged)\n\n[1] 10162\n\n\nThe number of unique tokens increased from 9,437 to 10,162 when not changing tokens to its lower-cased form. In the second case, upper-case tokens will be treated as a different feature than lower-case token.\n\n\n12.3.3 Feature Weighting\nWhen you have created a document-feature matrix, you can apply weights using dfm_weight() and adjust the scheme argument. Let’s weight the document-feature matrix of inaugural speeches by proportion to normalise the documents.\n\ndfmat_inaug &lt;- data_corpus_inaugural |&gt; \n    tokens() |&gt; \n    dfm()\n\n# document normalisation using dfm(x, scheme = prop)\ndfmat_inaug_prop &lt;- dfm_weight(dfmat_inaug, scheme = \"prop\")\n\n# inspect dfm: first three documents, first four features\nprint(dfmat_inaug_prop, max_ndoc = 3, max_nfeat = 4)\n\nDocument-feature matrix of: 59 documents, 9,437 features (91.84% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens         of        the       senate\n  1789-Washington    0.0006506181 0.04619388 0.07547170 0.0006506181\n  1793-Washington    0            0.07482993 0.08843537 0           \n  1797-Adams         0.0011641444 0.05432674 0.06325184 0.0003880481\n[ reached max_ndoc ... 56 more documents, reached max_nfeat ... 9,433 more features ]\n\n\nWe can transform non-zero counts to 1 with dfm_weight(x, scheme = \"boolean\").\n\n# apply boolean weighting\ndfmat_inaug_boolen &lt;- dfm_weight(dfmat_inaug, scheme = \"boolean\")\n\n# inspect dfm: first five documents, first four features\nprint(dfmat_inaug_boolen, max_ndoc = 3, max_nfeat = 4)\n\nDocument-feature matrix of: 59 documents, 9,437 features (91.84% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens of the senate\n  1789-Washington               1  1   1      1\n  1793-Washington               0  1   1      0\n  1797-Adams                    1  1   1      1\n[ reached max_ndoc ... 56 more documents, reached max_nfeat ... 9,433 more features ]\n\n\nAfter applying dfm_weight(x, scheme = \"boolean\"), the cells contain only 0 (document does not include feature) or 1 (document contains feature at least once) rather than absolute or relative frequencies.\nYou can apply tf-idf weighting with dfm_tfidf(). Let’s transform dfmat_inaug to tf-idf by using counts instead of normalised frequencies first for the term frequencies.\n\n# apply dfm-idf weighting; use count-based measure of term frequencies\ndfmat_tfidfcount &lt;- dfm_tfidf(dfmat_inaug, scheme_tf = \"count\")\n\n# inspect dfm: first five documents, first four features\nprint(dfmat_tfidfcount, max_nfeat = 5, max_ndoc = 4)\n\nDocument-feature matrix of: 59 documents, 9,437 features (91.84% sparse) and 4 docvars.\n                 features\ndocs              fellow-citizens of the    senate and\n  1789-Washington       0.4920984  0   0 0.8166095   0\n  1793-Washington       0          0   0 0           0\n  1797-Adams            1.4762952  0   0 0.8166095   0\n  1801-Jefferson        0.9841968  0   0 0           0\n[ reached max_ndoc ... 55 more documents, reached max_nfeat ... 9,432 more features ]\n\n\nThe output reveals that features appearing in all documents, including of and the receive a score of 0. This “feature” of tf-idf will down-weight all terms appearing across documents, which might be problematic since it might also include meaningful terms. For example, it is very likely that every politicians will mention “climate” in parliamentary debates about environmental protection. Tf-idf, in turn, would assign the word “climate” a score of 0 for all speakers.\nYou can adjust the way quanteda calculates tf-idf, for example by using harmonised term frequencies (dfm_tfidf(x, scheme_tf = \"prop\")).\n\n# use harmonised counts (=prop) for term frequencies\ndfmat_tfidfprop &lt;- dfm_tfidf(dfmat_inaug, scheme_tf = \"prop\")\n\nYou can also change the base for the logarithms. quanteda’s default base 10, which mirrors the tf-idf implementation in Manning, Raghavan, and Schütze (2008). Other R packages use different default weights. tidytext (Silge and Robinson 2016) applies the natural log, whereas tm (Feinerer, Hornik, and Meyer 2008a) uses base 2. Below we show how to replicate other packages’ tf-idf scores.\n\n# mirror tidytext's implementation\ndfm_tfidf(dfmat_inaug, \n          scheme_tf = \"prop\",\n          scheme_df = \"inverse\", \n          base = exp(1))\n\n# mirror tm's implementation\ndfm_tfidf(dfmat_inaug, \n          scheme_tf = \"prop\", \n          scheme_df = \"inverse\", \n          base = 2)\n\n\n\n12.3.4 Trimming\nAs mentioned above, most dfms are very sparse. Sparsity above 90% is the norm in most text analysis applications, especially if documents are short. Most features simply do not appear in a document. Very high levels of sparsity can result in convergence issues for topic models or unsupervised scaling approaches such as Wordfish (Slapin and Proksch 2008). In turn, if we keep the most frequent terms in our dfm, many models might be heavily influenced by these very frequent terms, and potential differences across documents will not be uncovered. Let’s take the following example: as we have seen above, a large proportion of documents are punctuation characters and stopwords. If we keep these features in our dfm, these terms will drive the results and, possibly, underestimate differences in topic prevalence or policy positions. Chapter 22 and Chapter 24 cover these issues in more detail.\nThe function dfm_trim() allows you to exclude very infrequent or very frequent terms from your dfm before proceeding with the analysis to avoid these issues or (at least) to assess the validity of the results when you exclude very frequent and infrequent terms. We can features in terms of absolute frequencies, proportions, ranks, or quantiles. In addition, we can reduce the size based on document frequencies (“In how many documents does a feature appear?”) or term frequencies (“How often does a feature appear across all documents?”). We explain all of these arguments in the example below. We use the corpus of hotel review as an example and always return the number of types and tokens to provide an overview of the reduction.\n\n# tokenise corpus of hotel reviews without any tokens removal\ntoks_hotels &lt;- tokens(TAUR::data_corpus_TAhotels)\n\n# no processing apart from lower-casing terms (quanteda's default)\ndfmat_hotels &lt;- dfm(tokes)\n\n# keep only features occurring at least 10 times (min_termfreq = 20) \n# and in &gt;= 10 documents (min_docfreq = 10)\ndfm_trim(dfmat_hotels, min_termfreq = 10, min_docfreq = 10)\n\n# keep only features occurring at least 10 times (min_termfreq = 10)\n# and in at least 40% of the documents (min_docfreq = 0.4)\ndfm_trim(dfmat_hotels, min_termfreq = 10, min_docfreq = 0.4)\n\n# keep only features occurring at most 10 times (max_termfreq = 10) \n# and in at most 200 documents (max_docfreq = 200)\ndfm_trim(dfmat_hotels, max_termfreq = 10, max_docfreq = 200)\n\n# keep only features occurring at most 10 times and \n# in at most 3/4 of the documents\ndfm_trim(dfmat_hotels, max_termfreq = 10, max_docfreq = 0.75)\n\n# keep only words occurring frequently (top 20% -&gt; min_termfreq = 0.2) \n# and in at most 2 documents\ndfm_trim(dfmat, min_termfreq = 0.2, max_docfreq = 2, \n         termfreq_type = \"quantile\")\n\n\n\n12.3.5 Grouping and Subsetting\nBy default, a dfm contains as many documents as the input tokens object. Yet, in some cases you might want to combine documents in a dfm by a grouping variable. Instead of analysing each speech by a delivered by a politician, we might want to aggregated speeches to the level of parties. If we want to understand differences between hotel reviews with low and high rankings, we could group our dfm by ranking rather than review.\nThe function dfm_group() sums up cell frequencies within groups, determined by a document level variable and creates new “documents” with the group labels. Let’s introduce dfm_group() with an example: we transform the corpus of 20,491 hotel reviews (data_corpus_TAhotels) to a dfm, and then group this dfm by the “Rating” document-level variable, indicating the reviewer’s satisfaction with the hotel (ranging from 1 to 5 stars). The grouped dfm should therefore consist of five “documents”, with each document summing up cell frequencies of all documents with the same rating.\n\n# tokenise the corpus and create a document feature matrix in one go\ndfmat_hotels &lt;- TAUR::data_corpus_TAhotels |&gt; \n    tokens(remove_punct = TRUE) |&gt; # remove punctuation\n    tokens_remove(pattern = stopwords(\"en\")) |&gt; # remove stopwords\n    dfm() # create dfm\n\n# inspect dfm: first five documents and first five features\nprint(dfmat_hotels, max_ndoc = 5, max_nfeat = 5)\n\nDocument-feature matrix of: 20,491 documents, 78,553 features (99.90% sparse) and 1 docvar.\n       features\ndocs    nice hotel expensive parking got\n  text1    5     2         1       3   1\n  text2    1     5         0       0   3\n  text3    3     3         0       0   2\n  text4    2     4         0       0   0\n  text5    0     2         0       1   1\n[ reached max_ndoc ... 20,486 more documents, reached max_nfeat ... 78,548 more features ]\n\n# now group this dfm consisting of 20,491 review\n# based on Rating document-level variable\ndfmat_hotels_grouped &lt;- dfm_group(dfmat_hotels, groups = Rating)\n\n# inspect the grouped dfm: all documents and first five features\nprint(dfmat_hotels_grouped, max_ndoc = 5, max_nfeat = 5)\n\nDocument-feature matrix of: 5 documents, 78,553 features (64.64% sparse) and 1 docvar.\n    features\ndocs nice hotel expensive parking  got\n   1  367  3604        97     180  630\n   2 1055  4187       195     177  844\n   3 1751  5078       269     232  812\n   4 4839 14183       669     567 1900\n   5 4424 21895       694     473 2003\n[ reached max_nfeat ... 78,548 more features ]\n\n\nThe grouped dfm consists of only 5 documents, while the number of features is the same as in the review-level dfm (dfmat_hotels). The number of frequencies does not change because dfm_group() simply sums up the cell counts. Inspecting the output of the first five features, we see that the nice appears 367 times in 1-star reviews, while the term appears 4,424 times in 5-star reviews.\nIn Chapter 9, we introduced corpus_subset() to filter documents based on one or more document-level variables. We can do the same with dfm’s by using dfm_subset(). For example, we can filter only 1-star reviews to investigate word frequencies in very negative reviews.\n\n# get overview of reviews by rating\ntable(dfmat_hotels$Rating)\n\n\n   1    2    3    4    5 \n1421 1793 2184 6039 9054 \n\n# subset dfmat_hotels by keeping only 1-star reviews\ndfmat_hotels_1star &lt;- dfm_subset(dfmat_hotels, Rating == \"1\")\n\n# check that subsetting worked as expected by \n# creating a cross-table of the Rating docvars\ntable(dfmat_hotels_1star$Rating)\n\n\n   1 \n1421 \n\n\n\n\n\n\n\n\nSubset your objects wisely\n\n\n\nTokenisation is the bottleneck operation in processing texts for quantitative analyses. If you know that you want to exclude certain documents from all analyses (for instance, all documents published prior to a specific date), we recommend filtering out these documents before from the corpus object using corpus_subset() rather than excluding the documents from the tokens object or dfm. Having said that, keep in mind that excluding documents may lead to selection biases and other problems (Grimmer, Roberts, and Stewart 2022: ch. 3).\n\n\n\n\n12.3.6 Converting a dfm Object for Further Use\nDocument-feature matrices are the input for many statistical analyses of textual data. The quanteda package infrastructure includes many text models that work directly with a quanteda dfm object. Other packages require a dfm in a slightly different format. The function convert() provides easy conversion from a dfm to document-term representations used in all other text analysis packages.\nquanteda’s convert() function allows users to convert a dfm object to the following formats.\n\nlda: a list with components “documents” and “vocab” as needed by the function lda.collapsed.gibbs.sampler from the lda package (Chang 2015).\ntm: a DocumentTermMatrix from the tm package (Feinerer, Hornik, and Meyer 2008b).\nstm: the format for the stm package (Roberts, Stewart, and Tingley 2019) for structural topic models. Note: the stm package also allows you to input a quanteda object directly, and no conversion is required.\ntopicmodels: the dtm format as used by the topicmodels package (Grün and Hornik 2011) for Latent Dirichlet Allocation (LDA) models and Correlated Topics Models (CTM)\n\nIn addition, the dfm can be converted to a data.frame and tripletlist consisting of document, feature, and frequency.\nMany recently developed R packages, such as keyATM (Eshima, Imai, and Sakasi 2023) for keyword-assisted topic models, LSX (Watanabe 2023) for latent semantic scaling, or conText (Rodriguez, Spirling, and Stewart 2023) for ‘a la Carte’ on Text Embedding Regression, rely on quanteda and allow you to input processed tokens() or dfm() objects.\nThe popular tidytext package (Silge and Robinson 2016) includes the function cast_dfm() which allows you to cast a data frame to a quanteda dfm (see also Chapter 28).\n\n\n12.3.7 Creating and Processing a Feature Co-Occurrence Matrix\nTo be added. Also reference Chapter 26 which will explain how to create word embeddings based on an fcm object.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building Document-Feature Matrices</span>"
    ]
  },
  {
    "objectID": "12-quanteda-dfms.html#advanced",
    "href": "12-quanteda-dfms.html#advanced",
    "title": "12  Building Document-Feature Matrices",
    "section": "12.4 Advanced",
    "text": "12.4 Advanced\nWe introduce two more operations for dfm objects, which are useful—and required—for many analyses, but slightly more advanced: smoothing and matching dfms.\n\n12.4.1 Smoothing\nAs described in the Methods section above, smoothing a dfm implies that zero counts in a dfm are changed to a constant other than zero. Smoothing is required for models that do not work with zero probabilities, for instance Naive Bayes classifiers. We return to the minimal dfm object created above to show how to smooth your dfm().\n\n# print dfm object\nprint(dfmat)\n\nDocument-feature matrix of: 2 documents, 9 features (27.78% sparse) and 0 docvars.\n      features\ndocs   i love quanteda . text analysis with is fun\n  doc1 2    2        1 2    1        1    0  0   0\n  doc2 0    0        1 1    1        1    1  1   1\n\n# smooth dfm by adding a constant of 1 to zero cells\ndfm_smooth(dfmat, smoothing = 1)\n\nDocument-feature matrix of: 2 documents, 9 features (0.00% sparse) and 0 docvars.\n      features\ndocs   i love quanteda . text analysis with is fun\n  doc1 3    3        2 3    2        2    1  1   1\n  doc2 1    1        2 2    2        2    2  2   2\n\n\n\n# equivalent to:\ndfm_weight(dfmat, smoothing = 1)\n\n\n# smooth dfm by adding a constant of 0.5 to zero cells\ndfm_smooth(dfmat, smoothing = 0.5)\n\nDocument-feature matrix of: 2 documents, 9 features (0.00% sparse) and 0 docvars.\n      features\ndocs     i love quanteda   . text analysis with  is fun\n  doc1 2.5  2.5      1.5 2.5  1.5      1.5  0.5 0.5 0.5\n  doc2 0.5  0.5      1.5 1.5  1.5      1.5  1.5 1.5 1.5\n\n\n\n\n12.4.2 Matching Two Document-Feature Matrices\nFinally, we show how to match the features of two dfms. Matching features is required for some machine learning classifiers that can only consider features occurring in the training set for predictions of documents in a second dfm. More details are provided in Chapter 23.\nLet’s split the corpus of US inaugural speeches into two objects: one corpus containing speeches delivered between 1945 and 1990, and another one including speeches delivered since 1990. Afterwards, we match the feature set of one dfm with the features existing in the other dfm.\n\n# corpus consisting of speeches between 1945 and 1990\ncorp_pre1990 &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1945 & Year &lt;= 1990)\n\n# check that subsetting worked as expected\n# by inspecting the Year document-level variable\ndocvars(corp_pre1990, \"Year\")\n\n [1] 1949 1953 1957 1961 1965 1969 1973 1977 1981 1985 1989\n\n# corpus consisting of speeches since 1990\ncorp_post1990 &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1990)\n\n# check that subsetting worked as expected\n# by inspecting the Year document-level variable\ndocvars(corp_post1990, \"Year\")\n\n[1] 1993 1997 2001 2005 2009 2013 2017 2021\n\n# create two dfms\ndfmat_pre1990 &lt;- corp_pre1990 |&gt; \n    tokens() |&gt; \n    dfm()\n\ndfmat_post1990 &lt;- corp_post1990 |&gt; \n    tokens() |&gt; \n    dfm()\n\n# match dfms: only keep features in dfmat_pre1990\n# that appear in dfmat_post1990\ndfmat_pre1990matched &lt;- dfm_match(dfmat_pre1990, features = featnames(dfmat_post1990))\n\n# inspect dfm\nprint(dfmat_pre1990matched)\n\nDocument-feature matrix of: 11 documents, 2,691 features (82.54% sparse) and 4 docvars.\n                 features\ndocs              my fellow citizens   , today we celebrate the mystery  of\n  1949-Truman      1      1        1 105     2 59         0 141       0  96\n  1953-Eisenhower  4      2        3 126     1 66         0 171       0 142\n  1957-Eisenhower  4      0        0 121     2 51         0 114       0  96\n  1961-Kennedy     3      4        5  84     3 30         0  86       0  65\n  1965-Johnson     3      4        1  95     2 34         0  77       0  57\n  1969-Nixon       7      2        1 134     5 65         4 136       0  94\n[ reached max_ndoc ... 5 more documents, reached max_nfeat ... 2,681 more features ]\n\n# match dfms: only keep features in dfmat_post1990\n# that appear in dfmat_pre1990\ndfmat_post1990matched &lt;- dfm_match(dfmat_post1990, features = featnames(dfmat_pre1990))\n\n# inspect dfm\nprint(dfmat_post1990matched)\n\nDocument-feature matrix of: 8 documents, 3,331 features (84.20% sparse) and 4 docvars.\n              features\ndocs           mr   . vice president   , chief justice and fellow citizens\n  1993-Clinton  0  81    0         2 139     0       0  66      5        2\n  1997-Clinton  0 108    0         1 131     0       1  94      7        7\n  2001-Bush     0  96    1         3 110     0       3  82      1        9\n  2005-Bush     1  98    1         4 120     1       6 108      3        6\n  2009-Obama    0 118    0         1 130     0       0 111      1        1\n  2013-Obama    1  89    1         2  99     1       2  89      3        6\n[ reached max_ndoc ... 2 more documents, reached max_nfeat ... 3,321 more features ]",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building Document-Feature Matrices</span>"
    ]
  },
  {
    "objectID": "12-quanteda-dfms.html#further-reading",
    "href": "12-quanteda-dfms.html#further-reading",
    "title": "12  Building Document-Feature Matrices",
    "section": "12.5 Further Reading",
    "text": "12.5 Further Reading\n\nOne of the first, and possibly most famous bag-of-words applications: Mosteller and Wallace (1963)\nA deep dive into weighting document-feature matrices: Manning, Raghavan, and Schütze (2008, ch. 6)\nDocument-feature matrices, processing, and why the default options are often not ideal: Grimmer, Roberts, and Stewart (2022, ch. 5)",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building Document-Feature Matrices</span>"
    ]
  },
  {
    "objectID": "12-quanteda-dfms.html#exercises",
    "href": "12-quanteda-dfms.html#exercises",
    "title": "12  Building Document-Feature Matrices",
    "section": "12.6 Exercises",
    "text": "12.6 Exercises\nAdd some here.\n\n\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 461–97. Thousand Oaks: Sage.\n\n\nChang, Jonathan. 2015. Lda: Collapsed Gibbs Sampling Methods for Topic Models. https://CRAN.R-project.org/package=lda.\n\n\nCrisp, Brian F., Benjamin Schneider, Amy Catalinac, and Taishi Muraoka. 2021. “Capturing Vote-Seeking Incentives and the Cultivation of a Personal and Party Vote.” Electoral Studies 72: 102369. https://doi.org/10.1016/j.electstud.2021.102369.\n\n\nEshima, Shusei, Kosuke Imai, and Tomoya Sakasi. 2023. “Keyword Assisted Topic Models.” American Journal of Political Science online first. https://doi.org/10.1111/ajps.12779.\n\n\nFeinerer, Ingo, Kurt Hornik, and David Meyer. 2008b. “Text Mining Infrastructure in R.” Journal of Statistical Software 25 (5): 1–54. https://doi.org/10.18637/jss.v025.i05.\n\n\n———. 2008a. “Text Mining Infrastructure in R.” Journal of Statistical Software 25 (5): 1–54. https://www.jstatsoft.org/v25/i05/.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts: A New Framework for Machine Learning and the Social Sciences. Princeton: Princeton University Press.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nManning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. 2008. An Introduction to Information Retrieval. New York: Cambridge University Press. https://nlp.stanford.edu/IR-book/.\n\n\nMosteller, F., and D. L. Wallace. 1963. “Inference in an Authorship Problem.” Journal of the American Statistical Assocation 58 (302): 275–309.\n\n\nRoberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2019. “stm: An R Package for Structural Topic Models.” Journal of Statistical Software 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nRodriguez, Pedro L., Arthur Spirling, and Brandon Stewart. 2023. conText: ’A La Carte’ on Text (ConText) Embedding Regression. https://CRAN.R-project.org/package=conText.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in r.” Journal of Open Source Software 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSlapin, Jonathan B., and Sven-Oliver Proksch. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.” American Journal of Political Science 52 (3): 705–22. https://doi.org/10.1111/j.1540-5907.2008.00338.x.\n\n\nWatanabe, Kohei. 2023. LSX: Semi-Supervised Algorithm for Document Scaling. https://CRAN.R-project.org/package=LSX.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Building Document-Feature Matrices</span>"
    ]
  },
  {
    "objectID": "13-quanteda-fcms.html",
    "href": "13-quanteda-fcms.html",
    "title": "13  Building Feature Co-occurrence Matrices",
    "section": "",
    "text": "13.1 Objectives\nIn this chapter we explain the intuition behind feature co-occurrence matrices (fcm), when and why to use them, and how to construct an fcm from a quanteda tokens object. At the end of the chapter, readers will have a solid understanding of creating fcms and when and how they can use them.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Building Feature Co-occurrence Matrices</span>"
    ]
  },
  {
    "objectID": "13-quanteda-fcms.html#methods",
    "href": "13-quanteda-fcms.html#methods",
    "title": "13  Building Feature Co-occurrence Matrices",
    "section": "13.2 Methods",
    "text": "13.2 Methods\n\nCreating fcms\nAdvanced options",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Building Feature Co-occurrence Matrices</span>"
    ]
  },
  {
    "objectID": "13-quanteda-fcms.html#examples",
    "href": "13-quanteda-fcms.html#examples",
    "title": "13  Building Feature Co-occurrence Matrices",
    "section": "13.3 Examples",
    "text": "13.3 Examples\n\nCo-occurrence statistics\nNetwork analysis\nComputing word embeddings",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Building Feature Co-occurrence Matrices</span>"
    ]
  },
  {
    "objectID": "13-quanteda-fcms.html#issues",
    "href": "13-quanteda-fcms.html#issues",
    "title": "13  Building Feature Co-occurrence Matrices",
    "section": "13.4 Issues",
    "text": "13.4 Issues\nAdditional issues.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Building Feature Co-occurrence Matrices</span>"
    ]
  },
  {
    "objectID": "13-quanteda-fcms.html#further-reading",
    "href": "13-quanteda-fcms.html#further-reading",
    "title": "13  Building Feature Co-occurrence Matrices",
    "section": "13.5 Further Reading",
    "text": "13.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Building Feature Co-occurrence Matrices</span>"
    ]
  },
  {
    "objectID": "13-quanteda-fcms.html#exercises",
    "href": "13-quanteda-fcms.html#exercises",
    "title": "13  Building Feature Co-occurrence Matrices",
    "section": "13.6 Exercises",
    "text": "13.6 Exercises\nAdd some here.",
    "crumbs": [
      "Processing Texts",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Building Feature Co-occurrence Matrices</span>"
    ]
  },
  {
    "objectID": "14-exploring-description.html",
    "href": "14-exploring-description.html",
    "title": "14  Describing Texts",
    "section": "",
    "text": "14.1 Objectives\nDescribing the data in texts. - n functions - summary() - docnames() - docvars() - meta() - featnames() - head(), tail(), print()",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Describing Texts</span>"
    ]
  },
  {
    "objectID": "14-exploring-description.html#methods",
    "href": "14-exploring-description.html#methods",
    "title": "14  Describing Texts",
    "section": "14.2 Methods",
    "text": "14.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Describing Texts</span>"
    ]
  },
  {
    "objectID": "14-exploring-description.html#examples",
    "href": "14-exploring-description.html#examples",
    "title": "14  Describing Texts",
    "section": "14.3 Examples",
    "text": "14.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Describing Texts</span>"
    ]
  },
  {
    "objectID": "14-exploring-description.html#issues",
    "href": "14-exploring-description.html#issues",
    "title": "14  Describing Texts",
    "section": "14.4 Issues",
    "text": "14.4 Issues\nAdditional issues.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Describing Texts</span>"
    ]
  },
  {
    "objectID": "14-exploring-description.html#further-reading",
    "href": "14-exploring-description.html#further-reading",
    "title": "14  Describing Texts",
    "section": "14.5 Further Reading",
    "text": "14.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Describing Texts</span>"
    ]
  },
  {
    "objectID": "14-exploring-description.html#exercises",
    "href": "14-exploring-description.html#exercises",
    "title": "14  Describing Texts",
    "section": "14.6 Exercises",
    "text": "14.6 Exercises\nAdd some here.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Describing Texts</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html",
    "href": "15-exploring-kwic.html",
    "title": "15  Keywords-in-Context",
    "section": "",
    "text": "15.1 Objectives",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html#objectives",
    "href": "15-exploring-kwic.html#objectives",
    "title": "15  Keywords-in-Context",
    "section": "",
    "text": "What are “keywords”.\n\nThe notion of a “concordance”\nUsing patterns and different input types\nMulti-word patterns\nUsing kwic objects for subsequent input - to corpus(), or to textplot_xray()\nPreview: Statistical detection of key words",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html#methods",
    "href": "15-exploring-kwic.html#methods",
    "title": "15  Keywords-in-Context",
    "section": "15.2 Methods",
    "text": "15.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html#examples",
    "href": "15-exploring-kwic.html#examples",
    "title": "15  Keywords-in-Context",
    "section": "15.3 Examples",
    "text": "15.3 Examples\nUsing kwic() to verify a dictionary usage.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html#issues",
    "href": "15-exploring-kwic.html#issues",
    "title": "15  Keywords-in-Context",
    "section": "15.4 Issues",
    "text": "15.4 Issues\nAdditional issues.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html#further-reading",
    "href": "15-exploring-kwic.html#further-reading",
    "title": "15  Keywords-in-Context",
    "section": "15.5 Further Reading",
    "text": "15.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "15-exploring-kwic.html#exercises",
    "href": "15-exploring-kwic.html#exercises",
    "title": "15  Keywords-in-Context",
    "section": "15.6 Exercises",
    "text": "15.6 Exercises\nAdd some here.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Keywords-in-Context</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html",
    "href": "16-exploring-dictionaries.html",
    "title": "16  Applying Dictionaries",
    "section": "",
    "text": "16.1 Objectives\nCreating and revising dictionaries.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html#objectives",
    "href": "16-exploring-dictionaries.html#objectives",
    "title": "16  Applying Dictionaries",
    "section": "",
    "text": "The basic idea of counting token and feature matches as equivalence classes\nMatching phrases\nWeighting dictionary applications\nMulti-lingual applications\nScaling dictionary results\nConfidence intervals and bootstrapping\nDictionary validation using kwic()\nDictionary construction using textstat_keyness()\n\n\n\nkey-value lists\nmore details on pattern types\nthe phrase() argument\nimporting foreign dictionaries\nediting dictionaries - using dictionaries as pattern inputs to other functions\nexporting dictionaries\nusing dictionaries: tokens_lookup()",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html#methods",
    "href": "16-exploring-dictionaries.html#methods",
    "title": "16  Applying Dictionaries",
    "section": "16.2 Methods",
    "text": "16.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html#examples",
    "href": "16-exploring-dictionaries.html#examples",
    "title": "16  Applying Dictionaries",
    "section": "16.3 Examples",
    "text": "16.3 Examples\nSentiment analysis.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html#issues",
    "href": "16-exploring-dictionaries.html#issues",
    "title": "16  Applying Dictionaries",
    "section": "16.4 Issues",
    "text": "16.4 Issues\nCopyright.\nWhich format?\nWeighted dictionaries.\nAdditional issues.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html#further-reading",
    "href": "16-exploring-dictionaries.html#further-reading",
    "title": "16  Applying Dictionaries",
    "section": "16.5 Further Reading",
    "text": "16.5 Further Reading\nOn-line sources of dictionaries.\nQI’s quanteda.dictionaries package.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "16-exploring-dictionaries.html#exercises",
    "href": "16-exploring-dictionaries.html#exercises",
    "title": "16  Applying Dictionaries",
    "section": "16.6 Exercises",
    "text": "16.6 Exercises\nAdd some here.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Applying Dictionaries</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html",
    "href": "17-exploring-frequencies.html",
    "title": "17  Most Frequent Words",
    "section": "",
    "text": "17.1 Objectives",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html#objectives",
    "href": "17-exploring-frequencies.html#objectives",
    "title": "17  Most Frequent Words",
    "section": "",
    "text": "Summarizing features via a dfm()\nMost frequent features\ndfm_group() revisited\nUsing dictionaries to count keys\ntopfeatures()\ntextplot_wordcloud()\ntextstat_frequency()\ntextstat_keyness(), textplot_keyness()",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html#methods",
    "href": "17-exploring-frequencies.html#methods",
    "title": "17  Most Frequent Words",
    "section": "17.2 Methods",
    "text": "17.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html#examples",
    "href": "17-exploring-frequencies.html#examples",
    "title": "17  Most Frequent Words",
    "section": "17.3 Examples",
    "text": "17.3 Examples\nUsing kwic() to verify a dictionary usage.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html#issues",
    "href": "17-exploring-frequencies.html#issues",
    "title": "17  Most Frequent Words",
    "section": "17.4 Issues",
    "text": "17.4 Issues\nAdditional issues.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html#further-reading",
    "href": "17-exploring-frequencies.html#further-reading",
    "title": "17  Most Frequent Words",
    "section": "17.5 Further Reading",
    "text": "17.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "17-exploring-frequencies.html#exercises",
    "href": "17-exploring-frequencies.html#exercises",
    "title": "17  Most Frequent Words",
    "section": "17.6 Exercises",
    "text": "17.6 Exercises\nAdd some here.",
    "crumbs": [
      "Exploring and Describing Texts",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Most Frequent Words</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html",
    "href": "18-comparing-sophistication.html",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "",
    "text": "18.1 Objectives",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html#objectives",
    "href": "18-comparing-sophistication.html#objectives",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "",
    "text": "Sentence length\nSyllables\nTypes versus tokens\nUsage in word lists\nLexical diversity\nReadability\nBootstrapping methods and uncertainty accounting",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html#methods",
    "href": "18-comparing-sophistication.html#methods",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.2 Methods",
    "text": "18.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html#examples",
    "href": "18-comparing-sophistication.html#examples",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.3 Examples",
    "text": "18.3 Examples\nSentiment analysis.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html#issues",
    "href": "18-comparing-sophistication.html#issues",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.4 Issues",
    "text": "18.4 Issues\nNon-English languages.\nSampling.\nText length and its effect on diversity. Zipf’s law and Heap’s law.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html#further-reading",
    "href": "18-comparing-sophistication.html#further-reading",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.5 Further Reading",
    "text": "18.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "18-comparing-sophistication.html#exercises",
    "href": "18-comparing-sophistication.html#exercises",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.6 Exercises",
    "text": "18.6 Exercises\nAdd some here.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Profiling Lexical Patterns and Usage</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html",
    "href": "19-comparing-documents.html",
    "title": "19  Document Similarity and Distance",
    "section": "",
    "text": "19.1 Objectives",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html#objectives",
    "href": "19-comparing-documents.html#objectives",
    "title": "19  Document Similarity and Distance",
    "section": "",
    "text": "Distance and similarity measures\nMeasuring similarity\nMeasuring distance\nClustering\nMulti-dimensional scaling\nNetwork analysis of document connections",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html#methods",
    "href": "19-comparing-documents.html#methods",
    "title": "19  Document Similarity and Distance",
    "section": "19.2 Methods",
    "text": "19.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html#examples",
    "href": "19-comparing-documents.html#examples",
    "title": "19  Document Similarity and Distance",
    "section": "19.3 Examples",
    "text": "19.3 Examples\nExamples here.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html#issues",
    "href": "19-comparing-documents.html#issues",
    "title": "19  Document Similarity and Distance",
    "section": "19.4 Issues",
    "text": "19.4 Issues\nWeighting and feature selection and its effects on similarity and distance.\nComputational issues.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html#further-reading",
    "href": "19-comparing-documents.html#further-reading",
    "title": "19  Document Similarity and Distance",
    "section": "19.5 Further Reading",
    "text": "19.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "19-comparing-documents.html#exercises",
    "href": "19-comparing-documents.html#exercises",
    "title": "19  Document Similarity and Distance",
    "section": "19.6 Exercises",
    "text": "19.6 Exercises\nAdd some here.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Document Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html",
    "href": "20-comparing-features.html",
    "title": "20  Feature Similarity and Distance",
    "section": "",
    "text": "20.1 Objectives",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html#objectives",
    "href": "20-comparing-features.html#objectives",
    "title": "20  Feature Similarity and Distance",
    "section": "",
    "text": "Distance and similarity measures revisited\nClustering\nNetwork analysis of feature connections\nImproving feature comparisons through detection and pre-processing of multi-word expressions",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html#methods",
    "href": "20-comparing-features.html#methods",
    "title": "20  Feature Similarity and Distance",
    "section": "20.2 Methods",
    "text": "20.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html#examples",
    "href": "20-comparing-features.html#examples",
    "title": "20  Feature Similarity and Distance",
    "section": "20.3 Examples",
    "text": "20.3 Examples\nExamples here.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html#issues",
    "href": "20-comparing-features.html#issues",
    "title": "20  Feature Similarity and Distance",
    "section": "20.4 Issues",
    "text": "20.4 Issues\nComputational issues.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html#further-reading",
    "href": "20-comparing-features.html#further-reading",
    "title": "20  Feature Similarity and Distance",
    "section": "20.5 Further Reading",
    "text": "20.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "20-comparing-features.html#exercises",
    "href": "20-comparing-features.html#exercises",
    "title": "20  Feature Similarity and Distance",
    "section": "20.6 Exercises",
    "text": "20.6 Exercises\nAdd some here.",
    "crumbs": [
      "Statistics for Comparing Texts",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Feature Similarity and Distance</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html",
    "href": "21-ml-supervised-scaling.html",
    "title": "21  Supervised Document Scaling",
    "section": "",
    "text": "21.1 Objectives",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html#objectives",
    "href": "21-ml-supervised-scaling.html#objectives",
    "title": "21  Supervised Document Scaling",
    "section": "",
    "text": "Introduction to the notion of document scaling. Difference from classification, which we cover in Chapter 23.\nSupervised versus unsupervised methods\nWordscores\nClass affinity model",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html#methods",
    "href": "21-ml-supervised-scaling.html#methods",
    "title": "21  Supervised Document Scaling",
    "section": "21.2 Methods",
    "text": "21.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html#examples",
    "href": "21-ml-supervised-scaling.html#examples",
    "title": "21  Supervised Document Scaling",
    "section": "21.3 Examples",
    "text": "21.3 Examples\nExamples here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html#issues",
    "href": "21-ml-supervised-scaling.html#issues",
    "title": "21  Supervised Document Scaling",
    "section": "21.4 Issues",
    "text": "21.4 Issues\nTraining documents and how to select them.\nUncertainty accounting.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html#further-reading",
    "href": "21-ml-supervised-scaling.html#further-reading",
    "title": "21  Supervised Document Scaling",
    "section": "21.5 Further Reading",
    "text": "21.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "21-ml-supervised-scaling.html#exercises",
    "href": "21-ml-supervised-scaling.html#exercises",
    "title": "21  Supervised Document Scaling",
    "section": "21.6 Exercises",
    "text": "21.6 Exercises\nAdd some here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Supervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html",
    "href": "22-ml-unsupervised-scaling.html",
    "title": "22  Unsupervised Document Scaling",
    "section": "",
    "text": "22.1 Objectives",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#objectives",
    "href": "22-ml-unsupervised-scaling.html#objectives",
    "title": "22  Unsupervised Document Scaling",
    "section": "",
    "text": "Notion of unsupervised scaling, based on distance\nLSA\nCorrespondence analysis\nThe “wordfish” model",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#methods",
    "href": "22-ml-unsupervised-scaling.html#methods",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.2 Methods",
    "text": "22.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#examples",
    "href": "22-ml-unsupervised-scaling.html#examples",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.3 Examples",
    "text": "22.3 Examples\nExamples here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#issues",
    "href": "22-ml-unsupervised-scaling.html#issues",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.4 Issues",
    "text": "22.4 Issues\nThe hazards of ex post interpretation of scaling.\nDimensionality.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#further-reading",
    "href": "22-ml-unsupervised-scaling.html#further-reading",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.5 Further Reading",
    "text": "22.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#exercises",
    "href": "22-ml-unsupervised-scaling.html#exercises",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.6 Exercises",
    "text": "22.6 Exercises\nAdd some here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Unsupervised Document Scaling</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html",
    "href": "23-ml-classifiers.html",
    "title": "23  Methods for Text Classification",
    "section": "",
    "text": "23.1 Objectives",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html#objectives",
    "href": "23-ml-classifiers.html#objectives",
    "title": "23  Methods for Text Classification",
    "section": "",
    "text": "Class prediction versus scaling, and the notion of predicting classes\nNaive Bayes\nSVMs\nMore advanced methods\nFeature selection for improving prediction\nAssessing performance",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html#methods",
    "href": "23-ml-classifiers.html#methods",
    "title": "23  Methods for Text Classification",
    "section": "23.2 Methods",
    "text": "23.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html#examples",
    "href": "23-ml-classifiers.html#examples",
    "title": "23  Methods for Text Classification",
    "section": "23.3 Examples",
    "text": "23.3 Examples\nExamples here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html#issues",
    "href": "23-ml-classifiers.html#issues",
    "title": "23  Methods for Text Classification",
    "section": "23.4 Issues",
    "text": "23.4 Issues\nWhy kNN is poor choice.\nWhich classifier?\nngrams or unigrams?",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html#further-reading",
    "href": "23-ml-classifiers.html#further-reading",
    "title": "23  Methods for Text Classification",
    "section": "23.5 Further Reading",
    "text": "23.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "23-ml-classifiers.html#exercises",
    "href": "23-ml-classifiers.html#exercises",
    "title": "23  Methods for Text Classification",
    "section": "23.6 Exercises",
    "text": "23.6 Exercises\nAdd some here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Methods for Text Classification</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html",
    "href": "24-ml-topicmodels.html",
    "title": "24  Topic modelling",
    "section": "",
    "text": "24.1 Objectives",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html#objectives",
    "href": "24-ml-topicmodels.html#objectives",
    "title": "24  Topic modelling",
    "section": "",
    "text": "Latent Dirichlet Allocation\nOlder methods (e.g. LSA)\nAdvanced methods\nUsing the stm package\nPre-processing steps and their importance, including compounding MWEs",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html#methods",
    "href": "24-ml-topicmodels.html#methods",
    "title": "24  Topic modelling",
    "section": "24.2 Methods",
    "text": "24.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html#examples",
    "href": "24-ml-topicmodels.html#examples",
    "title": "24  Topic modelling",
    "section": "24.3 Examples",
    "text": "24.3 Examples\nExamples here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html#issues",
    "href": "24-ml-topicmodels.html#issues",
    "title": "24  Topic modelling",
    "section": "24.4 Issues",
    "text": "24.4 Issues\nUsing other topic model packages",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html#further-reading",
    "href": "24-ml-topicmodels.html#further-reading",
    "title": "24  Topic modelling",
    "section": "24.5 Further Reading",
    "text": "24.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "24-ml-topicmodels.html#exercises",
    "href": "24-ml-topicmodels.html#exercises",
    "title": "24  Topic modelling",
    "section": "24.6 Exercises",
    "text": "24.6 Exercises\nAdd some here.",
    "crumbs": [
      "Machine Learning for Texts",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Topic modelling</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html",
    "href": "25-lms-linguistic-parsing.html",
    "title": "25  Advanced NLP using R",
    "section": "",
    "text": "25.1 Objectives",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html#objectives",
    "href": "25-lms-linguistic-parsing.html#objectives",
    "title": "25  Advanced NLP using R",
    "section": "",
    "text": "What is “NLP” and how does it differ from what we have covered so far?\nPart of speech tagging\nDifferent schemes for part of speech tagging\nDifferentiating homographs based on POS\nPOS tagging in other languages\nNamed Entity Recognition\nNoun phrase extraction\nDependency parsing to extract syntactic relations",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html#methods",
    "href": "25-lms-linguistic-parsing.html#methods",
    "title": "25  Advanced NLP using R",
    "section": "25.2 Methods",
    "text": "25.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html#examples",
    "href": "25-lms-linguistic-parsing.html#examples",
    "title": "25  Advanced NLP using R",
    "section": "25.3 Examples",
    "text": "25.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html#issues",
    "href": "25-lms-linguistic-parsing.html#issues",
    "title": "25  Advanced NLP using R",
    "section": "25.4 Issues",
    "text": "25.4 Issues\nAlternative NLP tools. (Here we focus on spacyr.)\nOther languages.\nTraining new models.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html#further-reading",
    "href": "25-lms-linguistic-parsing.html#further-reading",
    "title": "25  Advanced NLP using R",
    "section": "25.5 Further Reading",
    "text": "25.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "25-lms-linguistic-parsing.html#exercises",
    "href": "25-lms-linguistic-parsing.html#exercises",
    "title": "25  Advanced NLP using R",
    "section": "25.6 Exercises",
    "text": "25.6 Exercises\nAdd some here.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html",
    "href": "26-lms-embeddings.html",
    "title": "26  Word Embedding Models",
    "section": "",
    "text": "26.1 Objectives",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html#objectives",
    "href": "26-lms-embeddings.html#objectives",
    "title": "26  Word Embedding Models",
    "section": "",
    "text": "Understanding what are word embeddings\nMethods: word2vec, doc2vec, GLoVe, SVD/LSA\nConstructing a feature-co-occurrence matrix\nFitting a word embedding model\nUsing pre-trained embedding scores with documents\nPrediction based on embeddings\nFurther methods",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html#methods",
    "href": "26-lms-embeddings.html#methods",
    "title": "26  Word Embedding Models",
    "section": "26.2 Methods",
    "text": "26.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html#examples",
    "href": "26-lms-embeddings.html#examples",
    "title": "26  Word Embedding Models",
    "section": "26.3 Examples",
    "text": "26.3 Examples\nExamples here.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html#issues",
    "href": "26-lms-embeddings.html#issues",
    "title": "26  Word Embedding Models",
    "section": "26.4 Issues",
    "text": "26.4 Issues\nIssues here.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html#further-reading",
    "href": "26-lms-embeddings.html#further-reading",
    "title": "26  Word Embedding Models",
    "section": "26.5 Further Reading",
    "text": "26.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "26-lms-embeddings.html#exercises",
    "href": "26-lms-embeddings.html#exercises",
    "title": "26  Word Embedding Models",
    "section": "26.6 Exercises",
    "text": "26.6 Exercises\nAdd some here.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Word Embedding Models</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html",
    "href": "27-lms-transformers.html",
    "title": "27  Advanced NLP using R",
    "section": "",
    "text": "27.1 Objectives",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html#objectives",
    "href": "27-lms-transformers.html#objectives",
    "title": "27  Advanced NLP using R",
    "section": "",
    "text": "POS tagging\nNamed Entity Recognition\nsentiment analysis\nclustering of words or documents\nclassification\ntopic models",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html#methods",
    "href": "27-lms-transformers.html#methods",
    "title": "27  Advanced NLP using R",
    "section": "27.2 Methods",
    "text": "27.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html#examples",
    "href": "27-lms-transformers.html#examples",
    "title": "27  Advanced NLP using R",
    "section": "27.3 Examples",
    "text": "27.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html#issues",
    "href": "27-lms-transformers.html#issues",
    "title": "27  Advanced NLP using R",
    "section": "27.4 Issues",
    "text": "27.4 Issues\nAlternative NLP tools. (Here we focus on spacyr.)\nOther languages.\nTraining new models.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html#further-reading",
    "href": "27-lms-transformers.html#further-reading",
    "title": "27  Advanced NLP using R",
    "section": "27.5 Further Reading",
    "text": "27.5 Further Reading\nAdditional resources from libraries or the web.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "27-lms-transformers.html#exercises",
    "href": "27-lms-transformers.html#exercises",
    "title": "27  Advanced NLP using R",
    "section": "27.6 Exercises",
    "text": "27.6 Exercises\nAdd some here.",
    "crumbs": [
      "Incorporating Language Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced NLP using R</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html",
    "href": "28-further-tidy.html",
    "title": "28  Integrating “tidy” approaches",
    "section": "",
    "text": "28.1 Objectives",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html#objectives",
    "href": "28-further-tidy.html#objectives",
    "title": "28  Integrating “tidy” approaches",
    "section": "",
    "text": "What is the “tidy” approach and how does it apply to text?\nThe tidytext package and its advantages\nSwitching from quanteda to tidytext (and back)\nAdvantages of non-tidy text objects",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html#methods",
    "href": "28-further-tidy.html#methods",
    "title": "28  Integrating “tidy” approaches",
    "section": "28.2 Methods",
    "text": "28.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html#examples",
    "href": "28-further-tidy.html#examples",
    "title": "28  Integrating “tidy” approaches",
    "section": "28.3 Examples",
    "text": "28.3 Examples\nExamples of each of the above methods.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html#issues",
    "href": "28-further-tidy.html#issues",
    "title": "28  Integrating “tidy” approaches",
    "section": "28.4 Issues",
    "text": "28.4 Issues\nEfficiency. Complexity.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html#further-reading",
    "href": "28-further-tidy.html#further-reading",
    "title": "28  Integrating “tidy” approaches",
    "section": "28.5 Further Reading",
    "text": "28.5 Further Reading\nTidy resources. Tidytext book. Data Science Using R (Wickham and Grolemund).",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "28-further-tidy.html#exercises",
    "href": "28-further-tidy.html#exercises",
    "title": "28  Integrating “tidy” approaches",
    "section": "28.6 Exercises",
    "text": "28.6 Exercises\nAdd some here.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Integrating \"tidy\" approaches</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html",
    "href": "29-further-hardlanguages.html",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "",
    "text": "29.1 Objectives",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html#objectives",
    "href": "29-further-hardlanguages.html#objectives",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "",
    "text": "Non-English characters, and encoding issues\nSegmentation in languages that do not use whitespace delimiters between words\nRight-to-left languages\nEmoji",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html#methods",
    "href": "29-further-hardlanguages.html#methods",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "29.2 Methods",
    "text": "29.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html#examples",
    "href": "29-further-hardlanguages.html#examples",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "29.3 Examples",
    "text": "29.3 Examples\nChinese, Japanese, Korean.\nHindi, Georgian.\nArabic and other RTL languages.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html#issues",
    "href": "29-further-hardlanguages.html#issues",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "29.4 Issues",
    "text": "29.4 Issues\nStemming, syllables, character counts may be off.\nFont issues.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html#further-reading",
    "href": "29-further-hardlanguages.html#further-reading",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "29.5 Further Reading",
    "text": "29.5 Further Reading\nFurther reading here.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "29-further-hardlanguages.html#exercises",
    "href": "29-further-hardlanguages.html#exercises",
    "title": "29  Text analysis in “Hard” Languages",
    "section": "29.6 Exercises",
    "text": "29.6 Exercises\nAdd some here.",
    "crumbs": [
      "Further Issues",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Text analysis in \"Hard\" Languages</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bécue-Bertaut, Monica. 2019. Textual Data Science with\nR. CRC Press.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In\nHandbook of Research Methods in Political Science and International\nRelations, edited by Luigi Curini and Robert Franzese, 461–97.\nThousand Oaks: Sage.\n\n\nBenoit, Kenneth, and Akitaka Matsuo. 2020. Spacyr: Wrapper to the\n’spaCy’ ’NLP’ Library. https://CRAN.R-project.org/package=spacyr.\n\n\nBenoit, Kenneth, Kevin Munger, and Arthur Spirling. 2019.\n“Measuring and Explaining Political Sophistication Through Textual\nComplexity.” American Journal of Political Science 63\n(2): 491–508. https://doi.org/10.1111/ajps.12423.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An\nR Package for the Quantitative Analysis of Textual\nData.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBoussalis, Constantine, Travis G. Coan, Mirya R. Holman, and Stefan\nMüller. 2021. “Gender, Candidate Emotional Expression, and Voter\nReactions During Televised Debates.” American Political\nScience Review 115 (4): 1242–57. https://doi.org/10.1017/S0003055421000666.\n\n\nCastanho Silva, Bruno, and Sven-Oliver Proksch. 2021. “Politicians\nUnleashed? Political Communication on Twitter and in Parliament in\nWestern Europe.” Political Science Research and Methods\npublished ahead of print (doi: 10.1017/psrm.2021.36). https://doi.org/10.1017/psrm.2021.36.\n\n\nChang, Jonathan. 2015. Lda: Collapsed Gibbs Sampling Methods for\nTopic Models. https://CRAN.R-project.org/package=lda.\n\n\nCrisp, Brian F., Benjamin Schneider, Amy Catalinac, and Taishi Muraoka.\n2021. “Capturing Vote-Seeking Incentives and the Cultivation of a\nPersonal and Party Vote.” Electoral Studies 72: 102369.\nhttps://doi.org/10.1016/j.electstud.2021.102369.\n\n\nDenny, Matthew W., and Arthur Spirling. 2018. “Text Preprocessing\nfor Unsupervised Learning: Why It Matters, When It Misleads, and What to\nDo about It.” Political Analysis 26 (2): 168–89. https://doi.org/10.1017/pan.2017.44.\n\n\nEshima, Shusei, Kosuke Imai, and Tomoya Sakasi. 2023. “Keyword\nAssisted Topic Models.” American Journal of Political\nScience online first. https://doi.org/10.1111/ajps.12779.\n\n\nFeinerer, Ingo, Kurt Hornik, and David Meyer. 2008a. “Text Mining\nInfrastructure in R.” Journal of Statistical\nSoftware 25 (5): 1–54. https://doi.org/10.18637/jss.v025.i05.\n\n\n———. 2008b. “Text Mining Infrastructure in R.”\nJournal of Statistical Software 25 (5): 1–54. https://www.jstatsoft.org/v25/i05/.\n\n\nGessler, Theresa, and Sophia Hunger. 2022. “How the Refugee Crisis\nand Radical Right Parties Shape Party Competition on\nImmigration.” Political Science Research and Methods 10\n(3): 524–44. https://doi.org/10.1017/psrm.2021.64.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022.\nText as Data: The Promise and Pitfalls of Automatic Content Analysis\nMethods for Political Texts: A New Framework for Machine Learning and\nthe Social Sciences. Princeton: Princeton University Press.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting\nTopic Models.” Journal of Statistical Software 40 (13):\n1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nHerzog, Alexander, and Kenneth Benoit. 2015. “The Most Unkindest\nCuts: Speaker Selection and Expressed Goverment Dissent During Economic\nCrisis.” The Journal of Politics 77 (4): 1157–75. https://doi.org/10.1086/682670.\n\n\nHonnibal, Matthew, Ines Montani, Sophie Van Landeghem, and Adriane Boyd.\n2020. “spaCy: Industrial-Strength\nNatural Language Processing in Python.” https://doi.org/10.5281/zenodo.1212303.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in r. Boca Raton: CRC Press. https://smltar.com.\n\n\nKwartler, Ted. 2017. Text Mining in Practice with\nR. Chichester: John Wiley & Sons.\n\n\nLupia, Arthur, Stuart N. Soroka, and Alison Beatty. 2020. “What\nDoes Congress Want from the National Science Foundation? A Content\nAnalysis of Remarks from 1995 to 2018.” Science Advances\n6 (33): eaaz6300. https://doi.org/10.1126/sciadv.aaz6300.\n\n\nManning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. 2008.\nAn Introduction to Information Retrieval. New York: Cambridge\nUniversity Press. https://nlp.stanford.edu/IR-book/.\n\n\nMonroe, B. L., K. M. Quinn, and M. P. Colaresi. 2008. “Fightin’\nWords: Lexical Feature Selection and Evaluation for\nIdentifying the Content of Political Conflict.” Political\nAnalysis 16 (4): 372–403.\n\n\nMonroe, Burt L., and Philip A. Schrodt. 2008. “Introduction to the\nSpecial Issue: The Statistical Analysis of Political Text.”\nPolitical Analysis 16 (4): 351–55. https://doi.org/10.1093/pan/mpn017.\n\n\nMosteller, F., and D. L. Wallace. 1963. “Inference in an\nAuthorship Problem.” Journal of the American Statistical\nAssocation 58 (302): 275–309.\n\n\nMullen, Lincoln A., Kenneth Benoit, Os Keyes, Dmitry Selivanov, and\nJeffrey Arnold. 2018. “Fast, Consistent Tokenization of Natural\nLanguage Text.” Journal of Open Source Software 3: 655.\nhttps://doi.org/10.21105/joss.00655.\n\n\nMüller, Stefan. 2022. “The Temporal Focus of Campaign\nCommunication.” The Journal of Politics 84 (1): 585–90.\nhttps://doi.org/10.1086/715165.\n\n\nPang, B., L. Lee, and S. Vaithyanathan. 2002. “Thumbs up?\nSentiment Classification Using Machine Learning Techniques.”\nProceedings of the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 79–86.\n\n\nPorter, Martin F. 2001. “Snowball: A Language for Stemming\nAlgorithms.” In.\n\n\nRauh, Christopher, Bart Joachim Bes, and Martijn Schoonvelde. 2020.\n“Undermining, Defusing or Defending European Integration?\nAssessing Public Communication of European Executives in Times of EU\nPoliticisation.” European Journal of Political Research\n59 (2): 397–423. https://doi.org/10.1111/1475-6765.12350.\n\n\nRoberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2019.\n“stm: An R Package for\nStructural Topic Models.” Journal of Statistical\nSoftware 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nRodriguez, Pedro L., Arthur Spirling, and Brandon Stewart. 2023.\nconText: ’A La Carte’ on Text (ConText) Embedding Regression.\nhttps://CRAN.R-project.org/package=conText.\n\n\nSarica, Serhad, and Jianxi Luo. 2021. “Stopwords in Technical\nLanguage Processing.” PLoS One 16 (8): e0254937. https://doi.org/10.1371/journal.pone.0254937.\n\n\nSchofield, Alexandra, and David Mimno. 2016. “Comparing Apples to\nApple: The Effects of Stemmers on Topic Models.” Transactions\nof the Association for Computational Linguistics 4: 287–300. https://doi.org/10.1162/tacl_a_00099.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and\nAnalysis Using Tidy Data Principles in r.” Journal of Open\nSource Software 1 (3). https://doi.org/10.21105/joss.00037.\n\n\n———. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media,\nInc.\n\n\nSlapin, Jonathan B., and Sven-Oliver Proksch. 2008. “A Scaling\nModel for Estimating Time-Series Party Positions from Texts.”\nAmerican Journal of Political Science 52 (3): 705–22. https://doi.org/10.1111/j.1540-5907.2008.00338.x.\n\n\nTurenne, Nicolas. 2016. Analyse de Données Textuelles\nSous r. ISTE Group.\n\n\nWatanabe, Kohei. 2023. LSX: Semi-Supervised Algorithm for Document\nScaling. https://CRAN.R-project.org/package=LSX.\n\n\nWickham, Hadley. 2021. Rvest: Easily Harvest (Scrape) Web\nPages. https://CRAN.R-project.org/package=rvest.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd ed. Sebastopol: O’Reilly. https://r4ds.hadley.nz.\n\n\nWilbur, W. John, and Karl Sirotkin. 1992. “The Automatic\nIdentification of Stop Words.” Journal of Information\nScience 18 (1): 45–55. https://doi.org/10.1177/01655515920180010.\n\n\nYoung, Lori, and Stuart N. Soroka. 2012. “Affective News: The\nAutomated Coding of Sentiment in Political Texts.” Political\nCommunication 29 (2): 205–31.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "30-appendix-installation.html",
    "href": "30-appendix-installation.html",
    "title": "Appendix A — Installing the Required Tools",
    "section": "",
    "text": "A.1 Objectives",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "30-appendix-installation.html#objectives",
    "href": "30-appendix-installation.html#objectives",
    "title": "Appendix A — Installing the Required Tools",
    "section": "",
    "text": "Installing R\nInstalling RStudio\nInstalling quanteda\nInstalling spacy\nInstalling companion package(s)\nKeeping up to date\nTroubleshooting problems",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "30-appendix-installation.html#installing-r",
    "href": "30-appendix-installation.html#installing-r",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.2 Installing R",
    "text": "A.2 Installing R\nR is a free software environment for statistical computing that runs on numerous platforms, including Windows, macOS, Linux, and Solaris. You can find details at https://www.r-project.org/, and link there to a set of mirror websites for downloading the latest version.\nWe recommend that you always using the latest version of R, which is what we used for compiling this book. There are seldom reasons to use older versions of R, and the R Core Team and the maintainers of the largest repository of R packages, CRAN (for Comprehensive R Archive Network) put an enormous amount of attention and energy into assuring that extension packages work with stably and with one another.\nYou verify which version of R you are using by either viewing the messages on startup, e.g.\nR version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\nor by calling\n\nR.Version()$version.string\n\n[1] \"R version 4.3.2 (2023-10-31)\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "30-appendix-installation.html#installing-recommended-tools-and-packages",
    "href": "30-appendix-installation.html#installing-recommended-tools-and-packages",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.3 Installing recommended tools and packages",
    "text": "A.3 Installing recommended tools and packages\n\nA.3.1 RStudio\nThere are numerous ways of running R, including from the “command line” (“Terminal” on Mac or Linux, or “Command Prompt” on Windows) or from the R Gui console that comes with most installations of R. But our very strong recommendation is to us the outstanding RStudio Dekstop. “IDE” stands for integrated development environment, and provides a combination of file manager, package manager, help viewer, graphics viewer, environment browser, editor, and much more. Before this can be used, however, you must have installed R.\nRStudio can be installed from https://www.rstudio.com/products/rstudio/download/.\n\n\nA.3.2 Additional packages\nThe main package you will need for the examples in this book is quanteda, which provides a framework for the quantitative analysis of textual data. When you install this package, by default it will also install any required packages that quanteda depends on to function (such as stringi that it uses for many essential string handling operations). Because “dependencies” are installed into your local library, these additional packages are also available for you to use independently. You only need to install them once (although they might need updating, see below).\nWe also suggest you install:\n\nreadtext - for reading in documents that contain text, and converting them automatically to plain text;\nspacyr - an R wrapper to the Python package spaCy for natural language processing, including part-of-speech tagging and entity extraction. While we have tried hard to make this automatic, installing and configuring spacyr actually involves some major work under the hood, such as installing a version of Python in a self-contained virtual environment and then installing spaCy and one of its language models.\n\n\n\nA.3.3 Keeping packages up-to-date\nR has an easy method for ensuring you have the latest versions of installed packages:\n\nupdate.packages()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "30-appendix-installation.html#additional-issues",
    "href": "30-appendix-installation.html#additional-issues",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.4 Additional Issues",
    "text": "A.4 Additional Issues\n\nA.4.1 Installing development versions of packages\nThe R packages that you can install using the methods described above are the pre-compiled binary versions that are distributed on CRAN. (Linux installations are the exception, as these are always compiled upon installation.) Sometimes, package developers will publish “development” versions of their packages that have yet to published on CRAN, for instance on the popular GitHub platform hosting the world’s largest collection of open-source software.\nThe quanteda package, for instance, is hosted on GitHub at https://github.com/quanteda/quanteda, where its development version tends to be slightly ahead of the CRAN version. If you are feeling adventurous, or need a new version in which a specific issue or bug has been fixed, you can install it from the GitHub source using:\n\ndevtools::install_github(\"quanteda/quanteda\") \n\nBecause installing from GitHub is the same as installing from the source code, this also involves compiling the C++ and Fortran source code that makes parts of quanteda so fast. For this source installation to work, you will need to have installed the appropriate compilers.\nIf you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN.\nIf you are using macOS, you should install the macOS tools, namely the Clang 6.x compiler and the GNU Fortran compiler (as quanteda requires gfortran to build).1\nLinux always compiles packages containing C++ code upon installation, so if you are using that platform then you are unlikely to need to install any additional components in order to install a development version.\n\n\nA.4.2 Troubleshooting\nMost problems come from not having the latest versions of packages installed, so make sure you have updated them using the instructions above.\nOther problems include: - Lack of permissions to install packages. This might affect Windows users of work laptops, whose workplace prevents user modification of any software. - Lack of internet access, or access being restricted by a firewall or proxy server.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "30-appendix-installation.html#further-reading",
    "href": "30-appendix-installation.html#further-reading",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.5 Further Reading",
    "text": "A.5 Further Reading\nHadley Wickham’s excellent book R Packages is well worth reading.\n\nWickham, Hadley. (2015). R packages: organize, test, document, and share your code. O’Reilly Media, Inc.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "30-appendix-installation.html#footnotes",
    "href": "30-appendix-installation.html#footnotes",
    "title": "Appendix A — Installing the Required Tools",
    "section": "",
    "text": "If you are still getting errors related to gfortran, follow the fixes here.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing the Required Tools</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html",
    "href": "31-appendix-encoding.html",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "",
    "text": "B.1 Objectives",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html#objectives",
    "href": "31-appendix-encoding.html#objectives",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "",
    "text": "The concept of encoding\nClassic 8-bit encodings\nHow to detect encodings\nConverting files\nConverting text once loaded\nEncoding “bit” versus actual text encoding\nWhen you are likely to encounter problems (with what sorts of files)\nUnicode encodings (UTF-8, UTF-16)\nHow to avoid encoding headaches forever",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html#methods",
    "href": "31-appendix-encoding.html#methods",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.2 Methods",
    "text": "B.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html#examples",
    "href": "31-appendix-encoding.html#examples",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.3 Examples",
    "text": "B.3 Examples\nExamples here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html#issues",
    "href": "31-appendix-encoding.html#issues",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.4 Issues",
    "text": "B.4 Issues\nUTF-16 in some operating systems (Windows).\nEditors that are “encoding smart”.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html#further-reading",
    "href": "31-appendix-encoding.html#further-reading",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.5 Further Reading",
    "text": "B.5 Further Reading\nFurther reading here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "31-appendix-encoding.html#exercises",
    "href": "31-appendix-encoding.html#exercises",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.6 Exercises",
    "text": "B.6 Exercises\nAdd some here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Everything You Never Wanted to Know about Encoding</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html",
    "href": "32-appendix-regex.html",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "",
    "text": "C.1 Objectives",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html#objectives",
    "href": "32-appendix-regex.html#objectives",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "",
    "text": "Basic of regular expressions\nFixed matching and “globs” and relationship to regexes\nBase regular expressions versus stringi\ncharacter classes\nUnicode character categories\nLookaheads",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html#methods",
    "href": "32-appendix-regex.html#methods",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.2 Methods",
    "text": "C.2 Methods\nApplicable methods for the objectives listed above.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html#examples",
    "href": "32-appendix-regex.html#examples",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.3 Examples",
    "text": "C.3 Examples\nExamples here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html#issues",
    "href": "32-appendix-regex.html#issues",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.4 Issues",
    "text": "C.4 Issues\nOlder variants: PCRE, versus GNU, versus stringi implementations.\nShortcut names versus Unicode categories.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html#further-reading",
    "href": "32-appendix-regex.html#further-reading",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.5 Further Reading",
    "text": "C.5 Further Reading\nFurther reading here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  },
  {
    "objectID": "32-appendix-regex.html#exercises",
    "href": "32-appendix-regex.html#exercises",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.6 Exercises",
    "text": "C.6 Exercises\nAdd some here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>A Survival Guide to Regular Expressions</span>"
    ]
  }
]