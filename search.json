[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Text Analysis Using R",
    "section": "Welcome",
    "text": "Welcome\nThis is the draft version of Text Analysis Using R. This book offers a comprehensive practical guide to text analysis and natural language processing using the R language. We have pitched the book at those already familiar with some R, but we also provide a gentle enough introduction that it is also suitable for newcomers to R.\nYou’ll learn how to prepare your texts for analysis, how to analyse texts for insight using statistical methods and machine learning, and how to present those results using graphical methods. Each chapter covers a distinct topic, fist presenting the methodology underlying each topic, and then providing practical examples using R. We also discuss advanced issues facing each method and its application. Finally, for those engaged in self-learning or wishing to use this book for instruction, we provide practical exercises for each chapter.\nThe book is organised into parts, starting with a review of R and especially the R packages and functions relevant to text analysis. If you are already comfortable with R you can skim or skip that section and proceed straight to part two.\n\n\n\n\n\n\nThis book is a work in progress. We will publish chapters as we write them, and open up the GitHub source repository for readers to make comments or even suggest corrections. You can view this at this https://github.com/quanteda/Text-Analysis-Using-R."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Text Analysis Using R",
    "section": "License",
    "text": "License\nWe will eventually seek a publisher for this book, but want to write it first. In the meantime we retaining full copyright and licensing it only to be free to read.\n© Kenneth Benoit and Stefan Müller all rights reserved."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "\\[\n\\def\\quanteda{{\\bf quanteda}}\n\\]\nThe uses and means for analysing text, especially using quantitative and computational approaches, have exploded in recent years across the fields of academic, industry, policy, and other forms of research and analysis. Text mining and text analysis have become the focus of a major methodological wave of innovation in the social sciences, especially political science but also extending to finance, media and communications, and sociology. In non-academic research or industry, furthermore, text mining applications are found in almost every sector, given the ubiquity of text and the need to analyse it.\n“Text analysis” is a broad label for a wide range of tools applied to textual data. It encompasses all manner of methods for turning unstructured raw data in the form of natural language documents into structured data that can be analysed systematically and using specific quantitative approaches. Text analytic methods include descriptive analysis, keyword analysis, topic analysis, measurement and scaling, clustering, text and vocabulary comparisons, or sentiment analysis. It may include causal analysis or predictive modelling. In their most advanced current forms, these methods extend to the natural language generation models that power the newest generation of artificial intelligence systems.\nThis book provides a practical guide introducing the fundamentals of text analysis methods and how to implement them using the R programming language. It covers a wide range of topics to provide a useful resource for a range of students from complete beginners to experienced users of R wanting to learn more advanced techniques. Our emphasis is on the text analytic workflow as a whole, ranging from an introduction to text manipulation in R, all the way through to advanced machine learning methods for textual data."
  },
  {
    "objectID": "preface.html#why-another-book-on-text-mining-in-r",
    "href": "preface.html#why-another-book-on-text-mining-in-r",
    "title": "Preface",
    "section": "Why another book on text mining in R?",
    "text": "Why another book on text mining in R?\nText mining tools for R have existed for many years, led by the venerable tm package (Feinerer, Hornik, and Meyer 2008). In 2015, the first version of quanteda (Benoit et al. 2018) was published on CRAN. Since then, the package has undergone three major versions, with each release improving its consistency, power, and usability. Starting with version 2, quanteda split into a series of packages designed around its different functions (such as plotting, statistics, or machine learning), with quanteda retaining the core text processing functions. Other popular alternatives exist, such as tidytext (Silge and Robinson 2016), although as we explain in Chapter 27, quanteda works perfectly well with the R “tidyverse” and related approaches to text analysis.\nThis is hardly the only book covering how to work with text in R. Silge and Robinson (2017) introduced the tidytext package and numerous examples for mining text using the tidyverse approach to programming in R, including keyword analysis, mining word associations, and topic modelling. Kwartler (2017) provides a good coverage of text mining workflows, plus methods for text visualisations, sentiment scoring, clustering, and classification, as well as methods for sourcing and manipulating source documents. Earlier works include Turenne (2016) and Bécue-Bertaut (2019).\n\n\n\n\n\n\nOne of the two hard things in computer science: Naming things\n\n\n\nWhere does does the package name come from? quanteda is a portmanteau name indicating the purpose of the package, which is the quantitative analysis of textual data.\n\n\nSo why another book? First, the R ecosystem for text analysis is rapidly evolving, with the publication in recent years of massively improved, specialist text analysis packages such as quanteda. None of the earlier books covers this amazing family of packages. In addition, the field of text analysis methodologies has also advanced rapidly, including machine learning approaches based on artificial neural networks and deep learning. These and the packages that make use of them—for instance spacyr (Benoit and Matsuo 2020) for harnessing the power of the spaCy natural language processing library for Python (Honnibal et al. 2020)—have yet to be presented in a systematic, book-length treatment. Furthermore, as we are the authors and creators of many of the packages we cover in this book, we view this as the authoritative reference and how-to guide for using these packages."
  },
  {
    "objectID": "preface.html#what-to-expect-from-this-book",
    "href": "preface.html#what-to-expect-from-this-book",
    "title": "Preface",
    "section": "What to Expect from This Book",
    "text": "What to Expect from This Book\nThis book is meant to be as a practical resource for those confronting the practical challenges of text analysis for the first time, focusing on how to do this in R. Our main focus is on the quanteda package and its extensions, although we also cover more general issues including a brief overview of the R functions required to get started quickly with practical text analysis We cover an introduction to the R language, for\nBenoit (2020) provides a detailed overview of the analysis of textual data, and what distinguishes textual data from other forms of data. It also clearly articulates what is meant by treating text “as data” for analysis. This book and the approaches it presents are firmly geared toward this mindset.\nEach chapter is structured so to provide a continuity across each topic. For each main subject explained in a chapter, we clearly explain the objective of the chapter, then describe the text analytic methods in an applied fashion so that readers are aware of the workings of the method. We then provide practical examples, with detailed working code in R as to how to implement the method. Next, we identify any special issues involved in correctly applying the method, including how to hand the more complicated situations that may arise in practice. Finally, we provide further reading for readers wishing to learn more, and exercises for those wishing for hands-on practice, or for assigning these when using them in teaching environment.\nWe have years of experience in teaching this material in many practical short courses, summer schools, and regular university courses. We have drawn extensively from this experience in designing the overall scope of this book and the structure of each chapter. Our goal is to make the book is suitable for self-learning or to form the basis for teaching and learning in a course on applied text analysis. Indeed, we have partly written this book to assign when teaching text analysis in our own curricula."
  },
  {
    "objectID": "preface.html#who-this-book-is-for",
    "href": "preface.html#who-this-book-is-for",
    "title": "Preface",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nWe don’t assume any previous knowledge of text mining—indeed, the goal of this book is to provide that, from a foundation through to some very advanced topics. Getting use from this book does not require a pre-existing familiarity with R, although, as the slogan goes, “every little helps”. In Part I we cover some of the basics of R and how to make use of it for text analysis specifically. Readers will also learn more of R through our extensive examples. However, experience in teaching and presenting this material tells us that a foundation of R will enable readers to advance through the applications far more rapidly than if they were learning R from scratch at the same time that they take the plunge into the possibly strange new world of text analysis.\nWe are both academics, although we also have experience working in industry or in applying text analysis for non-academic purposes. The typical reader may be a student of text analysis in the literal sense (of being an student) or in the general sense of someone studying techniques in order to improve their practical and conceptual knowledge. Our orientation as social scientists, with a specialization in political text and political communications and media. But this book is for everyone: social scientists, computer scientists, scholars of digital humanities, researchers in marketing and management, and applied researchers working in policy, government, or business fields. This book is written to have the credibility and scholarly rigour (for referencing methods, for instance) needed by academic readers, but is designed to be written in a straightforward, jargon-free (as much as we were able!) manner to be of maximum practical use to non-academic analysts as well."
  },
  {
    "objectID": "preface.html#how-the-book-is-structured",
    "href": "preface.html#how-the-book-is-structured",
    "title": "Preface",
    "section": "How the Book is Structured",
    "text": "How the Book is Structured\n\nSections\nThe book is divided into seven sections. These group topics that we feel represent common stages of learning in text analysis, or similar groups of topics that different users will be drawn too. By grouping stages of learning, we make it possible also for intermediate or advanced users to jump to the section that interests them most, or to the sections where they feel they need additional learning.\nOur sections are:\n\n(Working in R): This section is designed for beginners to learn quickly the R required for the techniques we cover in the book, and to guide them in learning a proper R workflow for text analysis.\nAcquiring texts: Often described (by us at least) as the hardest problem in text analysis, we cover how to source documents, including from Internet sources, and to import these into R as part of the quantitative text analysis pipeline.\nManaging textual data using quanteda: In this section, we introduce the quanteda package, and cover each stage of textual data processing, from creating structured corpora, to tokenisation, and building matrix representations of these tokens. WE also talk about how to build and manage structured lexical resources such as dictionaries and stop word lists.\nExploring and describing texts: How to get overviews of texts using summary statistics, exploring texts using keywords-in-context, extracting target words, and identifying key words.\nStatistics for comparing texts: How to characterise documents in terms of their lexical diversity. readability, similarity, or distance.\nMachine learning for texts: How to apply scaling models, predictive models, and classification models to textual matrices.\nFurther methods for texts: Advanced methods including the use of natural language models to annotate texts, extract entities, or use word embeddings; integrating quanteda with “tidy” data approaches; and how to apply text analysis to “hard” languages such as Chinese (hard because of the high dimensional character set and the lack of whitespace to delimit words).\n\nFinally, in several appendices, we provide more detail about some tricky subjects, such as text encoding formats and working with regular expressions.\n\n\nChapter structure\nOur approach in each chapter is split into the following components, which we apply in every chapter:\n\nObjectives. We explain the purpose of each chapter and what we believe are the most important learning outcomes.\nMethods. We clearly explain the methodological elements of each chapter, through a combination of high-level explanations, formulas, and references.\nExamples. We use practical examples, with R code, demonstrating how to apply the methods to realise the objectives.\nIssues. We identify any special issues, potential problems, or additional approaches that a user might face when applying the methods to their text analysis problem.\nFurther Reading. In part because our scholarly backgrounds compel us to do so, and in part because we know that many readers will want to read more about each method, each chapter contains its own set of references and further readings.\nExercises. For those wishing additional practice or to use this text as a teaching resource (which we strongly encourage!), we provide exercises that can be assigned for each chapter.\n\nThroughout the book, we will demonstrate with examples and build models using a selection of text data sets. A description of these data sets can be found in Appendix A.\n\n\nConventions\nThroughout the book we use several kinds of info boxes to call your attention to information, cautions, and warnings.\n\n\n\n\n\n\nNote\n\n\n\nThe information icon signals a note or a reference to further information.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTips provide suggestions for better ways of doing things.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe exclamation mark icon signals an important point to consider.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning icons flag things you definitely want to avoid.\n\n\nAs you may already have noticed, we put names of R packages in boldface.\nCode blocks will be self-evident, and will look like this, with the output produced from executing those commands shown below the highlighted code in a mono-spaced font.\n\nlibrary(\"quanteda\")\n\nPackage version: 3.2.2\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 10 of 10 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\ndata_corpus_inaugural[1:3]\n\nCorpus consisting of 3 documents and 4 docvars.\n1789-Washington :\n\"Fellow-Citizens of the Senate and of the House of Representa...\"\n\n1793-Washington :\n\"Fellow citizens, I am again called upon by the voice of my c...\"\n\n1797-Adams :\n\"When it was first perceived, in early times, that no middle ...\"\n\n\nWe love the pipe operator in R (when used with discipline!) and built quanteda with a the aim making all of the main functions easily and logically pipeable. Since version 1, we have re-exported the %>% operator from magrittr to make it available out of the box with quanteda. With the introduction of the |> pipe in R 4.1.0, however, we prefer to use this variant, so will use that in all code used in this book.\n\n\nData used in examples\nAll examples and code are bundled as a companion R package to the book, available from our public GitHub repository.\n\n\n\n\n\n\nTip\n\n\n\nWe have written a companion package for this book called TAUR, which can be installed from GitHub using this command:\n\nremotes::install_github(\"quanteda/TAUR\")\n\n\n\nWe largely rely on data from three sources:\n\nthe built-in-objects from the quanteda package, such as the US Presidential Inaugural speech corpus;\n\nadded corpora from the book’s companion package, TAUR; and\nsome additional quanteda corpora or dictionaries from from the additional sources or packages where indicated.\n\nAlthough not commonly used, our scheme for naming data follows a very consistent scheme. The data objects being with data, have the object class as the second part of the name, such as corpus, and the third and final part of the data object name contains a description. The three elements are separated by the underscore (_) character. This means that any object is known by its name to be data, so that it shows up in the index of package objects (from the all-important help page, e.g. from help(package = \"quanteda\")) in one location, under “d”. It also means that its object class is known from the name, without further inspection. So data_dfm_lbgexample is a dfm, while data_corpus_inaugural is clearly a corpus. We use this scheme and others like with an almost religious fervour, because we think that learning the functionality of a programming framework for NLP and quantitative text analysis is complicated enough without having also to decipher or remember a mishmash of haphazard and inconsistently named functions and objects. The more you use our software packages and specifically quanteda, the more you will come to appreciate the attention we have paid to implementing a consistent naming scheme for objects, functions, and their arguments as well as to their consistent functionality."
  },
  {
    "objectID": "preface.html#colophon",
    "href": "preface.html#colophon",
    "title": "Preface",
    "section": "Colophon",
    "text": "Colophon\nThis book was written in RStudio using Quarto. The website is hosted via GitHub Pages, and the complete source is available on GitHub.\nThis version of the book was built with R version 4.2.1 (2022-06-23) and the following packages:\n\n\n\n\nPackage\nVersion\nSource\n\n\n\n\nquanteda\n3.2.2\nCRAN (R 4.2.0)\n\n\nquanteda.textmodels\n0.9.4\nCRAN (R 4.2.0)\n\n\nquanteda.textplots\n0.94.1\nCRAN (R 4.2.0)\n\n\nquanteda.textstats\n0.95\nCRAN (R 4.2.0)\n\n\nreadtext\n0.81\nCRAN (R 4.2.0)\n\n\nstopwords\n2.3\nCRAN (R 4.2.0)\n\n\ntidyverse\n1.3.2\nCRAN (R 4.2.0)\n\n\n\n\n\nHow to Contact Us\nPlease address comments and questions concerning this book by filing an issue on our GitHub page, https://github.com/quanteda/Text-Analysis-Using-R/issues/. At this repository, you will also find instructions for installing the companion R package, TAUR.\nFor more information about the authors or the Quanteda Initiative, visit our website."
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nDeveloping quanteda has been a labour of many years involving many contributors. The most notable contributor to the package and its learning materials is Kohei Watanabe, without whom quanteda would not be the incredible package it is today. Kohei has provided a clear and vigorous vision for the package’s evolution across its major versions and continues to maintain it today. Others have contributed in major ways both through design and programming, namely Paul Nulty and Akitaka Matsuo, both who were present at creation and through the 1.0 launch which involved the first of many major redesigns. Adam Obeng made a brief but indelible imprint on several of the packages in the quanteda family. Haiyan Wang wrote versions of some of the core C++ code that makes quanteda so fast, as well as contributing to the R base. William Lowe and Christian Müller also contributed code and ideas that have enriched the package and the methods it implements.\nNo project this large could have existed without institutional and financial benefactors and supporters. Most notable of these is the European Research Council, who funded the original development under a its Starting Investigator Grant scheme, awarded to Kenneth Benoit in 2011 for a project entitled QUANTESS: Quantitative Text Analysis for the Social Sciences (ERC-2011-StG 283794-QUANTESS). Our day jobs (as university professors) have also provided invaluable material and intellectual support for the development of the ideas, methodologies, and software documented in this book. This list includes the London School of Economics and Political Science, which employs Ken Benoit and which hosted the QUANTESS grant and employed many of the core quanteda team and/or where they completed PhDs; University College Dublin where Stefan Müller is employed; Trinity College Dublin, where Ken Benoit was formerly a Professor of Quantitative Social Sciences and where Stefan completed his PhD in political science; and the Australian National University which provided a generous affiliation to Ken Benoit and where the outline for this book took first shape as a giant tableau of post-it notes on the wall of a visiting office in the former building of the School of Politics and International Relations.\nAs we develop this book, we hope that many readers of the work-in-progress will contribute feedback, comments, and even edits, and we plan to acknowledge you all. So, don’t be shy readers.\nWe also have to give a much-deserved shout-out to the amazing team at RStudio, many of whom we’ve been privileged to hear speak at events or meet in person. You made Quarto available at just the right time. That and the innovations in R tools and software that you have driven for the past decade have been rich beyond every expectation.\nFinally, no acknowledgements would be complete without a profound thanks to our partners, who have put up with us during the long incubation of this project. They have had to listen us talk about this book for years before we finally got around to writing it. They’ve tolerated us cursing software bugs, students, CRAN, each other, package users, and ourselves. But they’ve also seen they joy that we’ve experienced from creating tools and materials that empower users and students, and the excitement of the long intellectual voyage we have taken together and with our ever-growing base of users and students. Bina and Émeline, thank you for all of your support and encouragement. You’ll be so happy to know we finally have this book project well underway.\n\n\n\n\nBécue-Bertaut, Monica. 2019. Textual Data Science with r. CRC Press.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 461–97. Thousand Oaks: Sage.\n\n\nBenoit, Kenneth, and Akitaka Matsuo. 2020. Spacyr: Wrapper to the ’spaCy’ ’NLP’ Library. https://CRAN.R-project.org/package=spacyr.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An R Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nFeinerer, Ingo, Kurt Hornik, and David Meyer. 2008. “Text Mining Infrastructure in R.” Journal of Statistical Software 25 (5): 1–54. https://www.jstatsoft.org/v25/i05/.\n\n\nHonnibal, Matthew, Ines Montani, Sophie Van Landeghem, and Adriane Boyd. 2020. “spaCy: Industrial-Strength Natural Language Processing in Python.” https://doi.org/10.5281/zenodo.1212303.\n\n\nKwartler, Ted. 2017. Text Mining in Practice with r. John Wiley & Sons.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in r.” JOSS 1 (3). https://doi.org/10.21105/joss.00037.\n\n\n———. 2017. Text Mining with r: A Tidy Approach. \" O’Reilly Media, Inc.\".\n\n\nTurenne, Nicolas. 2016. Analyse de Données Textuelles Sous r. ISTE Group."
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Stefan Müller is an Assistant Professor and Ad Astra Fellow in the School of Politics and International Relations at University College Dublin. Previously, he was a Senior Researcher at the University of Zurich. He received his Ph.D. (2019) from Trinity College Dublin, Department of Political Science. His current research focuses on political representation, party competition, political communication, public opinion, and quantitative text analysis. His work has been published in journals such as the American Political Science Review, The Journal of Politics, Political Communication, the European Journal of Political Research, and Political Science Research and Methods, among others.\nThe Quanteda Initiative is a non-profit company we founded in 2018 in London, devoted to the promotion of open-source text analysis software. It supports active development of these tools, in addition to providing training materials, training events, and sponsoring workshops and conferences. Its main product is the open-source quanteda package for R, but the Quanteda Initiative also supports a family of independent but interrelated packages for providing additional functionality for natural language processing and document management.\nIts core objectives, as stated in its charter, are to:\n\nSupport open-source, text analysis software developed for research and scientific analysis. These efforts focus mainly on the open-source software library quanteda, written for the R programming language, and its related family of extension packages.\nPromote interoperability between text analytic software libraries, including those written in other languages such as Python, Java, or C++.\nProvide ongoing user, technical, and development support for open-source text analysis software. Organize training and dissemination activities related to open-source text analysis software."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction and Motivation",
    "section": "",
    "text": "To dive right in to text analysis using R, by means of an extended demonstration of how to acquire, process, quantify, and analyze a series of texts. The objective is to demonstrate the techniques covered in the book and the tools required to apply them.\nThis overview is intended to serve more as a demonstration of the kinds of natural language processing and text analysis that can be done powerfully and easily in R, rather than as primary instructional material. Starting in Chapter @ref(rbasics), you learn about the R fundamentals and work our way gradually up to basic and then more advanced text analysis. Until then, enjoy the demonstration, and don’t be discouraged if it seems too advanced to follow at this stage. By the time you have finished this book, this sort of analysis will be very familiar."
  },
  {
    "objectID": "01-intro.html#application-analyzing-candidate-debates-from-the-us-presidential-election-campaign-of-2020",
    "href": "01-intro.html#application-analyzing-candidate-debates-from-the-us-presidential-election-campaign-of-2020",
    "title": "1  Introduction and Motivation",
    "section": "1.2 Application: Analyzing Candidate Debates from the US Presidential Election Campaign of 2020",
    "text": "1.2 Application: Analyzing Candidate Debates from the US Presidential Election Campaign of 2020\nThe methods demonstrated in this chapter include:\n\nacquiring texts directly from the world-wide web;\ncreating a corpus from the texts, with associated document-level variables;\nsegmenting the texts by sections found in each document;\ncleaning up the texts further;\ntokenizing the texts into words;\nsummarizing the texts in terms of who spoke and how much;\nexamining keywords-in-context from the texts, and identifying keywords using statistical association measures;\ntransforming and selecting features using lower-casing, stemming, and removing punctuation and numbers;\nremoving selected features in the form of “stop words”;\ncreating a document-feature matrix from the tokens;\nperforming sentiment analysis on the texts;\nfitting topic models on the texts.\n\nFor demonstration, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.1\n\n1.2.1 Acquiring text directly from the world wide web.\nFull transcripts of both televized debates are available from The American Presidency Project. We start our demonstration by scraping the debates using the rvest package (Wickham 2021). You will learn more about scraping data from the internet in Chapter @ref(acquire-internet). We first need to install and load the packages required for this demonstration.\n\n# install packages\ninstall.packages(\"quanteda\")\ninstall.packages(\"quanteda.textstats\")\ninstall.packages(\"quanteda.textplots\")\ninstall.packages(\"rvest\")\ninstall.packages(\"stringr\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"quanteda/quanteda.tidy\")\n\n\n# load packages\nlibrary(\"quanteda\")\nlibrary(\"rvest\")\nlibrary(\"stringr\")\nlibrary(\"quanteda.textstats\")\nlibrary(\"quanteda.textplots\")\nlibrary(\"quanteda.tidy\")\n\nNext, we identify the presidential debates for the 2020 period, assign the URL of the debates to an object, and load the source page. We retrieve metadata (the location and dates of the debates), store the information as a data frame, and get the URL debates. Finally, we scrape the search results and save the texts as a character vector, with one element per debate.\n\n# search presidential debates for the 2020 period\n# https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50\n\n# assign URL of debates to an object called url_debates\nurl_debates <- \"https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=01-01-2020&to%5Bdate%5D=12-31-2020&person2=200301&category2%5B%5D=64&items_per_page=50\"\n\nsource_page <- read_html(url_debates)\n\n# get debate meta-data\n\nnodes_pres <- \".views-field-title a\"\ntext_pres <- \".views-field-field-docs-start-date-time-value.text-nowrap\"\n\ndebates_meta <- data.frame(\n    location = html_text(html_nodes(source_page, nodes_pres)),\n    date =  html_text(html_nodes(source_page, text_pres)),\n    stringsAsFactors = FALSE\n)\n\n# format the date\ndebates_meta$date <- as.Date(trimws(debates_meta$date), \n                             format = \"%b %d, %Y\")\n\n# get debate URLs\ndebates_links <- source_page |> \n    html_nodes(\".views-field-title a\") |> \n    html_attr(name = \"href\") \n\n# add first part of URL to debate links\ndebates_links <- paste0(\"https://www.presidency.ucsb.edu\", debates_links)\n\n# scrape search results\ndebates_scraped <- lapply(debates_links, read_html)\n\n# get character vector, one element per debate\ndebates_text <- sapply(debates_scraped, function(x) {\n    html_nodes(x, \"p\") |> \n        html_text() |>\n        paste(collapse = \"\\n\\n\")\n})\n\nHaving retrieved the text of the two debates, we clean up the character vector. More specifically, we clean up the details on the location using regular expressions and the stringr package (see Chapter @ref(appendix-regex)).\n\ndebates_meta$location <- str_replace(debates_meta$location, \"^.* in \", \"\")\n\n\n\n1.2.2 Creating a text corpus\nNow we have two objects. The character vector debates_text contains the text of both debates, and the data frame debates_meta stores the date and location of both debates. These objects allow us to create a quanteda corpus, with the metadata. We use the corpus() function, specify the location of the document-level variables, and assign the corpus to an object called data_corpus_debates.\n\ndata_corpus_debates <- corpus(debates_text, \n                              docvars = debates_meta)\n\n# check the number of documents included in the text corpus\nndoc(data_corpus_debates)\n\n[1] 2\n\n\nThe object data_corpus_debates contains two documents, one for each debate. While this unit of analysis may be suitable for some analyses, we want to identify all utterances by the moderator and the two candidates. With the function corpus_segment(), we can segment the corpus into statements. The unit of analysis changes from a full debate to a statement during a debate. Inspecting the page for one of the debates2 reveals that a new utterance starts with the speaker’s name in ALL CAPS, followed by a colon. We use this consistent pattern for segmenting the corpus to the level of utterances. The regular expression \"\\\\s*[[:upper:]]+:\\\\s+\" identifies speaker names in ALL CAPS (\\\\s*[[:upper:]]+), followed by a colon +: and a white space \\\\s+.\n\ndata_corpus_debatesseg <- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\nsummary(data_corpus_debatesseg, n = 4)\n\nCorpus consisting of 1207 documents, showing 4 documents:\n\n    Text Types Tokens Sentences        location       date       pattern\n text1.1   139    251        13 Cleveland, Ohio 2020-09-29 \\n\\nWALLACE: \n text1.2     6      6         1 Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n text1.3     5      5         1 Cleveland, Ohio 2020-09-29   \\n\\nTRUMP: \n text1.4     3      3         1 Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n\n\nThe segmentation results in a new corpus consisting of 1,207 utterances. The summary() function provides a useful overview of each utterance. The first text, for example, contains 251 tokens, 139 types (i.e., unique tokens) and 13 sentences.\nHaving segmented the corpus, we improve the document-level variables since such meta information on each document is crucial for subsequent analyses like subsetting the corpus or grouping the corpus to the level of speakers. We create a new document-level variable called “speaker” based on the pattern extracted above.\n\n# str_trim() removes empty whitespaces, \n# str_remove_all() removes the colons,\n# and str_to_title() changes speaker names \n# from UPPER CASE to Title Case\n\ndata_corpus_debatesseg <- data_corpus_debatesseg |> \n    rename(speaker = pattern) |> \n    mutate(speaker = str_trim(speaker),\n           speaker = str_remove_all(speaker, \":\"),\n           speaker = str_to_title(speaker)) \n\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$speaker,\n      data_corpus_debatesseg$location)\n\n         \n          Cleveland, Ohio Nashville, Tennessee\n  Biden               269                   84\n  Trump               340                  122\n  Wallace             246                    0\n  Welker                0                  146\n\n\nThe cross-table reports the number of statement by each speaker in each debate. The first debate in Cleveland seems to be longer: the number of Trump’s and Biden’s statements during the Cleveland debates are three times higher than those in Tennessee. The transcript reports 246 utterances by Chris Wallace during the first and 146 by Kristen Welker during the second debate. We can inspect all cleaned document-level variables in this text corpus.\n\ndata_corpus_debatesseg |> \n    docvars() |> \n    glimpse()\n\nRows: 1,207\nColumns: 3\n$ location <chr> \"Cleveland, Ohio\", \"Cleveland, Ohio\", \"Cleveland, Ohio\", \"Cle…\n$ date     <date> 2020-09-29, 2020-09-29, 2020-09-29, 2020-09-29, 2020-09-29, …\n$ speaker  <chr> \"Wallace\", \"Biden\", \"Trump\", \"Biden\", \"Wallace\", \"Trump\", \"Wa…\n\n\n\n\n1.2.3 Tokenizing a corpus\nNext, we tokenize our text corpus. Typically, tokenization involves separating texts by white spaces. We tokenize the text corpus without any pre-processing using tokens().\n\ntoks_usdebates2020 <- tokens(data_corpus_debatesseg)\n\n# let's inspect the first six tokens of the first four documents\nprint(toks_usdebates2020, max_ndoc = 4, max_ntoken = 6)\n\nTokens consisting of 1,207 documents and 3 docvars.\ntext1.1 :\n[1] \"Good\"      \"evening\"   \"from\"      \"the\"       \"Health\"    \"Education\"\n[ ... and 245 more ]\n\ntext1.2 :\n[1] \"How\"   \"you\"   \"doing\" \",\"     \"man\"   \"?\"    \n\ntext1.3 :\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"?\"    \n\ntext1.4 :\n[1] \"I'm\"  \"well\" \".\"   \n\n[ reached max_ndoc ... 1,203 more documents ]\n\ntokens(data_corpus_debatesseg)\n\nTokens consisting of 1,207 documents and 3 docvars.\ntext1.1 :\n [1] \"Good\"       \"evening\"    \"from\"       \"the\"        \"Health\"    \n [6] \"Education\"  \"Campus\"     \"of\"         \"Case\"       \"Western\"   \n[11] \"Reserve\"    \"University\"\n[ ... and 239 more ]\n\ntext1.2 :\n[1] \"How\"   \"you\"   \"doing\" \",\"     \"man\"   \"?\"    \n\ntext1.3 :\n[1] \"How\"   \"are\"   \"you\"   \"doing\" \"?\"    \n\ntext1.4 :\n[1] \"I'm\"  \"well\" \".\"   \n\ntext1.5 :\n [1] \"Gentlemen\" \",\"         \"a\"         \"lot\"       \"of\"        \"people\"   \n [7] \"been\"      \"waiting\"   \"for\"       \"this\"      \"night\"     \",\"        \n[ ... and 137 more ]\n\ntext1.6 :\n [1] \"Thank\" \"you\"   \"very\"  \"much\"  \",\"     \"Chris\" \".\"     \"I\"     \"will\" \n[10] \"tell\"  \"you\"   \"very\" \n[ ... and 282 more ]\n\n[ reached max_ndoc ... 1,201 more documents ]\n\n# check number of tokens and types\ntoks_usdebates2020 |> \n    ntoken() |> \n    sum()\n\n[1] 43742\n\ntoks_usdebates2020 |> \n    ntype() |> \n    sum()\n\n[1] 28174\n\n\nWithout any pre-processing, the corpus consists of 43,742 tokens and 28,174 types. We can easily check how these numbers change when transforming all tokens to lowercase and removing punctuation characters.\n\ntoks_usdebates2020_reduced <- toks_usdebates2020 |> \n    tokens(remove_punct = TRUE) |> \n    tokens_tolower()\n\n# check number of tokens and types\ntoks_usdebates2020_reduced |> \n    ntoken() |> \n    sum()\n\n[1] 36740\n\ntoks_usdebates2020_reduced |> \n    ntype() |> \n    sum()\n\n[1] 24965\n\n\nThe number of tokens and types decreases to 36,740 and 28,174, respectively, after removing punctuation and harmonizing all terms to lowercase.\n\n\n1.2.4 Keywords-in-context\nIn contrast to a document-feature matrix (covered below), tokens objects still preserve the order of words. We can use tokens objects to identify the occurrence of keywords and their immediate context.\n\nkw_america <- kwic(toks_usdebates2020, \n                   pattern = c(\"america\"),\n                   window = 2)\n\n# number of mentions\nnrow(kw_america)\n\n[1] 18\n\n# print first 6 mentions of America and the context of ±2 words\nhead(kw_america, n = 6)\n\nKeyword-in-context with 6 matches.                                                     \n [text1.284, 66]        towns in | America | , how   \n [text1.290, 80]       states in | America | with a  \n [text1.290, 96]       States of | America | , and   \n  [text1.335, 5] worst president | America | has ever\n [text1.489, 37]        whole of | America | . But   \n [text1.500, 53]      applied in | America | .\"      \n\n\n\n\n1.2.5 Text processing\nThe keywords-in-context analysis above reveals that all terms are still in upper case and that very frequent, uninformative words (so-called stopwords) and punctuation are still part of the text. In most applications, we remove very frequent features and transform all words to lowercase. The code below shows how to adjust the object accordingly.\n\ntoks_usdebates2020_processed <- data_corpus_debatesseg |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\")) |> \n    tokens_tolower()\n\nLet’s inspect if the changes have been implemented as we expect by calling kwic() on the new tokens object.\n\nkw_america_processed <- kwic(toks_usdebates2020_processed, \n                             pattern = c(\"america\"),\n                             window = 2)\n\n# print first 6 mentions of America and the context of ±2 words\nhead(kw_america_processed, n = 6)\n\nKeyword-in-context with 6 matches.                                                                 \n [text1.284, 32]     class towns | america | well guy            \n [text1.290, 29]     half states | america | significant increase\n [text1.290, 38]   united states | america | wants open          \n  [text1.335, 3] worst president | america | ever come           \n [text1.489, 15]  equality whole | america | never accomplished  \n [text1.500, 25] equally applied | america | believe separate    \n\n# test: print as table+\nlibrary(kableExtra)\nkw_america_processed |> data.frame() |> \n  dplyr::select(Pre = pre, Keyword = keyword, Post = post, Pattern = pattern) |>  \n  kbl(booktabs = T) %>%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"), html_font = \"Source Sans Pro\", full_width = F)\n\n\n\n \n  \n    Pre \n    Keyword \n    Post \n    Pattern \n  \n \n\n  \n    class towns \n    america \n    well guy \n    america \n  \n  \n    half states \n    america \n    significant increase \n    america \n  \n  \n    united states \n    america \n    wants open \n    america \n  \n  \n    worst president \n    america \n    ever come \n    america \n  \n  \n    equality whole \n    america \n    never accomplished \n    america \n  \n  \n    equally applied \n    america \n    believe separate \n    america \n  \n  \n    defeat racism \n    america \n     \n    america \n  \n  \n    increase homicides \n    america \n    summer particularly \n    america \n  \n  \n    less violence \n    america \n    today president \n    america \n  \n  \n    fired plant \n    america \n    one's going \n    america \n  \n  \n    fire plant \n    america \n    going move \n    america \n  \n  \n    united states \n    america \n    situation thousands \n    america \n  \n  \n    every company \n    america \n    blow away \n    america \n  \n  \n    united states \n    america \n     \n    america \n  \n  \n    united states \n    america \n    anybody seeking \n    america \n  \n  \n    section race \n    america \n    want talk \n    america \n  \n  \n    institutional racism \n    america \n    always said \n    america \n  \n  \n    growing industry \n    america \n    electric excuse \n    america \n  \n\n\n\n\n\nThe processing of the tokens object worked as expected. Let’s imagine we want to group the documents by debate and speaker, resulting in two documents for Trump, two for Biden, and one for each moderator. tokens_group() allows us to change the unit of analysis. After aggregating the documents, we can use the function textplot_xray() to observe the occurrences of specific keywords during the debates.\n\n# new document-level variable with date and speaker\ntoks_usdebates2020$speaker_date <- paste(\n    toks_usdebates2020$speaker,\n    toks_usdebates2020$date,\n    sep = \", \")\n\n# reshape the tokens object to speaker-date level \n# and keep only Trump and Biden\ntoks_usdebates2020_grouped <- toks_usdebates2020 |> \n    tokens_subset(speaker %in% c(\"Trump\", \"Biden\")) |> \n    tokens_group(groups = speaker_date)\n\n# check number of documents\nndoc(toks_usdebates2020_grouped)\n\n[1] 4\n\n# use absolute position of the token in the document\ntextplot_xray(\n    kwic(toks_usdebates2020_grouped, pattern = \"america\"), \n    kwic(toks_usdebates2020_grouped, pattern = \"tax*\")\n)\n\n\n\n\nThe grouped document also allows us to check how often each candidate spoke during the debate.\n\nntoken(toks_usdebates2020_grouped)\n\nBiden, 2020-09-29 Biden, 2020-10-22 Trump, 2020-09-29 Trump, 2020-10-22 \n             7996              8093              8973              9016 \n\n\nThe number of tokens between Trump and Biden do not differ substantively in both debates. Trump’s share of speech is only slightly higher than Biden’s (7996 v 8093 tokens; 8973 v 9016 tokens).\n\n\n1.2.6 Identifying multiword expressions\nMany languages build on multiword expressions. For instance, “income” and “tax” as separate unigrams have a different meaning than the bigram “income tax”. The package quanteda.textstats includes the function textstat_collocation() that automatically retrieves common multiword expressions.\n\ntstat_coll <- data_corpus_debatesseg |> \n    tokens(remove_punct = TRUE) |> \n    tokens_remove(pattern = stopwords(\"en\"), padding = TRUE) |> \n    textstat_collocations(size = 2:3, min_count = 5)\n\n# for illustration purposes select the first 20 collocations\nhead(tstat_coll, 20)\n\n       collocation count count_nested length   lambda        z\n1  president trump    71           44      2 6.928550 22.01175\n2        make sure    30            4      2 7.439572 21.53052\n3  president biden    52           52      2 6.450825 20.63073\n4     mr president    34           21      2 5.389748 19.78036\n5      health care    20           15      2 7.580147 19.32484\n6       number one    18           16      2 5.595991 17.94433\n7        right now    18           12      2 4.801444 16.82701\n8       number two    15           11      2 5.472508 16.69323\n9     half million    15           13      2 6.852610 16.55670\n10         mr vice    15           15      2 5.323919 16.52522\n11      four years    19           15      2 7.433288 16.49838\n12 american people    22            4      2 4.969408 16.44720\n13     two minutes    28           19      2 8.504176 16.09104\n14       come back    12            7      2 5.537489 15.46566\n15     three years    12            5      2 5.464293 15.20941\n16      one number    12           12      2 5.122270 14.65987\n17  climate change    11            8      2 8.963835 14.65614\n18  million people    18           18      2 4.073645 14.42229\n19  final question    14            7      2 7.008316 14.40121\n20  vice president    99           81      2 9.070192 14.05817\n\n\nWe can use tokens_compound() to compound certain multiword expressions before creating a document-feature matrix which does not consider word order. For illustration purposes, we compound climate change ,social securit*, and health insurance*. By default, compounded tokens are concatenated by _.\n\ntoks_usdebates2020_comp <- toks_usdebates2020 |> \n    tokens(remove_punct = TRUE) |> \n    tokens_compound(pattern = phrase(c(\"climate change\",\n                                       \"social securit*\",\n                                       \"health insuranc*\"))) |> \n    tokens_remove(pattern = stopwords(\"en\"))\n\n\n\n1.2.7 Document-feature matrix\nWe have come a long way already. We downloaded debate transcripts, segmented the texts to utterances, added document-level variables, tokenized the corpus, inspected keywords, and compounded multiword expressions. Next, we transform our tokens object into a document-feature matrix (dfm). A dfm counts the occurrences of tokens in each document. We can create a document feature matrix, print the structure, and get the most frequent words.\n\ndfmat_presdebates20 <- dfm(toks_usdebates2020_comp)\n\n# most frequent features\ntopfeatures(dfmat_presdebates20, n = 10)\n\npresident     going    people      said      know      want       get       say \n      288       278       259       164       139       129       119       114 \n     look      vice \n      105       101 \n\n# most frequent features by speaker\n\ntopfeatures(dfmat_presdebates20, groups = speaker, n = 10)\n\n$Biden\n    going    people      fact      said       get      make      know president \n      126       119        72        62        54        46        46        45 \n     sure       can \n       44        43 \n\n$Trump\npeople  going   know   said   look   want    joe   done    say  think \n   111     92     85     77     65     62     55     55     54     52 \n\n$Wallace\npresident       sir     going  question        mr      vice     trump        go \n      110        56        44        39        39        36        33        30 \n      two     right \n       28        25 \n\n$Welker\npresident      vice     trump     biden  question     right       let      move \n      100        52        39        35        31        28        28        24 \n     talk      want \n       21        18 \n\n\nMany methods build on document-feature matrices and the “bag-of-words” approach. In this section, we introduce textstat_keyness(), which identifies features that occur differentially across different categories – in our case, Trump’s and Biden’s utterances. The function textplot_keyness() provides a straightforward way of visualize the results of the keyness analysis (Figure (fig:keynesspres).)\n\ntstat_key <- dfmat_presdebates20 |>\n    dfm_subset(speaker %in% c(\"Trump\", \"Biden\")) |> \n    dfm_group(groups = speaker) |> \n    textstat_keyness(target = \"Trump\")\n\ntextplot_keyness(tstat_key)\n\n\n\n\n\n\n1.2.8 Bringing it all together: Sentiment analysis\nBefore introducing the R Basics in the next chapter, we show how to conduct a dictionary-based sentiment analysis using the Lexicoder Sentiment Dictionary (Young and Soroka 2012). The dictionary, included in quanteda as data_dictionary_LSD2015 contains 1709 positive and 2858 negative terms (as well as their negations). We discuss advanced sentiment analyses with measures of uncertainty in Chapter ??.\n\ntoks_sent <- toks_usdebates2020 |> \n    tokens_group(groups = speaker) |> \n    tokens_lookup(dictionary = data_dictionary_LSD2015,\n                  nested_scope = \"dictionary\")\n\n# create a dfm with the count of matches,\n# transform object into a data frame,\n# and add document-level variables\ndat_sent <- toks_sent |> \n    dfm() |> \n    convert(to = \"data.frame\") |> \n    cbind(docvars(toks_sent))\n\n# select Trump and Biden and aggregate sentiment\ndat_sent$sentiment = with(\n    dat_sent, \n    log((positive + neg_negative + 0.5) /  \n            (negative + neg_positive + 0.5)))\n\ndat_sent\n\n   doc_id negative positive neg_positive neg_negative speaker   sentiment\n1   Biden      357      465           33            8   Biden  0.19272394\n2   Trump      430      509            7            2   Trump  0.15627088\n3 Wallace      136      185            0            4 Wallace  0.32806441\n4  Welker      106      102            0            0  Welker -0.03828219"
  },
  {
    "objectID": "01-intro.html#issues",
    "href": "01-intro.html#issues",
    "title": "1  Introduction and Motivation",
    "section": "1.3 Issues",
    "text": "1.3 Issues\nSome issues raised from the example, where we might have done things differently or added additional analysis."
  },
  {
    "objectID": "01-intro.html#further-reading",
    "href": "01-intro.html#further-reading",
    "title": "1  Introduction and Motivation",
    "section": "1.4 Further Reading",
    "text": "1.4 Further Reading\nSome studies of debate transcripts? Or issues involved in analyzing interactive or “dialogical” documents."
  },
  {
    "objectID": "01-intro.html#exercises",
    "href": "01-intro.html#exercises",
    "title": "1  Introduction and Motivation",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nAdd some here.\n\n\n\n\nWickham, Hadley. 2021. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\nYoung, Lori, and Stuart N. Soroka. 2012. “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication 29 (2): 205–31."
  },
  {
    "objectID": "02-r-basics.html",
    "href": "02-r-basics.html",
    "title": "2  R Basics",
    "section": "",
    "text": "To provide a targeted introduction to R, for those needing an introduction or a review.\nWe will cover:\n\ncore object types (atomic, vectors, lists, complex)\ndetailed survey of characters and “strings”\ndetailed coverage of list types\ndetailed coverage of matrix types\nintroduction to the data.frame\noutput data in the form of a data.frame\noutput data in the form of a plot (ggplot2 object)\n\nThe objective is to introduce how these core object types behave, and use them to store basic quantities required in text analysis."
  },
  {
    "objectID": "02-r-basics.html#methods",
    "href": "02-r-basics.html#methods",
    "title": "2  R Basics",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nThe methods are primarily about how to use R, including:\nWe will build up the basic objects needed for understand the core structures needed in R to hold texts (character), meta-data (data.frame), tokens (lists), dictionaries (lists), and document-feature matrices (matrices)."
  },
  {
    "objectID": "02-r-basics.html#examples",
    "href": "02-r-basics.html#examples",
    "title": "2  R Basics",
    "section": "2.3 Examples",
    "text": "2.3 Examples\nExamples working through the construction of each text object container using the types above."
  },
  {
    "objectID": "02-r-basics.html#issues",
    "href": "02-r-basics.html#issues",
    "title": "2  R Basics",
    "section": "2.4 Issues",
    "text": "2.4 Issues\nCould have used “tidy” approaches.\nSparsity.\nIndexing.\nWouldn’t it be nice if we could nest some objects inside others?"
  },
  {
    "objectID": "02-r-basics.html#further-reading",
    "href": "02-r-basics.html#further-reading",
    "title": "2  R Basics",
    "section": "2.5 Further Reading",
    "text": "2.5 Further Reading\nSome resources on R."
  },
  {
    "objectID": "02-r-basics.html#exercises",
    "href": "02-r-basics.html#exercises",
    "title": "2  R Basics",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\nAdd some here."
  },
  {
    "objectID": "03-r-workflow.html",
    "href": "03-r-workflow.html",
    "title": "3  R Workflow",
    "section": "",
    "text": "How R works: - passing by value, compared to other languages - using “pipes” - object orientation: object classes and methods (functions)\nWorkflow issues: - clear code - reproducible workflow - R markdown and R notebooks\nUsing packages"
  },
  {
    "objectID": "03-r-workflow.html#methods",
    "href": "03-r-workflow.html#methods",
    "title": "3  R Workflow",
    "section": "3.2 Methods",
    "text": "3.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "03-r-workflow.html#examples",
    "href": "03-r-workflow.html#examples",
    "title": "3  R Workflow",
    "section": "3.3 Examples",
    "text": "3.3 Examples\nObjects that work on character and lists.\nThings that work for data.frames. Non-standard evaluation for named columns in a data.frame.\nggplot2"
  },
  {
    "objectID": "03-r-workflow.html#issues",
    "href": "03-r-workflow.html#issues",
    "title": "3  R Workflow",
    "section": "3.4 Issues",
    "text": "3.4 Issues\nAdditional issues."
  },
  {
    "objectID": "03-r-workflow.html#further-reading",
    "href": "03-r-workflow.html#further-reading",
    "title": "3  R Workflow",
    "section": "3.5 Further Reading",
    "text": "3.5 Further Reading\nAdvanced reading on R. Packages."
  },
  {
    "objectID": "03-r-workflow.html#exercises",
    "href": "03-r-workflow.html#exercises",
    "title": "3  R Workflow",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\nAdd some here."
  },
  {
    "objectID": "04-r-strings.html",
    "href": "04-r-strings.html",
    "title": "4  Working with Text in R",
    "section": "",
    "text": "A deeper dive into how text is represented in R. - character vectors vesus “strings” - factors - base package string handling - the stringi and stringr packages - regular expressions - encoding, briefly."
  },
  {
    "objectID": "04-r-strings.html#methods",
    "href": "04-r-strings.html#methods",
    "title": "4  Working with Text in R",
    "section": "4.2 Methods",
    "text": "4.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "04-r-strings.html#examples",
    "href": "04-r-strings.html#examples",
    "title": "4  Working with Text in R",
    "section": "4.3 Examples",
    "text": "4.3 Examples\nLots of the above."
  },
  {
    "objectID": "04-r-strings.html#issues",
    "href": "04-r-strings.html#issues",
    "title": "4  Working with Text in R",
    "section": "4.4 Issues",
    "text": "4.4 Issues\nAdditional issues."
  },
  {
    "objectID": "04-r-strings.html#further-reading",
    "href": "04-r-strings.html#further-reading",
    "title": "4  Working with Text in R",
    "section": "4.5 Further Reading",
    "text": "4.5 Further Reading\nString handling in R. More regular expressions. More on encoding."
  },
  {
    "objectID": "04-r-strings.html#exercises",
    "href": "04-r-strings.html#exercises",
    "title": "4  Working with Text in R",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\nAdd some here."
  },
  {
    "objectID": "05-acquiring-files.html",
    "href": "05-acquiring-files.html",
    "title": "5  Working with Files",
    "section": "",
    "text": "Really basic issues for working with files, such as: - Where files typically live on your computer; - Paths, the “current working directory” and how to change it; - Input formats and methods to convert them; - saving and naming files; - text editors; - cleaning texts; - detecting and converting text file encodings."
  },
  {
    "objectID": "05-acquiring-files.html#methods",
    "href": "05-acquiring-files.html#methods",
    "title": "5  Working with Files",
    "section": "5.2 Methods",
    "text": "5.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "05-acquiring-files.html#examples",
    "href": "05-acquiring-files.html#examples",
    "title": "5  Working with Files",
    "section": "5.3 Examples",
    "text": "5.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "05-acquiring-files.html#issues",
    "href": "05-acquiring-files.html#issues",
    "title": "5  Working with Files",
    "section": "5.4 Issues",
    "text": "5.4 Issues\nAdditional issues."
  },
  {
    "objectID": "05-acquiring-files.html#further-reading",
    "href": "05-acquiring-files.html#further-reading",
    "title": "5  Working with Files",
    "section": "5.5 Further Reading",
    "text": "5.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "05-acquiring-files.html#exercises",
    "href": "05-acquiring-files.html#exercises",
    "title": "5  Working with Files",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\nAdd some here."
  },
  {
    "objectID": "06-acquiring-packages.html",
    "href": "06-acquiring-packages.html",
    "title": "6  Using Text Import Tools",
    "section": "",
    "text": "The readtext package\nUsing API gateway packages: twitteR and other social media packages\nDealing with JSON formats\nNewspaper formats, LexisNexis, etc.\nOCR tools"
  },
  {
    "objectID": "06-acquiring-packages.html#methods",
    "href": "06-acquiring-packages.html#methods",
    "title": "6  Using Text Import Tools",
    "section": "6.2 Methods",
    "text": "6.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "06-acquiring-packages.html#examples",
    "href": "06-acquiring-packages.html#examples",
    "title": "6  Using Text Import Tools",
    "section": "6.3 Examples",
    "text": "6.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "06-acquiring-packages.html#issues",
    "href": "06-acquiring-packages.html#issues",
    "title": "6  Using Text Import Tools",
    "section": "6.4 Issues",
    "text": "6.4 Issues\nAdditional issues."
  },
  {
    "objectID": "06-acquiring-packages.html#further-reading",
    "href": "06-acquiring-packages.html#further-reading",
    "title": "6  Using Text Import Tools",
    "section": "6.5 Further Reading",
    "text": "6.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "06-acquiring-packages.html#exercises",
    "href": "06-acquiring-packages.html#exercises",
    "title": "6  Using Text Import Tools",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\nAdd some here."
  },
  {
    "objectID": "07-acquiring-internet.html",
    "href": "07-acquiring-internet.html",
    "title": "7  Obtaining Texts from the Internet",
    "section": "",
    "text": "Web scraping\nMarkup formats\nChallenges of removing tags\nAPIs, and JSON"
  },
  {
    "objectID": "07-acquiring-internet.html#methods",
    "href": "07-acquiring-internet.html#methods",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.2 Methods",
    "text": "7.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "07-acquiring-internet.html#examples",
    "href": "07-acquiring-internet.html#examples",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.3 Examples",
    "text": "7.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "07-acquiring-internet.html#issues",
    "href": "07-acquiring-internet.html#issues",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.4 Issues",
    "text": "7.4 Issues\nAdditional issues."
  },
  {
    "objectID": "07-acquiring-internet.html#further-reading",
    "href": "07-acquiring-internet.html#further-reading",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.5 Further Reading",
    "text": "7.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "07-acquiring-internet.html#exercises",
    "href": "07-acquiring-internet.html#exercises",
    "title": "7  Obtaining Texts from the Internet",
    "section": "7.6 Exercises",
    "text": "7.6 Exercises\nAdd some here."
  },
  {
    "objectID": "08-quanteda-overview.html",
    "href": "08-quanteda-overview.html",
    "title": "8  Introducing the quanteda Package",
    "section": "",
    "text": "Purpose\nBasic object types in quanteda\nInter-related classes\nWorkflow\nKey differences between quanteda and other packages\nWorking with other packages"
  },
  {
    "objectID": "08-quanteda-overview.html#methods",
    "href": "08-quanteda-overview.html#methods",
    "title": "8  Introducing the quanteda Package",
    "section": "8.2 Methods",
    "text": "8.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "08-quanteda-overview.html#examples",
    "href": "08-quanteda-overview.html#examples",
    "title": "8  Introducing the quanteda Package",
    "section": "8.3 Examples",
    "text": "8.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "08-quanteda-overview.html#issues",
    "href": "08-quanteda-overview.html#issues",
    "title": "8  Introducing the quanteda Package",
    "section": "8.4 Issues",
    "text": "8.4 Issues\nAdditional issues."
  },
  {
    "objectID": "08-quanteda-overview.html#further-reading",
    "href": "08-quanteda-overview.html#further-reading",
    "title": "8  Introducing the quanteda Package",
    "section": "8.5 Further Reading",
    "text": "8.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "08-quanteda-overview.html#exercises",
    "href": "08-quanteda-overview.html#exercises",
    "title": "8  Introducing the quanteda Package",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\nAdd some here."
  },
  {
    "objectID": "09-quanteda-corpus.html",
    "href": "09-quanteda-corpus.html",
    "title": "9  Creating and Managing Corpora",
    "section": "",
    "text": "In this section, we cover the corpus object. We explain why you need a text corpus for text analysis and how the selection of texts can affect your results and inferences. We also outline approaches for changing the unit of analysis (reshaping and segmenting corpora), how to filter a text corpus based on variables associated with texts, how to retrieve the raw texts from a text corpus, and how to manage metadata about a text corpus."
  },
  {
    "objectID": "09-quanteda-corpus.html#methods",
    "href": "09-quanteda-corpus.html#methods",
    "title": "9  Creating and Managing Corpora",
    "section": "9.2 Methods",
    "text": "9.2 Methods\nAll text analysis projects involve choices about the texts to be analyzed. Creating a text corpus starts with defining a sample of the available texts, out of all possible texts you could have selected. Afterwards, the texts identified for analysis need to be collected, gathered in a text corpus, and (usually) accompanied by attributes that distinguish texts. Examples include the newspaper, the date of publication, and the article’s author (Benoit 2020). A text corpus could be all articles published on immigration in Irish newspapers, with each article constituting one document in the text corpus. These so-called document-level variables contain additional information on each document and allow researchers to distinguish between texts in their analysis.\nIn some text analysis applications, the sample could constitute the entirety of texts. Analyzing all texts released by the actor(s) of interest does not necessarily mean that an analysis is without problems. Selection issues can drive the information that is recorded. Texts or information that are not transcribed or published cannot be included in the analysis. When analyzing budget speeches, speaker selection is an important caveat. Parties strategically select Members of Parliament who can express their opinion on government policy and budget decisions (Herzog and Benoit 2015). The positions of politicians not selected for speaking at a budget debate, cannot be considered in the textual analysis. PROVIDE ANOTHER EXAMPLE? Researchers should therefore consider potential selection effects or systematic missingness of information when assembling a text corpus.\nThe selection of texts will determine the scope of the analysis, generalizability, and inferences you can draw from your analysis. The principles of your research design should justify the inclusion or exclusion of documents for analysis. For example, if you want to study rhetoric during televised debates, the corpus would be limited to transcripts of televised debates. When comparing issue salience in televised debates and campaign speeches, the corpus will include debate transcripts and speeches. Thus, the research question should drive document selection.\n\n\n\n\n\n\nIncluding and comparing texts with very different characteristics may affect your analysis. For example, written speeches often differ from spoken speeches (Benoit, Munger, and Spirling 2019). Debates follow a different data-generating process than campaign speeches. Debates rely on questions and spontaneous responses, while politicians or their campaign teams draft campaign speeches well in advance. This does not mean that different types of text cannot or should not be compared since such a comparison can reveal structural differences based on the medium of communication. However, we would strongly advise you to identify structural differences between types of texts by comparing different groups of texts. We discuss how to differences in word usage across groups in Chapter 17. Chapter 25 shows how to identify variation in topic prevalence for two or more groups of documents.\n\n\n\nBesides selecting texts for analysis, researchers need to determine the unit of analysis of the text corpus. The unit of analysis should be driven by your research design. If a researcher is interested in textual features associated with likes and retweets on Twitter, the unit of analysis is an individual tweet. A project about the association between sentiment and real-time voter reactions may shift the unit of analysis could be a candidate’s speech utterance (Boussalis et al. 2021)."
  },
  {
    "objectID": "09-quanteda-corpus.html#examples",
    "href": "09-quanteda-corpus.html#examples",
    "title": "9  Creating and Managing Corpora",
    "section": "9.3 Examples",
    "text": "9.3 Examples\nFor demonstration, we will use the corpus of televised debate transcripts from the U.S. Presidential election campaign of 2020. Donald Trump and Joe Biden participated in two televised debates. The first debate took place in Cleveland, Ohio, on 29 September 2020. The two candidates met again in Nashville, Tennessee on 10 December.1 Following the quanteda naming conventinos, the object name starts with data_ (since it contains data), followed by corpus_ (indicating that the object is a text corpus) and debates, describing the text corpus.\nFirst, we inspect the text corpus using the summary() and ndoc() functions.\n\n\n\n\nsummary(data_corpus_debates)\n\nCorpus consisting of 2 documents, showing 2 documents:\n\n               Text Types Tokens Sentences\n Debate: 2020-09-29  2565  24548      1928\n Debate: 2020-10-22  2413  21652      1417\n                                           location       date\n Case Western Reserve University in Cleveland, Ohio 2020-09-29\n         Belmont University in Nashville, Tennessee 2020-10-22\n\nndoc(data_corpus_debates)\n\n[1] 2\n\n\nThe corpus consists of 2 documents. When inspecting the output of summary(data_corpus_debates) reveals that each document currently is the full transcript (Biden, Trump, and moderator) of a debate. The first document, containing the transcripts of the debate in Cleveland contains 24,548 tokens, 2565 types (i.e., unique tokens) and 1928 sentences. The second debate in Nashville is slightly shorter (21,652 tokens and 2,413 types).\n\n9.3.1 Changing the Unit of Analysis\nWhen analyzing debates, researchers often move to the level of utterances. We can achieve this using corpus_segment(). In the transcript, an utterance starts with the speaker’s name in ALL CAPS, followed by a colon. The regular expression \"\\\\s*[[:upper:]]+:\\\\s+\" identifies speaker names in ALL CAPS (\\\\s*[[:upper:]]+), followed by a colon +: and a white space \\\\s+. For a primer on regular expression see Chapter C.\n\n# segment text corpus to level of utterances\ndata_corpus_debatesseg <- corpus_segment(data_corpus_debates, \n                                         pattern =  \"\\\\s*[[:upper:]]+:\\\\s+\", \n                                         valuetype = \"regex\", \n                                         case_insensitive = FALSE)\n\n# overview of text corpus; n = 4 prints only the first four documents\nsummary(data_corpus_debatesseg, n = 4)\n\nCorpus consisting of 1207 documents, showing 4 documents:\n\n                 Text Types Tokens Sentences\n Debate: 2020-09-29.1   139    251        13\n Debate: 2020-09-29.2     6      6         1\n Debate: 2020-09-29.3     5      5         1\n Debate: 2020-09-29.4     3      3         1\n                                           location       date       pattern\n Case Western Reserve University in Cleveland, Ohio 2020-09-29 \\n\\nWALLACE: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29   \\n\\nTRUMP: \n Case Western Reserve University in Cleveland, Ohio 2020-09-29   \\n\\nBIDEN: \n\nndoc(data_corpus_debatesseg)\n\n[1] 1207\n\n\n\n\n9.3.2 Creating New Document-Level Variables\nThe new corpus consists of 1207 utterances by the moderators and candidates. The document-level variable pattern assigned the speaker name to each document. We can create a new speaker document-level variable by combining functions from the stringr and quanteda.tidy packages: mutate() creates a new speaker variable, and the stringr functions remove empty whitespaces (str_trim()), the colon (str_remove_all()) and change the names from UPPER CASE to Title Case (str_to_title()).\n\nlibrary(\"stringr\")\nlibrary(\"quanteda.tidy\")\n\ndata_corpus_debatesseg <- data_corpus_debatesseg |> \n    mutate(speaker = stringr::str_trim(pattern),\n           speaker = stringr::str_remove_all(speaker, \":\"),\n           speaker = stringr::str_to_title(speaker)) \n\nNext, we can use simple base R functions to inspect the count of utterances by speaker and debate.\n\n# cross-table of speaker statements by debate\ntable(data_corpus_debatesseg$location,\n      data_corpus_debatesseg$speaker)\n\n                                                    \n                                                     Biden Trump Wallace Welker\n  Belmont University in Nashville, Tennessee            84   122       0    146\n  Case Western Reserve University in Cleveland, Ohio   269   340     246      0\n\n\nWe could further reshape the corpus to the level of sentences with corpus_reshape() if we are interested, for instance, in sentence-level sentiment or issue salience.\n\ndata_corpus_debatessent <- corpus_reshape(data_corpus_debatesseg,\n                                          to = \"sentences\")\n\nndoc(data_corpus_debatessent)\n\n[1] 3564\n\n\nThe new text corpus moved from 1207 utterances to 3564 sentences. Using functions such as quanteda.textstat’s textstat_summary() we can retrieve summary statistics about each sentence.\n\nlibrary(\"quanteda.textstats\")\ndat_summary_sents <- textstat_summary(data_corpus_debatessent)\n\n# aggregated summary statistics\nsummary(dat_summary_sents)\n\n   document             chars            sents       tokens     \n Length:3564        Min.   :  3.00   Min.   :1   Min.   : 1.00  \n Class :character   1st Qu.: 24.00   1st Qu.:1   1st Qu.: 6.00  \n Mode  :character   Median : 41.00   Median :1   Median : 9.00  \n                    Mean   : 55.46   Mean   :1   Mean   :12.27  \n                    3rd Qu.: 73.00   3rd Qu.:1   3rd Qu.:16.00  \n                    Max.   :414.00   Max.   :1   Max.   :79.00  \n     types           puncts          numbers          symbols       \n Min.   : 1.00   Min.   : 0.000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 6.00   1st Qu.: 1.000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median : 9.00   Median : 1.000   Median :0.0000   Median :0.00000  \n Mean   :10.83   Mean   : 1.965   Mean   :0.0794   Mean   :0.01543  \n 3rd Qu.:14.00   3rd Qu.: 3.000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :56.00   Max.   :15.000   Max.   :4.0000   Max.   :3.00000  \n      urls                tags       emojis \n Min.   :0.0000000   Min.   :0   Min.   :0  \n 1st Qu.:0.0000000   1st Qu.:0   1st Qu.:0  \n Median :0.0000000   Median :0   Median :0  \n Mean   :0.0005612   Mean   :0   Mean   :0  \n 3rd Qu.:0.0000000   3rd Qu.:0   3rd Qu.:0  \n Max.   :1.0000000   Max.   :0   Max.   :0  \n\n\n\n\n9.3.3 Subset a Text Corpus\nThe last example in this chapter covers subsetting a text corpus based on document-level variables. Researchers who want to compare Trump and Biden may not be interested in the moderator’s statements. We can exclude the moderators with corpus_subset().\n\ndata_corpus_debatescand <- corpus_subset(data_corpus_debatesseg,\n                                          speaker %in% c(\"Trump\", \"Biden\"))\n\n# check that subsetting worked as expected\ntable(data_corpus_debatescand$speaker,\n      data_corpus_debatescand$location)\n\n       \n        Belmont University in Nashville, Tennessee\n  Biden                                         84\n  Trump                                        122\n       \n        Case Western Reserve University in Cleveland, Ohio\n  Biden                                                269\n  Trump                                                340"
  },
  {
    "objectID": "09-quanteda-corpus.html#issues",
    "href": "09-quanteda-corpus.html#issues",
    "title": "9  Creating and Managing Corpora",
    "section": "9.4 Issues",
    "text": "9.4 Issues\n\n9.4.1 Identifying Patterns for Corpus Segmentation\nSegmenting corpora into smaller units requires a common pattern across the documents. In the example above, we identified utterances based on the combination of a speaker’s surname in capital letters followed by a colon. Other corpora may include markers such as line breaks or headings that can be used to segment a corpus. When segmenting text corpora, we strongly recommend inspecting the resulting text corpus and spot-check that the segmentation worked as expected.\n\n\n9.4.2 Reshaping Corpora after Statistical Analyis of Texts\nIn many applications, the unit of analysis of the text corpus differs from the dataset used for statistical analyses. For example, Castanho Silva and Proksch (2021) study sentiment on European politics in tweets and parliamentary speeches. The authors construct a corpus of speeches and tweets that mention keywords relating to Europe or the EU and apply a sentiment dictionary to each document. The authors aggregate sentiment to the level of all relevant texts by a single politician, moving from over 100,000 Europe-related tweets and 20,000 Europe-related speeches to around 2500 observations. Each observation stores the sentiment by one Member of Parliament during their period of investigation. These sentiment scores are then used in regression models.\nMüller (2022) studied the temporal focus of parties’ campaign communication. First, he reshaped party manifestos to the level of sentences before identifying the temporal focus of each sentence using supervised classification (Chapter 23). The regression models use an aggregated dataset with only three observations per manifesto (sentiment in sections about the past, present, and future).\n\n\n\n\n\n\nApplying the group_by() in combination with the summarise() functions of the dplyr packages allows you to reshape the output of a textual analysis, stored as a data frame, to a higher-level unit of analysis."
  },
  {
    "objectID": "09-quanteda-corpus.html#further-reading",
    "href": "09-quanteda-corpus.html#further-reading",
    "title": "9  Creating and Managing Corpora",
    "section": "9.5 Further Reading",
    "text": "9.5 Further Reading\n\nSelecting document and considerations of “found data”: Grimmer, Roberts, and Stewart (2022, ch. 4)\nAdjusting strings: Wickham and Grolemund (2017, ch. 14)"
  },
  {
    "objectID": "09-quanteda-corpus.html#exercises",
    "href": "09-quanteda-corpus.html#exercises",
    "title": "9  Creating and Managing Corpora",
    "section": "9.6 Exercises",
    "text": "9.6 Exercises\nIn the exercises below, we use a corpus of speeches from the 2017 UN General Debates (data_corpus_ungd2017, included in the quanteda.corpora package).2\n\nIdentify the number of documents in data_corpus_ungd2017.\nSelect only speeches delivered by representatives of African and Asian countries.\nReshape this subsetted corpus to the level of sentences.\nExplore textstat_summary() of the quanteda.textstats package. Apply the function to data_corpus_ungd2017 and assign it to an object called tstat_sum_ungd.\nWhat are the average, median, minimum, and maximum document lengths?\nAdvanced: filter only speeches consisting of at least ?? tokens.\nAdvanced: use tstat_sum_ungd and create a histogram of document length using the ggplot2 package.\nAdvanced: rerun the code for plotting the distribution, but add facet_wrap() and create small multiples for each continent.\n\n\n\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 461–97. Thousand Oaks: Sage.\n\n\nBenoit, Kenneth, Kevin Munger, and Arthur Spirling. 2019. “Measuring and Explaining Political Sophistication Through Textual Complexity.” American Journal of Political Science 63 (2): 491–508. https://doi.org/10.1111/ajps.12423.\n\n\nBoussalis, Constantine, Travis G. Coan, Mirya R. Holman, and Stefan Müller. 2021. “Gender, Candidate Emotional Expression, and Voter Reactions During Televised Debates.” American Political Science Review 115 (4): 1242–57. https://doi.org/10.1017/S0003055421000666.\n\n\nCastanho Silva, Bruno, and Sven-Oliver Proksch. 2021. “Politicians Unleashed? Political Communication on Twitter and in Parliament in Western Europe.” Political Science Research and Methods published ahead of print (doi: 10.1017/psrm.2021.36). https://doi.org/10.1017/psrm.2021.36.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts: A New Framework for Machine Learning and the Social Sciences. Princeton: Princeton University Press.\n\n\nHerzog, Alexander, and Kenneth Benoit. 2015. “The Most Unkindest Cuts: Speaker Selection and Expressed Goverment Dissent During Economic Crisis.” The Journal of Politics 77 (4): 1157–75. https://doi.org/10.1086/682670.\n\n\nMüller, Stefan. 2022. “The Temporal Focus of Campaign Communication.” The Journal of Politics 84 (1): 585–90. https://doi.org/10.1086/715165.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol: O’Reilly. https://r4ds.had.co.nz."
  },
  {
    "objectID": "10-quanteda-tokens.html",
    "href": "10-quanteda-tokens.html",
    "title": "10  Creating and Managing tokens",
    "section": "",
    "text": "Creating and basic selection and manipulation of tokens.\n\ntokenizing and tokenization options\nselecting tokens\nremoving tokens\n“stop words”\nstemming and lemmatization\nmanaging metadata in tokens objects"
  },
  {
    "objectID": "10-quanteda-tokens.html#methods",
    "href": "10-quanteda-tokens.html#methods",
    "title": "10  Creating and Managing tokens",
    "section": "10.2 Methods",
    "text": "10.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "10-quanteda-tokens.html#examples",
    "href": "10-quanteda-tokens.html#examples",
    "title": "10  Creating and Managing tokens",
    "section": "10.3 Examples",
    "text": "10.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "10-quanteda-tokens.html#issues",
    "href": "10-quanteda-tokens.html#issues",
    "title": "10  Creating and Managing tokens",
    "section": "10.4 Issues",
    "text": "10.4 Issues\nAdditional issues."
  },
  {
    "objectID": "10-quanteda-tokens.html#further-reading",
    "href": "10-quanteda-tokens.html#further-reading",
    "title": "10  Creating and Managing tokens",
    "section": "10.5 Further Reading",
    "text": "10.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "10-quanteda-tokens.html#exercises",
    "href": "10-quanteda-tokens.html#exercises",
    "title": "10  Creating and Managing tokens",
    "section": "10.6 Exercises",
    "text": "10.6 Exercises\nAdd some here."
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html",
    "href": "11-quanteda-tokensadvanced.html",
    "title": "11  Advanced Token Manipulation",
    "section": "",
    "text": "Advanced operations with tokens.\n\nselecting within windows\nreplacement\nsplitting\nchunking\ncompounding\nthe phrase() function\nngrams\nskipgrams\nbrief introduction to lookup"
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#methods",
    "href": "11-quanteda-tokensadvanced.html#methods",
    "title": "11  Advanced Token Manipulation",
    "section": "11.2 Methods",
    "text": "11.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#examples",
    "href": "11-quanteda-tokensadvanced.html#examples",
    "title": "11  Advanced Token Manipulation",
    "section": "11.3 Examples",
    "text": "11.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#issues",
    "href": "11-quanteda-tokensadvanced.html#issues",
    "title": "11  Advanced Token Manipulation",
    "section": "11.4 Issues",
    "text": "11.4 Issues\nAdditional issues."
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#further-reading",
    "href": "11-quanteda-tokensadvanced.html#further-reading",
    "title": "11  Advanced Token Manipulation",
    "section": "11.5 Further Reading",
    "text": "11.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "11-quanteda-tokensadvanced.html#exercises",
    "href": "11-quanteda-tokensadvanced.html#exercises",
    "title": "11  Advanced Token Manipulation",
    "section": "11.6 Exercises",
    "text": "11.6 Exercises\nAdd some here."
  },
  {
    "objectID": "12-quanteda-dictionaries.html",
    "href": "12-quanteda-dictionaries.html",
    "title": "12  Creating and Managing Dictionaries",
    "section": "",
    "text": "Creating and revising dictionaries. - key-value lists - more details on pattern types - the phrase() argument - importing foreign dictionaries - editing dictionaries - using dictionaries as pattern inputs to other functions - exporting dictionaries - using dictionaries: tokens_lookup()"
  },
  {
    "objectID": "12-quanteda-dictionaries.html#methods",
    "href": "12-quanteda-dictionaries.html#methods",
    "title": "12  Creating and Managing Dictionaries",
    "section": "12.2 Methods",
    "text": "12.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "12-quanteda-dictionaries.html#examples",
    "href": "12-quanteda-dictionaries.html#examples",
    "title": "12  Creating and Managing Dictionaries",
    "section": "12.3 Examples",
    "text": "12.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "12-quanteda-dictionaries.html#issues",
    "href": "12-quanteda-dictionaries.html#issues",
    "title": "12  Creating and Managing Dictionaries",
    "section": "12.4 Issues",
    "text": "12.4 Issues\nCopyright.\nWhich format?\nWeighted dictionaries."
  },
  {
    "objectID": "12-quanteda-dictionaries.html#further-reading",
    "href": "12-quanteda-dictionaries.html#further-reading",
    "title": "12  Creating and Managing Dictionaries",
    "section": "12.5 Further Reading",
    "text": "12.5 Further Reading\nOn-line sources of dictionaries.\nQI’s quanteda.dictionaries package."
  },
  {
    "objectID": "12-quanteda-dictionaries.html#exercises",
    "href": "12-quanteda-dictionaries.html#exercises",
    "title": "12  Creating and Managing Dictionaries",
    "section": "12.6 Exercises",
    "text": "12.6 Exercises\nAdd some here."
  },
  {
    "objectID": "13-quanteda-dfms.html",
    "href": "13-quanteda-dfms.html",
    "title": "13  Building Document-Feature Matrices",
    "section": "",
    "text": "Creating and revising dfm objects.\n\nthe nature of a dfm object, and the importance of sparsity\ndfm creation - document selection (subsetting)\ndocument grouping (and via docvars)\nfeature selection and transformation - weighting - trimming - smoothing - sorting - matching dfms for machine learning - fcm creation and manipulation - converting dfm objects for further use"
  },
  {
    "objectID": "13-quanteda-dfms.html#methods",
    "href": "13-quanteda-dfms.html#methods",
    "title": "13  Building Document-Feature Matrices",
    "section": "13.2 Methods",
    "text": "13.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "13-quanteda-dfms.html#examples",
    "href": "13-quanteda-dfms.html#examples",
    "title": "13  Building Document-Feature Matrices",
    "section": "13.3 Examples",
    "text": "13.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "13-quanteda-dfms.html#issues",
    "href": "13-quanteda-dfms.html#issues",
    "title": "13  Building Document-Feature Matrices",
    "section": "13.4 Issues",
    "text": "13.4 Issues\nAdditional issues."
  },
  {
    "objectID": "13-quanteda-dfms.html#further-reading",
    "href": "13-quanteda-dfms.html#further-reading",
    "title": "13  Building Document-Feature Matrices",
    "section": "13.5 Further Reading",
    "text": "13.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "13-quanteda-dfms.html#exercises",
    "href": "13-quanteda-dfms.html#exercises",
    "title": "13  Building Document-Feature Matrices",
    "section": "13.6 Exercises",
    "text": "13.6 Exercises\nAdd some here."
  },
  {
    "objectID": "14-exploring-description.html",
    "href": "14-exploring-description.html",
    "title": "14  Describing Texts",
    "section": "",
    "text": "Describing the data in texts. - n functions - summary() - docnames() - docvars() - meta() - featnames() - head(), tail(), print()"
  },
  {
    "objectID": "14-exploring-description.html#methods",
    "href": "14-exploring-description.html#methods",
    "title": "14  Describing Texts",
    "section": "14.2 Methods",
    "text": "14.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "14-exploring-description.html#examples",
    "href": "14-exploring-description.html#examples",
    "title": "14  Describing Texts",
    "section": "14.3 Examples",
    "text": "14.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "14-exploring-description.html#issues",
    "href": "14-exploring-description.html#issues",
    "title": "14  Describing Texts",
    "section": "14.4 Issues",
    "text": "14.4 Issues\nAdditional issues."
  },
  {
    "objectID": "14-exploring-description.html#further-reading",
    "href": "14-exploring-description.html#further-reading",
    "title": "14  Describing Texts",
    "section": "14.5 Further Reading",
    "text": "14.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "14-exploring-description.html#exercises",
    "href": "14-exploring-description.html#exercises",
    "title": "14  Describing Texts",
    "section": "14.6 Exercises",
    "text": "14.6 Exercises\nAdd some here."
  },
  {
    "objectID": "15-exploring-kwic.html",
    "href": "15-exploring-kwic.html",
    "title": "15  Keywords-in-Context",
    "section": "",
    "text": "What are “keywords”.\n\nThe notion of a “concordance”\nUsing patterns and different input types\nMulti-word patterns\nUsing kwic objects for subsequent input - to corpus(), or to textplot_xray()\nPreview: Statistical detection of key words"
  },
  {
    "objectID": "15-exploring-kwic.html#methods",
    "href": "15-exploring-kwic.html#methods",
    "title": "15  Keywords-in-Context",
    "section": "15.2 Methods",
    "text": "15.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "15-exploring-kwic.html#examples",
    "href": "15-exploring-kwic.html#examples",
    "title": "15  Keywords-in-Context",
    "section": "15.3 Examples",
    "text": "15.3 Examples\nUsing kwic() to verify a dictionary usage."
  },
  {
    "objectID": "15-exploring-kwic.html#issues",
    "href": "15-exploring-kwic.html#issues",
    "title": "15  Keywords-in-Context",
    "section": "15.4 Issues",
    "text": "15.4 Issues\nAdditional issues."
  },
  {
    "objectID": "15-exploring-kwic.html#further-reading",
    "href": "15-exploring-kwic.html#further-reading",
    "title": "15  Keywords-in-Context",
    "section": "15.5 Further Reading",
    "text": "15.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "15-exploring-kwic.html#exercises",
    "href": "15-exploring-kwic.html#exercises",
    "title": "15  Keywords-in-Context",
    "section": "15.6 Exercises",
    "text": "15.6 Exercises\nAdd some here."
  },
  {
    "objectID": "16-exploring-dictionaries.html",
    "href": "16-exploring-dictionaries.html",
    "title": "16  Applying Dictionaries",
    "section": "",
    "text": "The basic idea of counting token and feature matches as equivalence classes\nMatching phrases\nWeighting dictionary applications\nMulti-lingual applications\nScaling dictionary results\nConfidence intervals and bootstrapping\nDictionary validation using kwic()\nDictionary construction using textstat_keyness()"
  },
  {
    "objectID": "16-exploring-dictionaries.html#methods",
    "href": "16-exploring-dictionaries.html#methods",
    "title": "16  Applying Dictionaries",
    "section": "16.2 Methods",
    "text": "16.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "16-exploring-dictionaries.html#examples",
    "href": "16-exploring-dictionaries.html#examples",
    "title": "16  Applying Dictionaries",
    "section": "16.3 Examples",
    "text": "16.3 Examples\nSentiment analysis."
  },
  {
    "objectID": "16-exploring-dictionaries.html#issues",
    "href": "16-exploring-dictionaries.html#issues",
    "title": "16  Applying Dictionaries",
    "section": "16.4 Issues",
    "text": "16.4 Issues\nAdditional issues."
  },
  {
    "objectID": "16-exploring-dictionaries.html#further-reading",
    "href": "16-exploring-dictionaries.html#further-reading",
    "title": "16  Applying Dictionaries",
    "section": "16.5 Further Reading",
    "text": "16.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "16-exploring-dictionaries.html#exercises",
    "href": "16-exploring-dictionaries.html#exercises",
    "title": "16  Applying Dictionaries",
    "section": "16.6 Exercises",
    "text": "16.6 Exercises\nAdd some here."
  },
  {
    "objectID": "17-exploring-frequencies.html",
    "href": "17-exploring-frequencies.html",
    "title": "17  Most Frequent Words",
    "section": "",
    "text": "Summarizing features via a dfm()\nMost frequent features\ndfm_group() revisited\nUsing dictionaries to count keys\ntopfeatures()\ntextplot_wordcloud()\ntextstat_frequency()\ntextstat_keyness(), textplot_keyness()"
  },
  {
    "objectID": "17-exploring-frequencies.html#methods",
    "href": "17-exploring-frequencies.html#methods",
    "title": "17  Most Frequent Words",
    "section": "17.2 Methods",
    "text": "17.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "17-exploring-frequencies.html#examples",
    "href": "17-exploring-frequencies.html#examples",
    "title": "17  Most Frequent Words",
    "section": "17.3 Examples",
    "text": "17.3 Examples\nUsing kwic() to verify a dictionary usage."
  },
  {
    "objectID": "17-exploring-frequencies.html#issues",
    "href": "17-exploring-frequencies.html#issues",
    "title": "17  Most Frequent Words",
    "section": "17.4 Issues",
    "text": "17.4 Issues\nAdditional issues."
  },
  {
    "objectID": "17-exploring-frequencies.html#further-reading",
    "href": "17-exploring-frequencies.html#further-reading",
    "title": "17  Most Frequent Words",
    "section": "17.5 Further Reading",
    "text": "17.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "17-exploring-frequencies.html#exercises",
    "href": "17-exploring-frequencies.html#exercises",
    "title": "17  Most Frequent Words",
    "section": "17.6 Exercises",
    "text": "17.6 Exercises\nAdd some here."
  },
  {
    "objectID": "18-comparing-sophistication.html",
    "href": "18-comparing-sophistication.html",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "",
    "text": "Sentence length\nSyllables\nTypes versus tokens\nUsage in word lists\nLexical diversity\nReadability\nBootstrapping methods and uncertainty accounting"
  },
  {
    "objectID": "18-comparing-sophistication.html#methods",
    "href": "18-comparing-sophistication.html#methods",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.2 Methods",
    "text": "18.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "18-comparing-sophistication.html#examples",
    "href": "18-comparing-sophistication.html#examples",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.3 Examples",
    "text": "18.3 Examples\nSentiment analysis."
  },
  {
    "objectID": "18-comparing-sophistication.html#issues",
    "href": "18-comparing-sophistication.html#issues",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.4 Issues",
    "text": "18.4 Issues\nNon-English languages.\nSampling.\nText length and its effect on diversity. Zipf’s law and Heap’s law."
  },
  {
    "objectID": "18-comparing-sophistication.html#further-reading",
    "href": "18-comparing-sophistication.html#further-reading",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.5 Further Reading",
    "text": "18.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "18-comparing-sophistication.html#exercises",
    "href": "18-comparing-sophistication.html#exercises",
    "title": "18  Profiling Lexical Patterns and Usage",
    "section": "18.6 Exercises",
    "text": "18.6 Exercises\nAdd some here."
  },
  {
    "objectID": "19-comparing-documents.html",
    "href": "19-comparing-documents.html",
    "title": "19  Document Similarity and Distance",
    "section": "",
    "text": "Distance and similarity measures\nMeasuring similarity\nMeasuring distance\nClustering\nMulti-dimensional scaling\nNetwork analysis of document connections"
  },
  {
    "objectID": "19-comparing-documents.html#methods",
    "href": "19-comparing-documents.html#methods",
    "title": "19  Document Similarity and Distance",
    "section": "19.2 Methods",
    "text": "19.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "19-comparing-documents.html#examples",
    "href": "19-comparing-documents.html#examples",
    "title": "19  Document Similarity and Distance",
    "section": "19.3 Examples",
    "text": "19.3 Examples\nExamples here."
  },
  {
    "objectID": "19-comparing-documents.html#issues",
    "href": "19-comparing-documents.html#issues",
    "title": "19  Document Similarity and Distance",
    "section": "19.4 Issues",
    "text": "19.4 Issues\nWeighting and feature selection and its effects on similarity and distance.\nComputational issues."
  },
  {
    "objectID": "19-comparing-documents.html#further-reading",
    "href": "19-comparing-documents.html#further-reading",
    "title": "19  Document Similarity and Distance",
    "section": "19.5 Further Reading",
    "text": "19.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "19-comparing-documents.html#exercises",
    "href": "19-comparing-documents.html#exercises",
    "title": "19  Document Similarity and Distance",
    "section": "19.6 Exercises",
    "text": "19.6 Exercises\nAdd some here."
  },
  {
    "objectID": "20-comparing-features.html",
    "href": "20-comparing-features.html",
    "title": "20  Feature Similarity and Distance",
    "section": "",
    "text": "Distance and similarity measures revisited\nClustering\nNetwork analysis of feature connections\nImproving feature comparisons through detection and pre-processing of multi-word expressions"
  },
  {
    "objectID": "20-comparing-features.html#methods",
    "href": "20-comparing-features.html#methods",
    "title": "20  Feature Similarity and Distance",
    "section": "20.2 Methods",
    "text": "20.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "20-comparing-features.html#examples",
    "href": "20-comparing-features.html#examples",
    "title": "20  Feature Similarity and Distance",
    "section": "20.3 Examples",
    "text": "20.3 Examples\nExamples here."
  },
  {
    "objectID": "20-comparing-features.html#issues",
    "href": "20-comparing-features.html#issues",
    "title": "20  Feature Similarity and Distance",
    "section": "20.4 Issues",
    "text": "20.4 Issues\nComputational issues."
  },
  {
    "objectID": "20-comparing-features.html#further-reading",
    "href": "20-comparing-features.html#further-reading",
    "title": "20  Feature Similarity and Distance",
    "section": "20.5 Further Reading",
    "text": "20.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "20-comparing-features.html#exercises",
    "href": "20-comparing-features.html#exercises",
    "title": "20  Feature Similarity and Distance",
    "section": "20.6 Exercises",
    "text": "20.6 Exercises\nAdd some here."
  },
  {
    "objectID": "21-ml-supervised-scaling.html",
    "href": "21-ml-supervised-scaling.html",
    "title": "21  Supervised Document Scaling",
    "section": "",
    "text": "Introduction to the notion of document scaling. Difference from classification, which we cover in Chapter @ref(ml-classifiers).\n\nSupervised versus unsupervised methods\nWordscores\nClass affinity model"
  },
  {
    "objectID": "21-ml-supervised-scaling.html#methods",
    "href": "21-ml-supervised-scaling.html#methods",
    "title": "21  Supervised Document Scaling",
    "section": "21.2 Methods",
    "text": "21.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "21-ml-supervised-scaling.html#examples",
    "href": "21-ml-supervised-scaling.html#examples",
    "title": "21  Supervised Document Scaling",
    "section": "21.3 Examples",
    "text": "21.3 Examples\nExamples here."
  },
  {
    "objectID": "21-ml-supervised-scaling.html#issues",
    "href": "21-ml-supervised-scaling.html#issues",
    "title": "21  Supervised Document Scaling",
    "section": "21.4 Issues",
    "text": "21.4 Issues\nTraining documents and how to select them.\nUncertainty accounting."
  },
  {
    "objectID": "21-ml-supervised-scaling.html#further-reading",
    "href": "21-ml-supervised-scaling.html#further-reading",
    "title": "21  Supervised Document Scaling",
    "section": "21.5 Further Reading",
    "text": "21.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "21-ml-supervised-scaling.html#exercises",
    "href": "21-ml-supervised-scaling.html#exercises",
    "title": "21  Supervised Document Scaling",
    "section": "21.6 Exercises",
    "text": "21.6 Exercises\nAdd some here."
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html",
    "href": "22-ml-unsupervised-scaling.html",
    "title": "22  Unsupervised Document Scaling",
    "section": "",
    "text": "Notion of unsupervised scaling, based on distance\nLSA\nCorrespondence analysis\nThe “wordfish” model"
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#methods",
    "href": "22-ml-unsupervised-scaling.html#methods",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.2 Methods",
    "text": "22.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#examples",
    "href": "22-ml-unsupervised-scaling.html#examples",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.3 Examples",
    "text": "22.3 Examples\nExamples here."
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#issues",
    "href": "22-ml-unsupervised-scaling.html#issues",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.4 Issues",
    "text": "22.4 Issues\nThe hazards of ex post interpretation of scaling.\nDimensionality."
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#further-reading",
    "href": "22-ml-unsupervised-scaling.html#further-reading",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.5 Further Reading",
    "text": "22.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "22-ml-unsupervised-scaling.html#exercises",
    "href": "22-ml-unsupervised-scaling.html#exercises",
    "title": "22  Unsupervised Document Scaling",
    "section": "22.6 Exercises",
    "text": "22.6 Exercises\nAdd some here."
  },
  {
    "objectID": "23-ml-classifiers.html",
    "href": "23-ml-classifiers.html",
    "title": "23  Methods for Text Classification",
    "section": "",
    "text": "Class prediction versus scaling, and the notion of predicting classes\nNaive Bayes\nSVMs\nMore advanced methods\nFeature selection for improving prediction\nAssessing performance"
  },
  {
    "objectID": "23-ml-classifiers.html#methods",
    "href": "23-ml-classifiers.html#methods",
    "title": "23  Methods for Text Classification",
    "section": "23.2 Methods",
    "text": "23.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "23-ml-classifiers.html#examples",
    "href": "23-ml-classifiers.html#examples",
    "title": "23  Methods for Text Classification",
    "section": "23.3 Examples",
    "text": "23.3 Examples\nExamples here."
  },
  {
    "objectID": "23-ml-classifiers.html#issues",
    "href": "23-ml-classifiers.html#issues",
    "title": "23  Methods for Text Classification",
    "section": "23.4 Issues",
    "text": "23.4 Issues\nWhy kNN is poor choice.\nWhich classifier?\nngrams or unigrams?"
  },
  {
    "objectID": "23-ml-classifiers.html#further-reading",
    "href": "23-ml-classifiers.html#further-reading",
    "title": "23  Methods for Text Classification",
    "section": "23.5 Further Reading",
    "text": "23.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "23-ml-classifiers.html#exercises",
    "href": "23-ml-classifiers.html#exercises",
    "title": "23  Methods for Text Classification",
    "section": "23.6 Exercises",
    "text": "23.6 Exercises\nAdd some here."
  },
  {
    "objectID": "24-ml-embedding.html",
    "href": "24-ml-embedding.html",
    "title": "24  Word Embedding Models",
    "section": "",
    "text": "The word2vec method and its descendants\nConstructing a feature-cooccurrence matrix\nFitting a word embedding model\nUsing pre-trained embedding scores with documents\nPrediction based on embeddings\nFurther methods"
  },
  {
    "objectID": "24-ml-embedding.html#methods",
    "href": "24-ml-embedding.html#methods",
    "title": "24  Word Embedding Models",
    "section": "24.2 Methods",
    "text": "24.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "24-ml-embedding.html#examples",
    "href": "24-ml-embedding.html#examples",
    "title": "24  Word Embedding Models",
    "section": "24.3 Examples",
    "text": "24.3 Examples\nExamples here."
  },
  {
    "objectID": "24-ml-embedding.html#issues",
    "href": "24-ml-embedding.html#issues",
    "title": "24  Word Embedding Models",
    "section": "24.4 Issues",
    "text": "24.4 Issues\nIssues here."
  },
  {
    "objectID": "24-ml-embedding.html#further-reading",
    "href": "24-ml-embedding.html#further-reading",
    "title": "24  Word Embedding Models",
    "section": "24.5 Further Reading",
    "text": "24.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "24-ml-embedding.html#exercises",
    "href": "24-ml-embedding.html#exercises",
    "title": "24  Word Embedding Models",
    "section": "24.6 Exercises",
    "text": "24.6 Exercises\nAdd some here."
  },
  {
    "objectID": "25-ml-topicmodels.html",
    "href": "25-ml-topicmodels.html",
    "title": "25  Topic modelling",
    "section": "",
    "text": "Latent Dirichlet Allocation\nOlder methods (e.g. LSA)\nAdvanced methods\nUsing the stm package\nPre-processing steps and their importance, including compounding MWEs"
  },
  {
    "objectID": "25-ml-topicmodels.html#methods",
    "href": "25-ml-topicmodels.html#methods",
    "title": "25  Topic modelling",
    "section": "25.2 Methods",
    "text": "25.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "25-ml-topicmodels.html#examples",
    "href": "25-ml-topicmodels.html#examples",
    "title": "25  Topic modelling",
    "section": "25.3 Examples",
    "text": "25.3 Examples\nExamples here."
  },
  {
    "objectID": "25-ml-topicmodels.html#issues",
    "href": "25-ml-topicmodels.html#issues",
    "title": "25  Topic modelling",
    "section": "25.4 Issues",
    "text": "25.4 Issues\nUsing other topic model packages"
  },
  {
    "objectID": "25-ml-topicmodels.html#further-reading",
    "href": "25-ml-topicmodels.html#further-reading",
    "title": "25  Topic modelling",
    "section": "25.5 Further Reading",
    "text": "25.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "25-ml-topicmodels.html#exercises",
    "href": "25-ml-topicmodels.html#exercises",
    "title": "25  Topic modelling",
    "section": "25.6 Exercises",
    "text": "25.6 Exercises\nAdd some here."
  },
  {
    "objectID": "26-further-advanced-nlp.html",
    "href": "26-further-advanced-nlp.html",
    "title": "26  Advanced NLP using R",
    "section": "",
    "text": "What is “NLP” and how does it differ from what we have covered so far?\nPart of speech tagging\nDifferent schemes for part of speech tagging\nDifferentiating homographs based on POS\nPOS tagging in other languages\nNamed Entity Recognition\nNoun phrase extraction\nDependency parsing to extract syntactic relations"
  },
  {
    "objectID": "26-further-advanced-nlp.html#methods",
    "href": "26-further-advanced-nlp.html#methods",
    "title": "26  Advanced NLP using R",
    "section": "26.2 Methods",
    "text": "26.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "26-further-advanced-nlp.html#examples",
    "href": "26-further-advanced-nlp.html#examples",
    "title": "26  Advanced NLP using R",
    "section": "26.3 Examples",
    "text": "26.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "26-further-advanced-nlp.html#issues",
    "href": "26-further-advanced-nlp.html#issues",
    "title": "26  Advanced NLP using R",
    "section": "26.4 Issues",
    "text": "26.4 Issues\nAlternative NLP tools. (Here we focus on spacyr.)\nOther languages.\nTraining new models."
  },
  {
    "objectID": "26-further-advanced-nlp.html#further-reading",
    "href": "26-further-advanced-nlp.html#further-reading",
    "title": "26  Advanced NLP using R",
    "section": "26.5 Further Reading",
    "text": "26.5 Further Reading\nAdditional resources from libraries or the web."
  },
  {
    "objectID": "26-further-advanced-nlp.html#exercises",
    "href": "26-further-advanced-nlp.html#exercises",
    "title": "26  Advanced NLP using R",
    "section": "26.6 Exercises",
    "text": "26.6 Exercises\nAdd some here."
  },
  {
    "objectID": "27-further-tidy.html",
    "href": "27-further-tidy.html",
    "title": "27  Integrating “tidy” approaches",
    "section": "",
    "text": "What is the “tidy” approach and how does it apply to text?\nThe tidytext package and its advantages\nSwitching from quanteda to tidytext (and back)\nAdvantages of non-tidy text objects"
  },
  {
    "objectID": "27-further-tidy.html#methods",
    "href": "27-further-tidy.html#methods",
    "title": "27  Integrating “tidy” approaches",
    "section": "27.2 Methods",
    "text": "27.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "27-further-tidy.html#examples",
    "href": "27-further-tidy.html#examples",
    "title": "27  Integrating “tidy” approaches",
    "section": "27.3 Examples",
    "text": "27.3 Examples\nExamples of each of the above methods."
  },
  {
    "objectID": "27-further-tidy.html#issues",
    "href": "27-further-tidy.html#issues",
    "title": "27  Integrating “tidy” approaches",
    "section": "27.4 Issues",
    "text": "27.4 Issues\nEfficiency. Complexity."
  },
  {
    "objectID": "27-further-tidy.html#further-reading",
    "href": "27-further-tidy.html#further-reading",
    "title": "27  Integrating “tidy” approaches",
    "section": "27.5 Further Reading",
    "text": "27.5 Further Reading\nTidy resources. Tidytext book. Data Science Using R (Wickham and Grolemund)."
  },
  {
    "objectID": "27-further-tidy.html#exercises",
    "href": "27-further-tidy.html#exercises",
    "title": "27  Integrating “tidy” approaches",
    "section": "27.6 Exercises",
    "text": "27.6 Exercises\nAdd some here."
  },
  {
    "objectID": "28-further-hardlanguages.html",
    "href": "28-further-hardlanguages.html",
    "title": "28  Text analysis in “Hard” Languages",
    "section": "",
    "text": "Non-English characters, and encoding issues\nSegmentation in languages that do not use whitespace delimiters between words\nRight-to-left languages\nEmoji"
  },
  {
    "objectID": "28-further-hardlanguages.html#methods",
    "href": "28-further-hardlanguages.html#methods",
    "title": "28  Text analysis in “Hard” Languages",
    "section": "28.2 Methods",
    "text": "28.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "28-further-hardlanguages.html#examples",
    "href": "28-further-hardlanguages.html#examples",
    "title": "28  Text analysis in “Hard” Languages",
    "section": "28.3 Examples",
    "text": "28.3 Examples\nChinese, Japanese, Korean.\nHindi, Georgian.\nArabic and other RTL languages."
  },
  {
    "objectID": "28-further-hardlanguages.html#issues",
    "href": "28-further-hardlanguages.html#issues",
    "title": "28  Text analysis in “Hard” Languages",
    "section": "28.4 Issues",
    "text": "28.4 Issues\nStemming, syllables, character counts may be off.\nFont issues."
  },
  {
    "objectID": "28-further-hardlanguages.html#further-reading",
    "href": "28-further-hardlanguages.html#further-reading",
    "title": "28  Text analysis in “Hard” Languages",
    "section": "28.5 Further Reading",
    "text": "28.5 Further Reading\nFurther reading here."
  },
  {
    "objectID": "28-further-hardlanguages.html#exercises",
    "href": "28-further-hardlanguages.html#exercises",
    "title": "28  Text analysis in “Hard” Languages",
    "section": "28.6 Exercises",
    "text": "28.6 Exercises\nAdd some here."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bécue-Bertaut, Monica. 2019. Textual Data Science with r. CRC\nPress.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In\nHandbook of Research Methods in Political Science and International\nRelations, edited by Luigi Curini and Robert Franzese, 461–97.\nThousand Oaks: Sage.\n\n\nBenoit, Kenneth, and Akitaka Matsuo. 2020. Spacyr: Wrapper to the\n’spaCy’ ’NLP’ Library. https://CRAN.R-project.org/package=spacyr.\n\n\nBenoit, Kenneth, Kevin Munger, and Arthur Spirling. 2019.\n“Measuring and Explaining Political Sophistication Through Textual\nComplexity.” American Journal of Political Science 63\n(2): 491–508. https://doi.org/10.1111/ajps.12423.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An\nR Package for the Quantitative Analysis of Textual\nData.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBoussalis, Constantine, Travis G. Coan, Mirya R. Holman, and Stefan\nMüller. 2021. “Gender, Candidate Emotional Expression, and Voter\nReactions During Televised Debates.” American Political\nScience Review 115 (4): 1242–57. https://doi.org/10.1017/S0003055421000666.\n\n\nCastanho Silva, Bruno, and Sven-Oliver Proksch. 2021. “Politicians\nUnleashed? Political Communication on Twitter and in Parliament in\nWestern Europe.” Political Science Research and Methods\npublished ahead of print (doi: 10.1017/psrm.2021.36). https://doi.org/10.1017/psrm.2021.36.\n\n\nFeinerer, Ingo, Kurt Hornik, and David Meyer. 2008. “Text Mining\nInfrastructure in R.” Journal of Statistical\nSoftware 25 (5): 1–54. https://www.jstatsoft.org/v25/i05/.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022.\nText as Data: The Promise and Pitfalls of Automatic Content Analysis\nMethods for Political Texts: A New Framework for Machine Learning and\nthe Social Sciences. Princeton: Princeton University Press.\n\n\nHerzog, Alexander, and Kenneth Benoit. 2015. “The Most Unkindest\nCuts: Speaker Selection and Expressed Goverment Dissent During Economic\nCrisis.” The Journal of Politics 77 (4): 1157–75. https://doi.org/10.1086/682670.\n\n\nHonnibal, Matthew, Ines Montani, Sophie Van Landeghem, and Adriane Boyd.\n2020. “spaCy: Industrial-Strength\nNatural Language Processing in Python.” https://doi.org/10.5281/zenodo.1212303.\n\n\nKwartler, Ted. 2017. Text Mining in Practice with r. John Wiley\n& Sons.\n\n\nMüller, Stefan. 2022. “The Temporal Focus of Campaign\nCommunication.” The Journal of Politics 84 (1): 585–90.\nhttps://doi.org/10.1086/715165.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and\nAnalysis Using Tidy Data Principles in r.” JOSS 1 (3).\nhttps://doi.org/10.21105/joss.00037.\n\n\n———. 2017. Text Mining with r: A Tidy Approach. \" O’Reilly\nMedia, Inc.\".\n\n\nTurenne, Nicolas. 2016. Analyse de Données Textuelles\nSous r. ISTE Group.\n\n\nWickham, Hadley. 2021. Rvest: Easily Harvest (Scrape) Web\nPages. https://CRAN.R-project.org/package=rvest.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. Sebastopol:\nO’Reilly. https://r4ds.had.co.nz.\n\n\nYoung, Lori, and Stuart N. Soroka. 2012. “Affective News: The\nAutomated Coding of Sentiment in Political Texts.” Political\nCommunication 29 (2): 205–31."
  },
  {
    "objectID": "29-appendix-installation.html",
    "href": "29-appendix-installation.html",
    "title": "Appendix A — Installing the Required Tools",
    "section": "",
    "text": "Installing R\nInstalling RStudio\nInstalling quanteda\nInstalling spacy\nInstalling companion package(s)\nKeeping up to date\nTroubleshooting problems"
  },
  {
    "objectID": "29-appendix-installation.html#installing-r",
    "href": "29-appendix-installation.html#installing-r",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.2 Installing R",
    "text": "A.2 Installing R\nR is a free software environment for statistical computing that runs on numerous platforms, including Windows, macOS, Linux, and Solaris. You can find details at https://www.r-project.org/, and link there to a set of mirror websites for downloading the latest version.\nWe recommend that you always using the latest version of R, which is what we used for compiling this book. There are seldom reasons to use older versions of R, and the R Core Team and the maintainers of the largest repository of R packages, CRAN (for Comprehensive R Archive Network) put an enormous amount of attention and energy into assuring that extension packages work with stably and with one another.\nYou verify which version of R you are using by either viewing the messages on startup, e.g.\nR version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\nor by calling\n\nR.Version()$version.string\n\n[1] \"R version 4.2.1 (2022-06-23)\""
  },
  {
    "objectID": "29-appendix-installation.html#installing-recommended-tools-and-packages",
    "href": "29-appendix-installation.html#installing-recommended-tools-and-packages",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.3 Installing recommended tools and packages",
    "text": "A.3 Installing recommended tools and packages\n\nA.3.1 RStudio\nThere are numerous ways of running R, including from the “command line” (“Terminal” on Mac or Linux, or “Command Prompt” on Windows) or from the R Gui console that comes with most installations of R. But our very strong recommendation is to us the outstanding RStudio Dekstop. “IDE” stands for integrated development environment, and provides a combination of file manager, package manager, help viewer, graphics viewer, environment browser, editor, and much more. Before this can be used, however, you must have installed R.\nRStudio can be installed from https://www.rstudio.com/products/rstudio/download/.\n\n\nA.3.2 Additional packages\nThe main package you will need for the examples in this book is quanteda, which provides a framework for the quantitative analysis of textual data. When you install this package, by default it will also install any required packages that quanteda depends on to function (such as stringi that it uses for many essential string handling operations). Because “dependencies” are installed into your local library, these additional packages are also available for you to use independently. You only need to install them once (although they might need updating, see below).\nWe also suggest you install:\n\nreadtext - for reading in documents that contain text, and converting them automatically to plain text;\nspacyr - an R wrapper to the Python package spaCy for natural language processing, including part-of-speech tagging and entity extraction. While we have tried hard to make this automatic, installing and configuring spacyr actually involves some major work under the hood, such as installing a version of Python in a self-contained virtual environment and then installing spaCy and one of its language models.\n\n\n\nA.3.3 Keeping packages up-to-date\nR has an easy method for ensuring you have the latest versions of installed packages:\n\nupdate.packages()"
  },
  {
    "objectID": "29-appendix-installation.html#additional-issues",
    "href": "29-appendix-installation.html#additional-issues",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.4 Additional Issues",
    "text": "A.4 Additional Issues\n\nA.4.1 Installing development versions of packages\nThe R packages that you can install using the methods described above are the pre-compiled binary versions that are distributed on CRAN. (Linux installations are the exception, as these are always compiled upon installation.) Sometimes, package developers will publish “development” versions of their packages that have yet to published on CRAN, for instance on the popular GitHub platform hosting the world’s largest collection of open-source software.\nThe quanteda package, for instance, is hosted on GitHub at https://github.com/quanteda/quanteda, where its development version tends to be slightly ahead of the CRAN version. If you are feeling adventurous, or need a new version in which a specific issue or bug has been fixed, you can install it from the GitHub source using:\n\ndevtools::install_github(\"quanteda/quanteda\") \n\nBecause installing from GitHub is the same as installing from the source code, this also involves compiling the C++ and Fortran source code that makes parts of quanteda so fast. For this source installation to work, you will need to have installed the appropriate compilers.\nIf you are using a Windows platform, this means you will need also to install the Rtools software available from CRAN.\nIf you are using macOS, you should install the macOS tools, namely the Clang 6.x compiler and the GNU Fortran compiler (as quanteda requires gfortran to build).1\nLinux always compiles packages containing C++ code upon installation, so if you are using that platform then you are unlikely to need to install any additional components in order to install a development version.\n\n\nA.4.2 Troubleshooting\nMost problems come from not having the latest versions of packages installed, so make sure you have updated them using the instructions above.\nOther problems include: - Lack of permissions to install packages. This might affect Windows users of work laptops, whose workplace prevents user modification of any software. - Lack of internet access, or access being restricted by a firewall or proxy server."
  },
  {
    "objectID": "29-appendix-installation.html#further-reading",
    "href": "29-appendix-installation.html#further-reading",
    "title": "Appendix A — Installing the Required Tools",
    "section": "A.5 Further Reading",
    "text": "A.5 Further Reading\nHadley Wickham’s excellent book R Packages is well worth reading.\n\nWickham, Hadley. (2015). R packages: organize, test, document, and share your code. O’Reilly Media, Inc."
  },
  {
    "objectID": "30-appendix-encoding.html",
    "href": "30-appendix-encoding.html",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "",
    "text": "(and were afraid to ask)"
  },
  {
    "objectID": "30-appendix-encoding.html#objectives",
    "href": "30-appendix-encoding.html#objectives",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.1 Objectives",
    "text": "B.1 Objectives\n\nThe concept of encoding\nClassic 8-bit encodings\nHow to detect encodings\nConverting files\nConverting text once loaded\nEncoding “bit” versus actual text encoding\nWhen you are likely to encounter problems (with what sorts of files)\nUnicode encodings (UTF-8, UTF-16)\nHow to avoid encoding headaches forever"
  },
  {
    "objectID": "30-appendix-encoding.html#methods",
    "href": "30-appendix-encoding.html#methods",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.2 Methods",
    "text": "B.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "30-appendix-encoding.html#examples",
    "href": "30-appendix-encoding.html#examples",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.3 Examples",
    "text": "B.3 Examples\nExamples here."
  },
  {
    "objectID": "30-appendix-encoding.html#issues",
    "href": "30-appendix-encoding.html#issues",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.4 Issues",
    "text": "B.4 Issues\nUTF-16 in some operating systems (Windows).\nEditors that are “encoding smart”."
  },
  {
    "objectID": "30-appendix-encoding.html#further-reading",
    "href": "30-appendix-encoding.html#further-reading",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.5 Further Reading",
    "text": "B.5 Further Reading\nFurther reading here."
  },
  {
    "objectID": "30-appendix-encoding.html#exercises",
    "href": "30-appendix-encoding.html#exercises",
    "title": "Appendix B — Everything You Never Wanted to Know about Encoding",
    "section": "B.6 Exercises",
    "text": "B.6 Exercises\nAdd some here."
  },
  {
    "objectID": "31-appendix-regex.html",
    "href": "31-appendix-regex.html",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "",
    "text": "Basic of regular expressions\nFixed matching and “globs” and relationship to regexes\nBase regular expressions versus stringi\ncharacter classes\nUnicode character categories\nLookaheads"
  },
  {
    "objectID": "31-appendix-regex.html#methods",
    "href": "31-appendix-regex.html#methods",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.2 Methods",
    "text": "C.2 Methods\nApplicable methods for the objectives listed above."
  },
  {
    "objectID": "31-appendix-regex.html#examples",
    "href": "31-appendix-regex.html#examples",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.3 Examples",
    "text": "C.3 Examples\nExamples here."
  },
  {
    "objectID": "31-appendix-regex.html#issues",
    "href": "31-appendix-regex.html#issues",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.4 Issues",
    "text": "C.4 Issues\nOlder variants: PCRE, versus GNU, versus stringi implementations.\nShortcut names versus Unicode categories."
  },
  {
    "objectID": "31-appendix-regex.html#further-reading",
    "href": "31-appendix-regex.html#further-reading",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.5 Further Reading",
    "text": "C.5 Further Reading\nFurther reading here."
  },
  {
    "objectID": "31-appendix-regex.html#exercises",
    "href": "31-appendix-regex.html#exercises",
    "title": "Appendix C — A Survival Guide to Regular Expressions",
    "section": "C.6 Exercises",
    "text": "C.6 Exercises\nAdd some here."
  }
]